{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/M1F1/MasterThesis/blob/master/PreprocessingIMDB_MR_toBTformat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DqZevLgKdCA_",
    "outputId": "3901b896-0f46-42e8-d06f-99732aa067a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown==3.11.0\n",
      "  Downloading https://files.pythonhosted.org/packages/db/f9/757abd4b0ebf60f3d276b599046c515c070fab5161b22abb952e35f3c0a4/gdown-3.11.0.tar.gz\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (1.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (4.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (3.0.12)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (2020.4.5.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (1.7.1)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gdown: filename=gdown-3.11.0-cp36-none-any.whl size=9619 sha256=5007c6e75e91a92f907187f9e28264991fe6a99e50d9674299a55c9467c26f60\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/a6/67/ebb80360fc57bb0ddd5da77f57b275084cd8838bf7d5b91685\n",
      "Successfully built gdown\n",
      "Installing collected packages: gdown\n",
      "  Found existing installation: gdown 3.6.4\n",
      "    Uninstalling gdown-3.6.4:\n",
      "      Successfully uninstalled gdown-3.6.4\n",
      "Successfully installed gdown-3.11.0\n",
      "Collecting lineflow\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/09/6e8842e1c8ace250d352cf59503e3939169d8a3abe0d17115d5916054930/lineflow-0.6.4.tar.gz\n",
      "Collecting arrayfiles\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/be/297c365c7f8304ffa949bb252eac9d67e5c708a3615c9d5b18a7613e9006/arrayfiles-0.0.1.tar.gz\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from lineflow) (3.11.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown->lineflow) (4.41.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown->lineflow) (1.12.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.6/dist-packages (from gdown->lineflow) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from gdown->lineflow) (3.0.12)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown->lineflow) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown->lineflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown->lineflow) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown->lineflow) (2.9)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown->lineflow) (1.7.1)\n",
      "Building wheels for collected packages: lineflow, arrayfiles\n",
      "  Building wheel for lineflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lineflow: filename=lineflow-0.6.4-cp36-none-any.whl size=23202 sha256=fad549caa0adb4276c6b309a7e8e4c51dd81b28218223de25bb37e18d08e5db8\n",
      "  Stored in directory: /root/.cache/pip/wheels/b9/11/32/a6120f98d7d11ed8cf1b28b265a12a4b72842da341c13384c1\n",
      "  Building wheel for arrayfiles (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for arrayfiles: filename=arrayfiles-0.0.1-cp36-none-any.whl size=5434 sha256=e413826e6eeabe9fb5b4a9cb09b03cd7d396308d66c4a2a6b02d11c355c4ed85\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/43/9b/fb9049f9c3931d703182bd45418c3e8d67a6b6eded6325e16e\n",
      "Successfully built lineflow arrayfiles\n",
      "Installing collected packages: arrayfiles, lineflow\n",
      "Successfully installed arrayfiles-0.0.1 lineflow-0.6.4\n",
      "Collecting python-Levenshtein\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (47.3.1)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144793 sha256=77c87da2eec91db25528b7cc29023842d10497afea8d0e2542cce0100c17efd7\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.0\n",
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "!pip install gdown==3.11.0\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "if importlib.util.find_spec('lineflow') is None:\n",
    "  !pip install lineflow\n",
    "import lineflow as lf\n",
    "import lineflow.datasets as lfds\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "!pip install python-Levenshtein\n",
    "import Levenshtein\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "os.environ['DATASETS_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'datasets')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2iNTLytI7wxg"
   },
   "source": [
    "### IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHb88T_p7vlf"
   },
   "outputs": [],
   "source": [
    "def preprocessor(text): \n",
    "  import re\n",
    "  pattern1 = re.compile(r'<.*?>')\n",
    "  # pattern2 = re.compile('[\\W_]+')\n",
    "  text = pattern1.sub('', text)\n",
    "  # print('text after p1: ', text)\n",
    "  # text = text.lower()\n",
    "  return text\n",
    "\n",
    "ds = lfds.Imdb('train') + lfds.Imdb('test')\n",
    "ds = ds.map(lambda x: {'text': preprocessor(x[0]), 'intent': x[1]})\n",
    "df = pd.DataFrame(ds)\n",
    "df['bt'] = np.nan\n",
    "df.to_csv(os.path.join(os.environ['DATASETS_PATH'], 'dumbed_imdb_oafa.csv'))\n",
    "\n",
    "dfs = np.array_split(df, 50)\n",
    "for i, df_chunk in enumerate(dfs): \n",
    "  path = os.path.join(os.environ['DATASETS_PATH'], f'imdb_file_{i}.txt')\n",
    "  df_chunk['text'].to_csv(path, header=None, index=None, sep='\\n', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyM3KwV3HV2e"
   },
   "source": [
    "Reading from transcribed files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rruRAIc5Py14"
   },
   "outputs": [],
   "source": [
    "global_review_list = []\n",
    "for i in range(50):\n",
    "  path_to_file = os.path.join(os.environ['DATASETS_PATH'], 'imdb_file_{}.txt'.format(i))\n",
    "  with open(path_to_file, \"r\", encoding='utf-8') as bookFile:\n",
    "      review_list = bookFile.readlines()\n",
    "  global_review_list.extend(review_list)\n",
    "\n",
    "\n",
    "global_review_list_bt = []\n",
    "for i in range(50):\n",
    "  path_to_file = os.path.join(os.environ['DATASETS_PATH'], 'imdb_file_bt_{}'.format(i), 'file_0_of_1.json')\n",
    "  with open(path_to_file, \"r\", encoding='utf-8') as bookFile:\n",
    "      review_list = bookFile.readlines()\n",
    "  global_review_list_bt.extend(review_list)\n",
    "\n",
    "df_bt =  pd.DataFrame({'text':global_review_list, 'bt': global_review_list_bt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zypER9BI3B1d"
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def delete_non_alphanumeric(text):\n",
    "  return re.sub('[\\W_]+', '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "128zS1LNi_L-",
    "outputId": "7440655e-83c6-4c07-b15f-de71f1a65ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4777\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_bt['text'] = df_bt['text'].replace(r'\"','', regex=True)\n",
    "df_bt['bt'] = df_bt['bt'].replace(r'\"','', regex=True)\n",
    "df['text'] = df['text'].replace(r'\"','', regex=True)\n",
    "\n",
    "\n",
    "df_bt['bt'] = df_bt['bt'].replace(r'\\n','', regex=True)\n",
    "df_bt['text'] = df_bt['text'].replace(r'\\n','', regex=True)\n",
    "df['text'] = df['text'].replace(r'\\n','', regex=True)\n",
    "# df_bt['text'] = df_bt['text'].apply(lambda x: [item for item in x if item not in stop])\n",
    "# df['text'] = df['text'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "# df_bt['text'] = df_bt['text'].apply(delete_non_alphanumeric)\n",
    "# df['text'] = df['text'].apply(delete_non_alphanumeric)\n",
    "\n",
    "# df_bt['text'] = df_bt['text'].apply(lambda x: x[:200])\n",
    "# df['text'] = df['text'].apply(lambda x: x[:200])\n",
    "\n",
    "new_indexes_df = df.text.str.len().sort_values().index\n",
    "new_indexes_df_bt = df_bt.text.str.len().sort_values().index\n",
    "\n",
    "df = df.reindex(new_indexes_df)\n",
    "df_bt = df_bt.reindex(new_indexes_df_bt)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df_bt =  df_bt.reset_index(drop=True)\n",
    "\n",
    "df['len']  = df['text'].str.len()\n",
    "df_bt['len'] = df_bt['text'].str.len()\n",
    "\n",
    "s = df['text'] == df_bt['text']\n",
    "print(sum(s))\n",
    "d = df['text'] != df_bt['text']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "-MDjsHf4aX32",
    "outputId": "18ab045f-7e0f-4381-8aaf-dd3e0573bc5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "100%|██████████| 50000/50000 [41:27<00:00, 20.10it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio() \n",
    "\n",
    "for idx, rx in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "\n",
    "  df_temp = df_bt[df_bt['len'] > (rx['len'] - 1)]\n",
    "\n",
    "  for idy, ry in df_temp.iterrows():\n",
    "\n",
    "    ratio = similar(rx['text'], ry['text']) \n",
    "\n",
    "    if ratio > 0.97:\n",
    "      df['bt'].iloc[idx] = df_bt['bt'].iloc[idy] \n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2oRGP7-mrN-"
   },
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(os.environ['DATASETS_PATH'], \"imdb_with_bt.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7dlpZ29ZwDj"
   },
   "source": [
    "## MR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnbn7y7hxaeW"
   },
   "outputs": [],
   "source": [
    "import toolz\n",
    "if not Path('./txt_sentoken').exists():\n",
    "  !wget https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
    "  !tar -zxvf review_polarity.tar.gz\n",
    "\n",
    "text = []\n",
    "sentiment = []\n",
    "directory_neg = './txt_sentoken/neg'\n",
    "directory_pos = './txt_sentoken/pos'\n",
    "\n",
    "def open_file_and_save_as_string(file_path: str) -> str:\n",
    "  lines = []\n",
    "\n",
    "  with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "      lines = f.readlines()\n",
    "      \n",
    "  flatten_lines = list(toolz.concat(lines))\n",
    "  joined_str = \"\".join(flatten_lines)\n",
    "  return joined_str \n",
    "    \n",
    "def create_data_pair(directory: str, label_type: str) -> dict:\n",
    "  texts = []\n",
    "  labels = []\n",
    "  pathlist = Path(directory).rglob('*.txt')\n",
    "  for path in pathlist:\n",
    "      path_in_str = str(path)\n",
    "      texts.append(open_file_and_save_as_string(path_in_str))\n",
    "      labels.append(label_type)\n",
    "\n",
    "  return {'text': texts,\n",
    "          'labels': labels}\n",
    "\n",
    "data_neg = create_data_pair(directory_neg, 'neg')\n",
    "df_neg = pd.DataFrame(data_neg)\n",
    "data_pos = create_data_pair(directory_pos, 'pos')\n",
    "df_pos = pd.DataFrame(data_pos)\n",
    "df_all_data = pd.concat([df_neg, df_pos])\n",
    "df_all_data = df_all_data.reset_index(drop=True)\n",
    "\n",
    "df_all_data['text'] = df_all_data['text'].replace(r'\\n','', regex=True)\n",
    "df_all_data.to_csv(str(Path(os.environ['DATASETS_PATH']) / 'mr.csv'))\n",
    "mr_df = pd.read_csv(str(Path(os.environ['DATASETS_PATH']) / 'mr.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckjYj0tD6-TH"
   },
   "source": [
    "##### Create txt format for bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J0IJ2-viUtxC"
   },
   "outputs": [],
   "source": [
    "mr_df\n",
    "path = os.path.join(os.environ['DATASETS_PATH'], f'mr_file.txt')\n",
    "mr_df['text'].to_csv(path, header=None, index=None, sep='\\n', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lNWgoes37COW"
   },
   "source": [
    "##### Get back translated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O7gDo2jxVEpF"
   },
   "outputs": [],
   "source": [
    "\n",
    "path_to_file = os.path.join(os.environ['DATASETS_PATH'], 'mr_file_bt', 'file_0_of_1.json')\n",
    "with open(path_to_file, \"r\", encoding='utf-8') as bookFile:\n",
    "    review_list = bookFile.readlines()\n",
    "\n",
    "mr_df['paraphrases'] = review_list\n",
    "mr_df['paraphrases'] = mr_df['paraphrases'].replace(r'\\n','', regex=True)\n",
    "path = os.path.join(os.environ['DATASETS_PATH'], 'mr_with_bt.csv')\n",
    "mr_df.to_csv(path, header=None, index=None, sep='\\n', mode='w')\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "PreprocessingIMDB_MR_toBTformat.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
