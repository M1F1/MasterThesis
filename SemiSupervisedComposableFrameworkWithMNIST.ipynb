{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemiSupervisedComposableFramework.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_DWq_Q17ENpc",
        "_9cu0GG3qjHz",
        "aHn7X8Oc2IJN",
        "AhTVU9r32eyI",
        "8blDj0ji58J4",
        "AjAj0j4L5vTG"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cfed226301d94af7a5e000078a9ffbab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9c46a3f1d0d541c28e56d31a166a79ee",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ef4898e3f13541409b7f5cabd40c3837",
              "IPY_MODEL_7f19cb52705f4156ac4ec85ab6451986"
            ]
          }
        },
        "9c46a3f1d0d541c28e56d31a166a79ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "ef4898e3f13541409b7f5cabd40c3837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1eda2aa271f64a0eadc6d020e42cd283",
            "_dom_classes": [],
            "description": "Validation sanity check: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1539425a17c64d05a554ea73a7eac9ab"
          }
        },
        "7f19cb52705f4156ac4ec85ab6451986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_11efffd4fa2d44438ad8b32c6df198fe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:01&lt;00:00,  1.60it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_39a17e62633644b4afa30acdb46542ef"
          }
        },
        "1eda2aa271f64a0eadc6d020e42cd283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1539425a17c64d05a554ea73a7eac9ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "11efffd4fa2d44438ad8b32c6df198fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "39a17e62633644b4afa30acdb46542ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9169f11300d04f4b81d2f2d9cf058d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_87b71385f7c14b939e60f867f0596078",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_10d8b881e8614d8faf02f04eb500f12c",
              "IPY_MODEL_d256c7b03ccc489dbcdc5477fea46f7c"
            ]
          }
        },
        "87b71385f7c14b939e60f867f0596078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "10d8b881e8614d8faf02f04eb500f12c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_22f51b6fc1b04322b6857fb233045b3b",
            "_dom_classes": [],
            "description": "Epoch 3: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 68,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 68,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_68ceb60e8a2e4c39a42536dd2cfa2157"
          }
        },
        "d256c7b03ccc489dbcdc5477fea46f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4f189eeed7da46c6b8423f0a4b51b186",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 68/68 [00:02&lt;00:00, 26.64it/s, loss=7.289, v_num=2gma0cof, val_loss=2.85, val_accuracy_error=0.838, val_f1_error=0.838, val_recall_error=0.838, val_precision_error=0.838, val_max_confident=0.833, val_min_confident=0.249, val_mean_confident=0.481, val_std_confident=0.167]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_494dac7506c2477aa4c615ed10ed56ef"
          }
        },
        "22f51b6fc1b04322b6857fb233045b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "68ceb60e8a2e4c39a42536dd2cfa2157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f189eeed7da46c6b8423f0a4b51b186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "494dac7506c2477aa4c615ed10ed56ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab4bfa30558143a4b66dd98a84fd6496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7aae98c9160b41398e0833e42f7a0202",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4805c7ea211b4e3b9425b963506d7633",
              "IPY_MODEL_9e6069b656b9415e82984737f3abc665"
            ]
          }
        },
        "7aae98c9160b41398e0833e42f7a0202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "4805c7ea211b4e3b9425b963506d7633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d99b8c62284c498c9ba538cee6137ad0",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c40e234318f14ddfb06ebf5462d8eb30"
          }
        },
        "9e6069b656b9415e82984737f3abc665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dbaae0c90b944c37ac85e5d58121ae7a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 63/63 [00:01&lt;00:00, 11.89it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef1cfe9b13d240d3ad92d44c7bf81f24"
          }
        },
        "d99b8c62284c498c9ba538cee6137ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c40e234318f14ddfb06ebf5462d8eb30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dbaae0c90b944c37ac85e5d58121ae7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef1cfe9b13d240d3ad92d44c7bf81f24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d800d2110f9487f863bc81a33697172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b6a3944ed0d74cccbd4c6a7f602c86d4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_53e35c39c84f486b99fe593448bc8743",
              "IPY_MODEL_a6c7279d3e4e4904b92ec2b6cf299fd1"
            ]
          }
        },
        "b6a3944ed0d74cccbd4c6a7f602c86d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "53e35c39c84f486b99fe593448bc8743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e25239ebdc6440338e8a75dfd72ff3a0",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eedf6a6712f3429e9a112ac3bd82c6aa"
          }
        },
        "a6c7279d3e4e4904b92ec2b6cf299fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_07f6670f8f9a4a9197c194f13c6ae451",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 63/63 [00:01&lt;00:00, 12.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a5156a9436f4865a1b9c68b5b61d8a6"
          }
        },
        "e25239ebdc6440338e8a75dfd72ff3a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eedf6a6712f3429e9a112ac3bd82c6aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07f6670f8f9a4a9197c194f13c6ae451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a5156a9436f4865a1b9c68b5b61d8a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a610463fb75f484990975077bd4c424f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b92a50a9d47e417b9305f4c716e5fc58",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dde33337eb774a47be27c91459c5a2c4",
              "IPY_MODEL_83c70e39a7864253b1d365de25ab5383"
            ]
          }
        },
        "b92a50a9d47e417b9305f4c716e5fc58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "dde33337eb774a47be27c91459c5a2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ff3f247988524c92ab9153ac88f11860",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b11fd17dffd44b468b5b52423d6558c2"
          }
        },
        "83c70e39a7864253b1d365de25ab5383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0214107e92db4314b1e63135b7d4965a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 63/63 [00:01&lt;00:00, 12.50it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4de75f08b42b46e9b5ab4ff5310e3ec9"
          }
        },
        "ff3f247988524c92ab9153ac88f11860": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b11fd17dffd44b468b5b52423d6558c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0214107e92db4314b1e63135b7d4965a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4de75f08b42b46e9b5ab4ff5310e3ec9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0f7a77f25404d8895d52702f7e90ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cd658230e5f74900863c45669d919d6a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_85b2230322494354ae5065efa9f2e60e",
              "IPY_MODEL_b5a39566ba1a43b98ed3ad179e79c251"
            ]
          }
        },
        "cd658230e5f74900863c45669d919d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "85b2230322494354ae5065efa9f2e60e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f3f8034931a64c8b85233d9d728769be",
            "_dom_classes": [],
            "description": "Testing: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4bfdf04e124d4ab9af7b5835c39bd55a"
          }
        },
        "b5a39566ba1a43b98ed3ad179e79c251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_124f8c23a5f44327848f677e38a97516",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 438/438 [00:05&lt;00:00, 75.89it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ee2046cbc984b209a6d063b5f32ac23"
          }
        },
        "f3f8034931a64c8b85233d9d728769be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4bfdf04e124d4ab9af7b5835c39bd55a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "124f8c23a5f44327848f677e38a97516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ee2046cbc984b209a6d063b5f32ac23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M1F1/MasterThesis/blob/master/SemiSupervisedComposableFrameworkWithMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmIolHgLI6Yn",
        "colab_type": "text"
      },
      "source": [
        "### Setup env "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE7eYLcuXnX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "248f27c0-7035-4c2b-cd0b-2abaac8817db"
      },
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "import pkg_resources\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "if importlib.util.find_spec('neptune') is None:\n",
        "  !pip install neptune-client\n",
        "\n",
        "if importlib.util.find_spec('wandb') is None:\n",
        "  !pip install wandb\n",
        "\n",
        "!wandb login\n",
        "\n",
        "if importlib.util.find_spec('pytorch_lightning') is None:\n",
        "  !pip install pytorch-lightning\n",
        "\n",
        "if importlib.util.find_spec('logzero') is None:\n",
        "  !pip install logzero \n",
        "\n",
        "if importlib.util.find_spec('tensorboardX') is None:\n",
        "  !pip install tensorboardX \n",
        "\n",
        "if importlib.util.find_spec('lineflow') is None:\n",
        "  !pip install lineflow\n",
        "\n",
        "if importlib.util.find_spec('optuna') is None:\n",
        "  !pip install optuna\n",
        "\n",
        "#if importlib.util.find_spec('gdown') is None:\n",
        "!pip install gdown==3.11.0\n",
        "  \n",
        "if importlib.util.find_spec('transformers') is None:\n",
        "  !pip install transformers \n",
        "  \n",
        "if importlib.util.find_spec('nlpaug') is None:\n",
        "  !pip install nlpaug \n",
        "\n",
        "if importlib.util.find_spec('torchtest') is None:\n",
        "  !pip install torchtest \n",
        "\n",
        "import gdown\n",
        "import contextlib\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from typing import Dict\n",
        "import re\n",
        "import time\n",
        "\n",
        "import lineflow as lf\n",
        "import lineflow.datasets as lfds\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset, sampler, ConcatDataset\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "if pkg_resources.parse_version(pl.__version__) < pkg_resources.parse_version(\"0.7.1\"):\n",
        "  raise RuntimeError(\"PyTorch Lightning>=0.7.1 is required for this code.\")\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.logging.neptune import NeptuneLogger \n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning import Callback\n",
        "from pytorch_lightning.callbacks import LearningRateLogger\n",
        "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
        "\n",
        "from gensim.utils import tokenize as gensim_tokenizer\n",
        "import gensim\n",
        "from gensim.models.fasttext import FastText as FT_gensim\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, RobertaTokenizer, RobertaModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup \n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "import toolz\n",
        "from toolz import partial, compose, juxt\n",
        "from nltk.corpus import stopwords\n",
        "import optuna\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.model.word_stats as nmw\n",
        "import nlpaug.flow as nafc\n",
        "from nlpaug.util import Action\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "!pip freeze > requirements.txt\n",
        "\n",
        "os.environ['PROJECT_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning')\n",
        "os.environ['DATASETS_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'datasets')\n",
        "os.environ['REQUIREMENTS_PATH'] = str(Path()/'requirements.txt')\n",
        "os.environ['MODEL_CHECKPOINT_PATH'] = str(Path()/'model_checkpoints')\n",
        "os.environ['SPELLING_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'nlpaug'/'spelling_en.txt') \n",
        "os.environ['NLPAUG_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'nlpaug')\n",
        "artefacts_temp_dir = os.path.join(os.environ['PROJECT_PATH'], 'parametrized_nbs')\n",
        "\n",
        "neptune_api_token_key_file = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'neptune_api_token.txt')\n",
        "with open (neptune_api_token_key_file, 'r') as f:\n",
        "  os.environ['NEPTUNE_API_TOKEN'] = f.readlines()[0]\n",
        "\n",
        "if not os.path.exists(artefacts_temp_dir):\n",
        "  os.makedirs(artefacts_temp_dir)\n",
        "\n",
        "if not os.path.exists(os.environ['MODEL_CHECKPOINT_PATH']):\n",
        "  os.makedirs(os.environ['MODEL_CHECKPOINT_PATH'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/gdrive\n",
            "Collecting neptune-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/d6/00edcbff22f7ada15ff7b3e3c21b4cbbdd2bdd54aa72424691ad038c7d66/neptune-client-0.4.119.tar.gz (90kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 6.4MB/s \n",
            "\u001b[?25hCollecting bravado\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/cc/b3c8dadc3f51fa184db10172f031c1c5206b0e67f3207217bbdd326e81a4/bravado-10.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829kB 16.4MB/s \n",
            "\u001b[?25hCollecting py3nvml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/b3/cb30dd8cc1198ae3fdb5a320ca7986d7ca76e23d16415067eafebff8685f/py3nvml-0.2.6-py3-none-any.whl (55kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from neptune-client) (1.0.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (7.0.0)\n",
            "Collecting PyJWT\n",
            "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (1.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (1.15.0)\n",
            "Collecting websocket-client>=0.35.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204kB 24.3MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/1e/a45320cab182bf1c8656107b3d4c042e659742822fc6bff150d769a984dd/GitPython-3.1.7-py3-none-any.whl (158kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163kB 30.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from neptune-client) (20.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client) (2.8.1)\n",
            "Collecting msgpack-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/20/6eca772d1a5830336f84aca1d8198e5a3f4715cd1c7fc36d3cc7f7185091/msgpack-python-0.5.6.tar.gz (138kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143kB 41.4MB/s \n",
            "\u001b[?25hCollecting bravado-core>=5.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/11/18e9d28a156c33f2d5f15a5e155dc7130250acb0a569255a2b6b307b596d/bravado_core-5.17.0-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client) (3.7.4.2)\n",
            "Collecting simplejson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/96/1e6b19045375890068d7342cbe280dd64ae73fd90b9735b5efb8d1e044a1/simplejson-3.17.2-cp36-cp36m-manylinux2010_x86_64.whl (127kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 39.5MB/s \n",
            "\u001b[?25hCollecting monotonic\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Collecting xmltodict\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client) (1.18.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client) (1.24.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 12.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->neptune-client) (2.4.7)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (1.0.0)\n",
            "Collecting swagger-spec-validator>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/09/de/e78cefbf5838b434b63a789264b79821cb2267f1498fbed23ef8590133e4/swagger_spec_validator-2.7.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2.6.0)\n",
            "Collecting jsonref\n",
            "  Downloading https://files.pythonhosted.org/packages/07/92/f8e4ac824b14af77e613984e480fa818397c72d4141fc466decb26752749/jsonref-0.2-py3-none-any.whl\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Collecting webcolors; extra == \"format\"\n",
            "  Downloading https://files.pythonhosted.org/packages/12/05/3350559de9714b202e443a9e6312937341bd5f79f4e4f625744295e7dd17/webcolors-1.11.1-py3-none-any.whl\n",
            "Collecting rfc3987; extra == \"format\"\n",
            "  Downloading https://files.pythonhosted.org/packages/65/d4/f7407c3d15d5ac779c3dd34fbbc6ea2090f77bd7dd12f207ccf881551208/rfc3987-1.3.8-py2.py3-none-any.whl\n",
            "Collecting strict-rfc3339; extra == \"format\"\n",
            "  Downloading https://files.pythonhosted.org/packages/56/e4/879ef1dbd6ddea1c77c0078cd59b503368b0456bcca7d063a870ca2119d3/strict-rfc3339-0.7.tar.gz\n",
            "Building wheels for collected packages: neptune-client, future, msgpack-python, strict-rfc3339\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.4.119-py2.py3-none-any.whl size=150019 sha256=b5a54e75be586cdb5baede6d82e80db4fb92cbc338ec32a62b672cc84e132ef2\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/5c/c1/a81e80761b94b4467fd3fda1fd3109463702f6247fc422eb33\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=fb63d6750ffe99194e835e2888b2a1e58a512c96d00d52da5e2fd40b831f4b56\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for msgpack-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack-python: filename=msgpack_python-0.5.6-cp36-cp36m-linux_x86_64.whl size=304239 sha256=9aa2e48ad142e41cc43b54cd1c3fc09a2115d3e717c98377009d2bb7710b5675\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/de/86/7fa56fda12511be47ea0808f3502bc879df4e63ab168ec0406\n",
            "  Building wheel for strict-rfc3339 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strict-rfc3339: filename=strict_rfc3339-0.7-cp36-none-any.whl size=18120 sha256=3ad84acd1416cf543b5953cc966d7684cabd6be4b1157e222cfd806582a9f750\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/af/c9/b6e9fb5f9b2470e4ed2a7241c9ab3a8cdd3bc8555ae02ca2e6\n",
            "Successfully built neptune-client future msgpack-python strict-rfc3339\n",
            "Installing collected packages: msgpack-python, simplejson, swagger-spec-validator, jsonref, bravado-core, monotonic, bravado, future, xmltodict, py3nvml, PyJWT, websocket-client, smmap, gitdb, GitPython, neptune-client, webcolors, rfc3987, strict-rfc3339\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed GitPython-3.1.7 PyJWT-1.7.1 bravado-10.6.2 bravado-core-5.17.0 future-0.18.2 gitdb-4.0.5 jsonref-0.2 monotonic-1.5 msgpack-python-0.5.6 neptune-client-0.4.119 py3nvml-0.2.6 rfc3987-1.3.8 simplejson-3.17.2 smmap-3.0.4 strict-rfc3339-0.7 swagger-spec-validator-2.7.3 webcolors-1.11.1 websocket-client-0.57.0 xmltodict-0.12.0\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/19/f8db9eff4b0173adf6dd2e8b0c3d8de0bfe10ec9ed63d247665980d82258/wandb-0.9.4-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4MB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz (94kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 13.9MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/4c/49f899856e3a83e02bc88f2c4945aa0bda4f56b804baa9f71e6664a574a2/sentry_sdk-0.16.5-py2.py3-none-any.whl (113kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122kB 38.9MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 13.4MB/s \n",
            "\u001b[?25hCollecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.1.7)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.6.20)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\n",
            "Building wheels for collected packages: watchdog, subprocess32, gql, pathtools, graphql-core\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73870 sha256=75509082e10ebc3ca2451412453e43f4e3eeb365ee6fe47c8a8d4faa30d95fc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=d645166d11ca024fff89cb2c0033c40d17aa78bfc8cbd44d0806de0cbed3b280\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=10e7e49f8838630a31905b535cc3c983ff42b6ac711380fb85fe38722318aa6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=00b8485a08cb023c552eeb40d08011ecd86361df70a702efdd8c4ce5b8dcae85\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=5eda9e658b8dd0f0bf10a74b4076ee63ec765345608054b2301f66f26a593aed\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "Successfully built watchdog subprocess32 gql pathtools graphql-core\n",
            "Installing collected packages: pathtools, watchdog, docker-pycreds, configparser, sentry-sdk, subprocess32, graphql-core, gql, shortuuid, wandb\n",
            "Successfully installed configparser-5.0.0 docker-pycreds-0.4.0 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.16.5 shortuuid-1.0.1 subprocess32-3.5.4 wandb-0.9.4 watchdog-0.10.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: 3c9e821c5866cac1e8b131c58196f26c6e9a4349\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n",
            "Collecting pytorch-lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/64/65a5bd6b0c286217f2e53bb067c4099c0584a8eff1d229046b9a35ae3e26/pytorch_lightning-0.8.5-py3-none-any.whl (313kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 317kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.18.5)\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276kB 19.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.6.0+cu101)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (4.41.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (3.2.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.31.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (0.34.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (49.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (0.9.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (3.1.0)\n",
            "Building wheels for collected packages: PyYAML\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=946eb1024510a59b9a2e0aa73a109466d3181115d0117f4bc1d19546bd6bbeda\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built PyYAML\n",
            "Installing collected packages: PyYAML, pytorch-lightning\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1 pytorch-lightning-0.8.5\n",
            "Collecting logzero\n",
            "  Downloading https://files.pythonhosted.org/packages/97/24/27295d318ea8976b12cf9cc51d82e7c7129220f6a3cc9e3443df3be8afdb/logzero-1.5.0-py2.py3-none-any.whl\n",
            "Installing collected packages: logzero\n",
            "Successfully installed logzero-1.5.0\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 317kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.2.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n",
            "Collecting lineflow\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/09/6e8842e1c8ace250d352cf59503e3939169d8a3abe0d17115d5916054930/lineflow-0.6.4.tar.gz\n",
            "Collecting arrayfiles\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/be/297c365c7f8304ffa949bb252eac9d67e5c708a3615c9d5b18a7613e9006/arrayfiles-0.0.1.tar.gz\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from lineflow) (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown->lineflow) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown->lineflow) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown->lineflow) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->lineflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->lineflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->lineflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->lineflow) (2020.6.20)\n",
            "Building wheels for collected packages: lineflow, arrayfiles\n",
            "  Building wheel for lineflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lineflow: filename=lineflow-0.6.4-cp36-none-any.whl size=23202 sha256=96b18331d79a0b2b91ad1f8441cbc5961a031dd8b1780207d02e39905dee3d2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/11/32/a6120f98d7d11ed8cf1b28b265a12a4b72842da341c13384c1\n",
            "  Building wheel for arrayfiles (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for arrayfiles: filename=arrayfiles-0.0.1-cp36-none-any.whl size=5434 sha256=39397acff9d0e32089144601d0aa03609d3c2d8528dbcee862641ed94a9ab81e\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/43/9b/fb9049f9c3931d703182bd45418c3e8d67a6b6eded6325e16e\n",
            "Successfully built lineflow arrayfiles\n",
            "Installing collected packages: arrayfiles, lineflow\n",
            "Successfully installed arrayfiles-0.0.1 lineflow-0.6.4\n",
            "Collecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/b0/9a6313c78bca92abfacc08a2ad8b27bfe845256f615786ee2b6452ae1978/optuna-2.0.0.tar.gz (226kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235kB 8.4MB/s \n",
            "\u001b[?25hCollecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1e/cabc75a189de0fbb2841d0975243e59bde8b7822bacbb95008ac6fe9ad47/alembic-1.4.2.tar.gz (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 14.4MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/06/03b1f92d46546a18eabf33ff7f37ef422c18c93d5a926bf590fee32ebe75/cliff-3.4.0-py3-none-any.whl (76kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 10.4MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/63/88/d5e9b78151dce671d7e78ee4cc8905d83208254caa2a386b163ae0ab0027/cmaes-0.6.0-py3-none-any.whl\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/81/12d77537c82c5d46aa2721dfee25a0e873ef5920ebd0827152f411effb57/colorlog-4.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from optuna) (20.4)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.18)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
            "Collecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/f4/041afc90e684f2b7d00a7f49abcbaf0b8c03e916bbc398ce49dce2a3c408/stevedore-3.2.0-py3-none-any.whl (42kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 8.7MB/s \n",
            "\u001b[?25hCollecting cmd2!=0.8.3,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/db/1d2e8ac038324b6b0ba3d5892185f91594152ad39e4b2fbff46792aaeac7/cmd2-1.3.3-py3-none-any.whl (130kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 32.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.15.0)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (5.3.1)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/ba/aa953a11ec014b23df057ecdbc922fdb40ca8463466b1193f3367d2711a6/pbr-5.4.5-py2.py3-none-any.whl (110kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from stevedore>=2.0.1->cliff->optuna) (1.7.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/5b/55866e1cde0f86f5eec59dab5de8a66628cb0d53da74b8dbc15ad8dabda3/pyperclip-1.8.0.tar.gz\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools>=34.4 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (49.2.0)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (19.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->stevedore>=2.0.1->cliff->optuna) (3.1.0)\n",
            "Building wheels for collected packages: alembic\n",
            "  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.2-cp36-none-any.whl size=159543 sha256=6ad879c64b3b138482562049a20db19eb685b25b0eb33a0306bce8fbc9d74485\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/04/83/76023f7a4c14688c0b5c2682a96392cfdd3ee4449eaaa287ef\n",
            "Successfully built alembic\n",
            "Building wheels for collected packages: optuna, pyperclip\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.0.0-cp36-none-any.whl size=312964 sha256=68343ceeb4b033181e69f916913fcd8822c1d92bdeb5a8cdf1900d97cdfc9461\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/c9/03/c45484454bf657ffed0ed6af153bd3d213928df115eb2a56eb\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.0-cp36-none-any.whl size=8693 sha256=76783c33aaacd59fdf755bc710fae7ca5889e95d4db51f6ee04d9f500c8446de\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/ac/0a/b784f0afe26eaf52e88a7e15c7369090deea0354fa1c6fc689\n",
            "Successfully built optuna pyperclip\n",
            "Installing collected packages: python-editor, Mako, alembic, pbr, stevedore, pyperclip, colorama, cmd2, cliff, cmaes, colorlog, optuna\n",
            "Successfully installed Mako-1.1.3 alembic-1.4.2 cliff-3.4.0 cmaes-0.6.0 cmd2-1.3.3 colorama-0.4.3 colorlog-4.2.1 optuna-2.0.0 pbr-5.4.5 pyperclip-1.8.0 python-editor-1.0.4 stevedore-3.2.0\n",
            "Collecting gdown==3.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/db/f9/757abd4b0ebf60f3d276b599046c515c070fab5161b22abb952e35f3c0a4/gdown-3.11.0.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.11.0-cp36-none-any.whl size=9619 sha256=02b16623b9ae10214d74f581482a26732582329a06a8fbe33ee0674d54635491\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/a6/67/ebb80360fc57bb0ddd5da77f57b275084cd8838bf7d5b91685\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "Successfully installed gdown-3.11.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 778kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 22.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 53.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=09e4b2b94a4834daf386594a14f762d24f0813270698458f719eea9b59dd6cbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting nlpaug\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/61/b2b3e743877361affa423167b12fc76577a1562b2da23e4154dac34bda69/nlpaug-0.0.16-py3-none-any.whl (143kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153kB 9.0MB/s \n",
            "\u001b[?25hInstalling collected packages: nlpaug\n",
            "Successfully installed nlpaug-0.0.16\n",
            "Collecting torchtest\n",
            "  Downloading https://files.pythonhosted.org/packages/42/b1/eb572ef3dce32bfcc3bf60be6360cef0d01a58bde0c76f2e205234d1ff08/torchtest-0.5-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtest) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtest) (0.18.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchtest) (1.18.5)\n",
            "Installing collected packages: torchtest\n",
            "Successfully installed torchtest-0.5\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu76gSLTpagD",
        "colab_type": "text"
      },
      "source": [
        "### Dataset statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3KMkjEDKPoE",
        "colab_type": "text"
      },
      "source": [
        "#### IMDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isBfou21d5f7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train = lfds.Imdb('train')\n",
        "# test = lfds.Imdb('test')\n",
        "# dataset = train + test\n",
        "# ds = dataset.map(lambda x: {'text': x[0], 'label':x[1], 'tokens_len': len(list(gensim_tokenizer(x[0])))})\n",
        "# df = pd.DataFrame(ds)\n",
        "# df['tokens_len'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKH958ALCaju",
        "colab_type": "text"
      },
      "source": [
        "#### MR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QwNnk0zCYTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset_path = str(Path()/\n",
        "#                         'gdrive'/\n",
        "#                         'My Drive'/\n",
        "#                         'praca_magisterska'/\n",
        "#                         'pytorch_lightning'/\n",
        "#                         'datasets'/\n",
        "#                         'mr_with_bt2.csv')\n",
        "# df = pd.read_csv(dataset_path, index_col=None)\n",
        "# df['totalwords'] = df['text'].str.split().str.len()\n",
        "# df['totalwords'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCViuaUo82Ij",
        "colab_type": "text"
      },
      "source": [
        "#### NLUHD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GdQmqGu82YI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# datasets_path = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'datasets')\n",
        "# NLU_HD_path = os.path.join(datasets_path,'NLU-Data-Home-Domain-Annotated-All.csv')\n",
        "# print(NLU_HD_path)\n",
        "# # df = pd.read_csv(str(NLU_HD_path), delimiter=';')[['intent', 'answer_annotation']]\n",
        "# df = pd.read_csv(str(NLU_HD_path), delimiter=';')[['intent', 'answer_annotation', 'scenario']]\n",
        "# df['intent'] = df[['scenario', 'intent']].agg('-'.join, axis=1) \n",
        "# del df['scenario']\n",
        "# df = df[df['answer_annotation'].notna()]\n",
        "# df = df.rename(columns={\"answer_annotation\": \"text\"})\n",
        "# nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "# df['text'] = df['text'].apply(normalize,\n",
        "#                               lowercase=True,\n",
        "#                               remove_stopwords=False,\n",
        "#                               with_ner_tags=False,\n",
        "#                               nlp=nlp) \n",
        "# # df.to_csv(os.path.join(datasets_path,'NLU-Data-Home-Domain-preprocessed-without-ner_no-scenario.csv'))\n",
        "# df.to_csv(os.path.join(datasets_path,'NLU-Data-Home-Domain-preprocessed-without-ner.csv'))\n",
        "# df['intent'].value_counts().plot(kind=\"bar\", figsize= (21,20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgVh49TICYOV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IPyAeQnwtjk",
        "colab_type": "text"
      },
      "source": [
        "### Embeders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj41DiVYK6ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_ft_embeder():\n",
        "  my_file = Path(\"./cc.en.300.bin\")\n",
        "  # my_file = Path(\"./wiki-news-300d-1M.vec.zip\")\n",
        "  if not my_file.is_file():\n",
        "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "    # !https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "    !gunzip cc.en.300.bin.gz\n",
        "  print('Loading fastText model into memory, it can take a while...')\n",
        "  start = time.time()\n",
        "  # ft = gensim.models.fasttext.load_text_format()  # Original fasttext embeddings from https://fasttext.cc/\n",
        "  ft = gensim.models.FastText.load_fasttext_format(\"./cc.en.300.bin\")\n",
        "  # ft = FT_gensim.load(\"./cc.en.300.bin\")\n",
        "  # ft = fasttext.load_model(\"./cc.en.300.bin\")\n",
        "  end = time.time()\n",
        "  duration =  end - start\n",
        "  print(f'Loading took: {duration} s')\n",
        "  return ft\n",
        "\n",
        "def create_spacy_nlp_embeder():\n",
        "  my_file = Path(\"./crawl-300d-2M.vec.zip\")\n",
        "  if not my_file.is_file():\n",
        "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip \n",
        "  my_file = Path(\"./en_vectors_wiki_lg\")\n",
        "  if not my_file.is_file():\n",
        "    !python -m spacy init-model en /en_vectors_wiki_lg --vectors-loc crawl-300d-2M.vec.zip\n",
        "  print('Loading fastText vectors into spaCy model into memory, it can take a while...')\n",
        "  start = time.time()\n",
        "  nlp = spacy.load(\"/en_vectors_wiki_lg\")\n",
        "  end = time.time()\n",
        "  duration =  end - start\n",
        "  print(f'Loading took: {duration} s')\n",
        "  return nlp \n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbMKpkCwWlXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nlp = create_spacy_nlp_embeder()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wggG_1T2gT_y",
        "colab_type": "text"
      },
      "source": [
        "### VAT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEXOJ_dmgWOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@contextlib.contextmanager\n",
        "def _disable_tracking_bn_stats(model):\n",
        "\n",
        "    def switch_attr(m):\n",
        "        if hasattr(m, 'track_running_stats'):\n",
        "            m.track_running_stats ^= True\n",
        "    model.apply(switch_attr)\n",
        "    yield\n",
        "    model.apply(switch_attr)\n",
        "\n",
        "class VATLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, xi=10.0, eps=1.0, ip=1):\n",
        "        \"\"\"VAT loss\n",
        "        :param xi: hyperparameter of VAT (default: 10.0)\n",
        "        :param eps: hyperparameter of VAT (default: 1.0)\n",
        "        :param ip: iteration times of computing adv noise (default: 1)\n",
        "        \"\"\"\n",
        "        super(VATLoss, self).__init__()\n",
        "\n",
        "        self.xi = xi\n",
        "        self.eps = eps\n",
        "        self.ip = ip\n",
        "\n",
        "    def forward(self, model, x):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            logits = model(x)\n",
        "            pred = F.softmax(logits, dim=1)\n",
        "\n",
        "        # prepare random unit tensor\n",
        "\n",
        "        d = torch.rand(x.shape).sub(0.5).to(x.device)\n",
        "        def _l2_normalize(d):\n",
        "          d_reshaped = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))\n",
        "          d /= torch.norm(d_reshaped, dim=1, keepdim=True) + 1e-8\n",
        "          return d\n",
        "\n",
        "        d = _l2_normalize(d)\n",
        "        # self.forward as model it may crush?\n",
        "        with _disable_tracking_bn_stats(model):\n",
        "            # calc adversarial direction\n",
        "          for _ in range(self.ip):\n",
        "              d.requires_grad_()\n",
        "\n",
        "              pred_hat = model(x + self.xi * d)\n",
        "              logp_hat = F.log_softmax(pred_hat, dim=1)\n",
        "              adv_distance = F.kl_div(logp_hat, pred, reduction='batchmean')\n",
        "              adv_distance.backward(retain_graph=True)\n",
        "              d = _l2_normalize(d.grad)\n",
        "              model.zero_grad()\n",
        "\n",
        "        # calc LDS\n",
        "\n",
        "        r_adv = d * self.eps\n",
        "        pred_hat = model(x + r_adv)\n",
        "        logp_hat = F.log_softmax(pred_hat, dim=1)\n",
        "        lds = F.kl_div(logp_hat, pred, reduction='batchmean')\n",
        "\n",
        "        return lds "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY4LSqfiCuZu",
        "colab_type": "text"
      },
      "source": [
        "### FixMatch Image Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCDNS6xvCtVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code in this file is adpated from\n",
        "# https://github.com/ildoonet/pytorch-randaugment/blob/master/RandAugment/augmentations.py\n",
        "# https://github.com/google-research/fixmatch/blob/master/third_party/auto_augment/augmentations.py\n",
        "# https://github.com/google-research/fixmatch/blob/master/libml/ctaugment.py\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import PIL\n",
        "import PIL.ImageOps\n",
        "import PIL.ImageEnhance\n",
        "import PIL.ImageDraw\n",
        "from PIL import Image\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "PARAMETER_MAX = 10\n",
        "\n",
        "\n",
        "def AutoContrast(img, **kwarg):\n",
        "    return PIL.ImageOps.autocontrast(img)\n",
        "\n",
        "\n",
        "def Brightness(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Brightness(img).enhance(v)\n",
        "\n",
        "\n",
        "def Color(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Color(img).enhance(v)\n",
        "\n",
        "\n",
        "def Contrast(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Contrast(img).enhance(v)\n",
        "\n",
        "\n",
        "def Cutout(img, v, max_v, bias=0):\n",
        "    if v == 0:\n",
        "        return img\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    v = int(v * min(img.size))\n",
        "    return CutoutAbs(img, v)\n",
        "\n",
        "\n",
        "def CutoutAbs(img, v, **kwarg):\n",
        "    w, h = img.size\n",
        "    x0 = np.random.uniform(0, w)\n",
        "    y0 = np.random.uniform(0, h)\n",
        "    x0 = int(max(0, x0 - v / 2.))\n",
        "    y0 = int(max(0, y0 - v / 2.))\n",
        "    x1 = int(min(w, x0 + v))\n",
        "    y1 = int(min(h, y0 + v))\n",
        "    xy = (x0, y0, x1, y1)\n",
        "    # gray\n",
        "    # color = (127, 127, 127)\n",
        "    color = (255)\n",
        "    img = img.copy()\n",
        "    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n",
        "    return img\n",
        "\n",
        "\n",
        "def Equalize(img, **kwarg):\n",
        "    return PIL.ImageOps.equalize(img)\n",
        "\n",
        "\n",
        "def Identity(img, **kwarg):\n",
        "    return img\n",
        "\n",
        "\n",
        "def Invert(img, **kwarg):\n",
        "    return PIL.ImageOps.invert(img)\n",
        "\n",
        "\n",
        "def Posterize(img, v, max_v, bias=0):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    return PIL.ImageOps.posterize(img, v)\n",
        "\n",
        "\n",
        "def Rotate(img, v, max_v, bias=0):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    return img.rotate(v)\n",
        "\n",
        "\n",
        "def Sharpness(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n",
        "\n",
        "\n",
        "def ShearX(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n",
        "\n",
        "\n",
        "def ShearY(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n",
        "\n",
        "\n",
        "def Solarize(img, v, max_v, bias=0):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    return PIL.ImageOps.solarize(img, 256 - v)\n",
        "\n",
        "\n",
        "def SolarizeAdd(img, v, max_v, bias=0, threshold=128):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    img_np = np.array(img).astype(np.int)\n",
        "    img_np = img_np + v\n",
        "    img_np = np.clip(img_np, 0, 255)\n",
        "    img_np = img_np.astype(np.uint8)\n",
        "    img = Image.fromarray(img_np)\n",
        "    return PIL.ImageOps.solarize(img, threshold)\n",
        "\n",
        "\n",
        "def TranslateX(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    v = int(v * img.size[0])\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n",
        "\n",
        "\n",
        "def TranslateY(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    v = int(v * img.size[1])\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n",
        "\n",
        "\n",
        "def _float_parameter(v, max_v):\n",
        "    return float(v) * max_v / PARAMETER_MAX\n",
        "\n",
        "\n",
        "def _int_parameter(v, max_v):\n",
        "    return int(v * max_v / PARAMETER_MAX)\n",
        "\n",
        "\n",
        "def fixmatch_augment_pool():\n",
        "    # FixMatch paper\n",
        "    augs = [(AutoContrast, None, None),\n",
        "            (Brightness, 0.9, 0.05),\n",
        "            (Color, 0.9, 0.05),\n",
        "            (Contrast, 0.9, 0.05),\n",
        "            (Equalize, None, None),\n",
        "            (Identity, None, None),\n",
        "            (Posterize, 4, 4),\n",
        "            (Rotate, 30, 0),\n",
        "            (Sharpness, 0.9, 0.05),\n",
        "            (ShearX, 0.3, 0),\n",
        "            (ShearY, 0.3, 0),\n",
        "            (Solarize, 256, 0),\n",
        "            (TranslateX, 0.3, 0),\n",
        "            (TranslateY, 0.3, 0)]\n",
        "            \n",
        "    return augs\n",
        "\n",
        "\n",
        "def my_augment_pool():\n",
        "    # Test\n",
        "    augs = [(AutoContrast, None, None),\n",
        "            (Brightness, 1.8, 0.1),\n",
        "            (Color, 1.8, 0.1),\n",
        "            (Contrast, 1.8, 0.1),\n",
        "            (Cutout, 0.2, 0),\n",
        "            (Equalize, None, None),\n",
        "            (Invert, None, None),\n",
        "            (Posterize, 4, 4),\n",
        "            (Rotate, 30, 0),\n",
        "            (Sharpness, 1.8, 0.1),\n",
        "            (ShearX, 0.3, 0),\n",
        "            (ShearY, 0.3, 0),\n",
        "            (Solarize, 256, 0),\n",
        "            (SolarizeAdd, 110, 0),\n",
        "            (TranslateX, 0.45, 0),\n",
        "            (TranslateY, 0.45, 0)]\n",
        "    return augs\n",
        "\n",
        "\n",
        "class RandAugmentPC(object):\n",
        "    def __init__(self, n, m):\n",
        "        assert n >= 1\n",
        "        assert 1 <= m <= 10\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        self.augment_pool = my_augment_pool()\n",
        "\n",
        "    def __call__(self, img):\n",
        "        ops = random.choices(self.augment_pool, k=self.n)\n",
        "        for op, max_v, bias in ops:\n",
        "            prob = np.random.uniform(0.2, 0.8)\n",
        "            if random.random() + prob >= 1:\n",
        "                img = op(img, v=self.m, max_v=max_v, bias=bias)\n",
        "        img = CutoutAbs(img, 16)\n",
        "        return img\n",
        "\n",
        "\n",
        "class RandAugmentMC(object):\n",
        "    def __init__(self, n, m):\n",
        "        assert n >= 1\n",
        "        assert 1 <= m <= 10\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        self.augment_pool = fixmatch_augment_pool()\n",
        "\n",
        "    def __call__(self, img):\n",
        "        ops = random.choices(self.augment_pool, k=self.n)\n",
        "        for op, max_v, bias in ops:\n",
        "            v = np.random.randint(1, self.m)\n",
        "            if random.random() < 0.5:\n",
        "                img = op(img, v=v, max_v=max_v, bias=bias)\n",
        "        img = CutoutAbs(img, 16)\n",
        "        return img\n",
        "\n",
        "class TransformFixImage(object):\n",
        "    def __init__(self, mean, std, n):\n",
        "        self.weak = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(size=28,\n",
        "                                  padding=int(28*0.125),\n",
        "                                  padding_mode='reflect')])\n",
        "        self.strong = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(size=28,\n",
        "                                  padding=int(28*0.125),\n",
        "                                  padding_mode='reflect'),\n",
        "            RandAugmentMC(n=n, m=10)])\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        weak = self.weak(x)\n",
        "        strong = self.strong(x)\n",
        "        return self.normalize(weak), self.normalize(strong)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfNoF2dG1yGl",
        "colab_type": "text"
      },
      "source": [
        "### FixMatch Text Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nIV2Bly11Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------------    \n",
        "# augmentations on char level \n",
        "# ---------------------------------------------    \n",
        "def print_augmentations(func, text):\n",
        "  def wrapper():\n",
        "    print(\"Augmentation function: \", func.__name__)\n",
        "    print(\"Original: \")\n",
        "    print(text)\n",
        "    augmented_text = func(text)\n",
        "    print(\"Augmention result: \")\n",
        "    print(augmented_text)\n",
        "    return augmented_text\n",
        "\n",
        "  return wrapper() \n",
        "\n",
        "def substitute_character_by_keyboard_distance(text):\n",
        "  aug = nac.KeyboardAug()\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "\n",
        "def insert_character_randomly(text):\n",
        "  aug = nac.RandomCharAug(action=\"insert\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def substitute_character_randomly(text):\n",
        "  aug = nac.RandomCharAug(action=\"substitute\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def delete_char_randomly(text):\n",
        "  aug = nac.RandomCharAug(action=\"delete\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def swap_character_randomly(text):\n",
        "  aug = nac.RandomCharAug(action=\"swap\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "\n",
        "# ---------------------------------------------    \n",
        "# augmentations on word level \n",
        "# ---------------------------------------------    \n",
        "# models - spelling_en.txt\n",
        "# model_dir with fasttext or word2vec or glove \n",
        "# model dir with tf-idf\n",
        "\n",
        "# its consume to much RAM\n",
        "# def insert_word_randomly_by_word_embeddings_similarity(text):\n",
        "#   # model_type: word2vec, glove or fasttext\n",
        "#   aug = naw.WordEmbsAug(\n",
        "#       model_type='word2vec', model_path=os.environ('WORD2VEC_MODEL_PATH'),\n",
        "#       action=\"insert\")\n",
        "#   augmented_text = aug.augment(text)\n",
        "#   print(\"Original:\")\n",
        "#   print(text)\n",
        "#   print(\"Augmented Text:\")\n",
        "#   print(augmented_text)\n",
        "#   return augmented_text\n",
        "\n",
        "\n",
        "def insert_word_by_tf_idf_similarity(text):\n",
        "  aug = naw.TfIdfAug(\n",
        "      model_path=os.environ['NLPAUG_PATH'],\n",
        "      action=\"insert\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "\n",
        "def split_word_to_two_tokens_randomly(text):\n",
        "  aug = naw.SplitAug()\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def swap_word_randomly(text):\n",
        "  aug = naw.RandomWordAug(action=\"swap\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def substitute_word_by_antonym(text):\n",
        "  aug = naw.AntonymAug()\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def substitute_word_by_spelling_mistake_words_dictionary(text):\n",
        "  aug = naw.SpellingAug(os.environ['SPELLING_PATH'])\n",
        "  augmented_text = aug.augment(text, n=1)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def insert_word_by_contextual_word_embeddings(text):\n",
        "  aug = naw.ContextualWordEmbsAug(\n",
        "      model_path='bert-base-uncased', action=\"insert\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "\n",
        "\n",
        "def subtitute_word_by_contextual_word_embeddings(text):\n",
        "  aug = naw.ContextualWordEmbsAug(\n",
        "           model_path='bert-base-uncased', action=\"substitute\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "\n",
        "\n",
        "def substitute_word_by_WordNets_synonym(text):\n",
        "  aug = naw.SynonymAug(aug_src='wordnet')\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "  \n",
        "def fixmatch_weak_augment_pool():\n",
        "    augs = [\n",
        "            substitute_character_by_keyboard_distance,\n",
        "            insert_character_randomly,\n",
        "            substitute_character_randomly,\n",
        "            delete_char_randomly,\n",
        "            swap_character_randomly,\n",
        "            # insert_word_randomly_by_word_embeddings_similarity,\n",
        "            insert_word_by_tf_idf_similarity,\n",
        "            split_word_to_two_tokens_randomly,\n",
        "            swap_word_randomly,\n",
        "            substitute_word_by_antonym,\n",
        "            substitute_word_by_spelling_mistake_words_dictionary,\n",
        "            insert_word_by_contextual_word_embeddings,\n",
        "            subtitute_word_by_contextual_word_embeddings,\n",
        "            substitute_word_by_WordNets_synonym,\n",
        "           ]\n",
        "\n",
        "    return augs\n",
        "\n",
        "# def fixmatch_strong_augment_pool():\n",
        "#     augs = [\n",
        "#             insert_word_by_contextual_word_embeddings,\n",
        "#             subtitute_word_by_contextual_word_embeddings,\n",
        "#             substitute_word_by_WordNets_synonym,\n",
        "#            ]\n",
        "\n",
        "#     return augs\n",
        "\n",
        "\n",
        "class WeakRandAugment(object):\n",
        "  def __init__(self, n, show=False):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    n (int): number of operations\n",
        "\n",
        "    \"\"\"\n",
        "    assert n >= 0\n",
        "    self.n = n\n",
        "    self.augment_pool = fixmatch_weak_augment_pool()\n",
        "    self.show=show\n",
        "\n",
        "  def __call__(self, text):\n",
        "    if self.n <= 0:\n",
        "      return text\n",
        "    ops = random.choices(self.augment_pool, k=self.n)\n",
        "    for op in ops:\n",
        "        if random.random() < 1.:\n",
        "          if self.show:\n",
        "            text = print_augmentations(op, text)\n",
        "          else:\n",
        "            text = op(text)\n",
        "    return text \n",
        "\n",
        "# not necessery\n",
        "# class StrongRandAugment(object):\n",
        "#   def __init__(self, n, show=False):\n",
        "#     assert n >= 1\n",
        "#     self.n = n\n",
        "#     self.augment_pool = fixmatch_strong_augment_pool()\n",
        "#     self.show= show\n",
        "\n",
        "#   def __call__(self, text):\n",
        "#     ops = random.choices(self.augment_pool, k=self.n)\n",
        "#     for op in ops:\n",
        "#       if random.random() < 1.:\n",
        "#         if self.show:\n",
        "#           text = print_augmentations(op, text)\n",
        "#         else:\n",
        "#           text = op(text)\n",
        "#     return text \n",
        "\n",
        "\n",
        "class TransformFix(object):\n",
        "  def __init__(self, n_weak=3, show=False):\n",
        "  # def __init__(self, n_weak=3, n_strong=2, show=False):\n",
        "    self.weak = WeakRandAugment(n=n_weak, show=show) \n",
        "    # self.strong = StrongRandAugment(n=n_strong, show=show)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    weak = self.weak(x)\n",
        "    # strong = self.strong(x)\n",
        "    return weak #, strong\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKpMf7Gz3CpR",
        "colab_type": "text"
      },
      "source": [
        "### Datasets Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkdrQH7a3FKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoInOneDataset(Dataset):\n",
        "    def __init__(self, datasets):\n",
        "        self.datasets = datasets\n",
        "\n",
        "        self.map_indexes = [[] for _ in self.datasets]\n",
        "\n",
        "        self.min_length = min(len(d) for d in self.datasets)\n",
        "        self.max_length = max(len(d) for d in self.datasets)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return tuple(d[m[i]] for d, m in zip(self.datasets, self.map_indexes))\n",
        "\n",
        "    def construct_map_index(self):\n",
        "        def update_indices(original_indexes, target_len, max_len):\n",
        "            # map max_len to target_len (large to small)\n",
        "\n",
        "            # return: a list, which maps the range(max_len) to the valid index in the dataset\n",
        "            \n",
        "            original_indexes = original_indexes[max_len:] # remove used indices\n",
        "            fill_num = max_len - len(original_indexes)\n",
        "            batch = fill_num // target_len\n",
        "\n",
        "            if fill_num % target_len != 0:\n",
        "                # to let the fill_num + len(original_indexes) greater than max_len\n",
        "                batch += 1\n",
        "\n",
        "            additional_indexes = list(range(target_len)) * batch\n",
        "            random.shuffle(additional_indexes)\n",
        "\n",
        "            original_indexes += additional_indexes\n",
        "\n",
        "            assert len(original_indexes) >= max_len, \"the length of matcing indexes is too small\"\n",
        "\n",
        "            return original_indexes\n",
        "\n",
        "        self.map_indexes = [update_indices(m, len(d), self.max_length) \n",
        "            for m, d in zip(self.map_indexes, self.datasets)]\n",
        "\n",
        "    def __len__(self):\n",
        "        # will be called every epoch\n",
        "        self.construct_map_index()\n",
        "        return self.max_length\n",
        "        \n",
        "class SimpleTextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, x, y, transform=None):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # special dict convention for f: process_NLUHD \n",
        "        data_dict = { 'text': self.x[index], 'label': self.y[index]}\n",
        "        if self.transform is not None:\n",
        "          return self.transform(data_dict)\n",
        "        return tuple(data_dict.values()) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "class FixMatchAugmentedTextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, x, x_paraphrases, y,\n",
        "                 model_preprocessing = None,\n",
        "                 fix_match_augmentation = None, show=False):\n",
        "        self.x = x\n",
        "        self.x_paraphrases = x_paraphrases\n",
        "        self.y = y\n",
        "        self.model_preprocessing = model_preprocessing\n",
        "        self.fix_match_augmentation = fix_match_augmentation\n",
        "        self.show = show\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # special dict convention for f: process_NLUHD \n",
        "        if self.fix_match_augmentation is not None:\n",
        "          weak_augmented, strong_augmented = \\\n",
        "           self.fix_match_augmentation(self.x[index]), self.x_paraphrases[index] \n",
        "          if self.show:\n",
        "            print_augmentations(lambda x: x, self.x_paraphrases[index])\n",
        "        \n",
        "\n",
        "        weak_aug_data_dict = { 'text': weak_augmented , 'label': self.y[index]}\n",
        "        strong_aug_data_dict = { 'text': strong_augmented , 'label': self.y[index]}\n",
        "\n",
        "        if self.model_preprocessing is not None:\n",
        "          return self.model_preprocessing(weak_aug_data_dict), self.model_preprocessing(strong_aug_data_dict)\n",
        "\n",
        "        return tuple(weak_aug_data_dict.values()), tuple(strong_aug_data_dict.values()) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "class FixMatchAugmentedImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, x, y,\n",
        "                 model_preprocessing = None,\n",
        "                 fix_match_augmentation = None,\n",
        "                 show=False):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.model_preprocessing = model_preprocessing\n",
        "        self.fix_match_augmentation = fix_match_augmentation\n",
        "        self.show = show\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # special dict convention for f: process_NLUHD \n",
        "        if self.fix_match_augmentation is not None:\n",
        "          weak_augmented, strong_augmented = \\\n",
        "           self.fix_match_augmentation(self.x[index]) \n",
        "          if self.show:\n",
        "            print_image_augmentations(lambda x: x, self.x[index])\n",
        "            print_image_augmentations(lambda x: x, weak_augmented)\n",
        "            print_image_augmentations(lambda x: x, strong_augmented)\n",
        "        \n",
        "        weak_aug_data_dict = { 'text': weak_augmented , 'label': self.y[index]}\n",
        "        strong_aug_data_dict = { 'text': strong_augmented , 'label': self.y[index]}\n",
        "\n",
        "        if self.model_preprocessing is not None:\n",
        "          return self.model_preprocessing(weak_aug_data_dict), self.model_preprocessing(strong_aug_data_dict)\n",
        "\n",
        "        return tuple(weak_aug_data_dict.values()), tuple(strong_aug_data_dict.values()) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA1ZSYrtFVK6",
        "colab_type": "text"
      },
      "source": [
        "### Datasets related processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsGvdNvB90p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_NLUHD(comment, nlp,  ner_abstract_tag: bool=True):\n",
        "  expression = r\"\\[.*?\\]\"\n",
        "  matches = []\n",
        "  for match in re.finditer(expression, comment.text):\n",
        "      start, end = match.span()\n",
        "      span = comment.char_span(start, end)\n",
        "      # This is a Span object or None if match doesn't map to valid token sequence\n",
        "      if span is not None:\n",
        "          # print(\"Found match:\", span.text)\n",
        "          if ner_abstract_tag:\n",
        "            expression_scd = r\"\\[.*?\\:\"\n",
        "          else:\n",
        "            expression_scd = r\"\\:.*?\\]\"\n",
        "\n",
        "          temp_doc = nlp(span.text)\n",
        "          scd_match = next(re.finditer(expression_scd, temp_doc.text))\n",
        "          start1, end1 = scd_match.span()\n",
        "          # print(start1, end1)\n",
        "          s1 = int(start1) + 1\n",
        "          e1 = int(end1) - 1\n",
        "          # print(type(e1))\n",
        "          replace_str = temp_doc.text[s1:e1].strip()\n",
        "          # scd_doc = temp_doc.char_span(start1 + 1, end1 - 2) \n",
        "          matches += [((start, end), replace_str)]\n",
        "\n",
        "  start_line = 0\n",
        "  new_comment = \"\"\n",
        "  for match in matches:\n",
        "    s = match[0][0]\n",
        "    e = match[0][1]\n",
        "    replace_word = match[1]\n",
        "    new_comment += comment.text[start_line:s] + replace_word \n",
        "    start_line = e\n",
        "  new_comment += comment.text[start_line:]\n",
        "  ret_val = nlp(new_comment)\n",
        "  return ret_val\n",
        "\n",
        "def preprocess_NLUHD(lowercase,\n",
        "                     remove_stopwords,\n",
        "                     with_ner_tags,\n",
        "                     nlp,\n",
        "                     label_encoder,\n",
        "                     sample):\n",
        "  \n",
        "  stops = stopwords.words(\"english\")\n",
        "  comment = sample['text']\n",
        "  if lowercase:\n",
        "      comment = comment.lower()\n",
        "  comment = nlp(comment)\n",
        "  if with_ner_tags is True:\n",
        "    comment = prepare_NLUHD(comment, ner_abstract_tag=True, nlp=nlp)\n",
        "  else:\n",
        "    comment = prepare_NLUHD(comment, ner_abstract_tag=False, nlp=nlp)\n",
        "  lemmatized = list()\n",
        "  if remove_stopwords:\n",
        "    for word in comment:\n",
        "        lemma = word.lemma_.strip()\n",
        "        if lemma:\n",
        "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
        "                lemmatized.append(lemma)\n",
        "    processed_text = \" \".join(lemmatized) \n",
        "  processed_text = comment.text\n",
        "  encoded_label = torch.tensor(int(label_encoder.transform([sample[\"label\"]])))\n",
        "  return {\"text\": processed_text,\n",
        "          \"label\": encoded_label}\n",
        "\n",
        "def preprocess_IMDB(label_encoder, sample: Dict):\n",
        "  pattern1 = re.compile(r'<.*?>')\n",
        "  # pattern2 = re.compile('[\\W_]+ ')\n",
        "  # text = pattern2.sub(' ', text)\n",
        "  text = pattern1.sub('', sample['text']).lower()\n",
        "  encoded_label = torch.tensor(int(label_encoder.transform([sample[\"label\"]])))\n",
        "  return {\"text\": text,\n",
        "          \"label\": encoded_label}\n",
        "          \n",
        "def preprocess_MR(label_encoder, sample: Dict):\n",
        "  pattern1 = re.compile(r'<.*?>')\n",
        "  # pattern2 = re.compile('[\\W_]+ ')\n",
        "  # text = pattern2.sub(' ', text)\n",
        "  text = pattern1.sub('', sample['text']).lower()\n",
        "  encoded_label = torch.tensor(int(label_encoder.transform([sample[\"label\"]])))\n",
        "  return {\"text\": text,\n",
        "          \"label\": encoded_label}\n",
        "        \n",
        "def preprocess_MNIST(label_encoder, sample: Dict):\n",
        "  encoded_label = torch.tensor(int(label_encoder.transform([sample[\"label\"]])))\n",
        "  x = sample['text'].float().unsqueeze(0)\n",
        "  return {\"text\": x,\n",
        "          \"label\": encoded_label}\n",
        "          \n",
        "def preprocess_MNIST_FixMatch(label_encoder, sample: Dict):\n",
        "  encoded_label = torch.tensor(int(label_encoder.transform([sample[\"label\"]])))\n",
        "  x = sample['text'].float()\n",
        "  return {\"text\": x,\n",
        "          \"label\": encoded_label}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGCcDLd_-udN",
        "colab_type": "text"
      },
      "source": [
        "### Model related processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvkT6fi64bEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transformer_preprocessing(model_type: str,\n",
        "                              MAX_LEN: int,\n",
        "                              tokenizer: BertTokenizer,\n",
        "                              sample:Dict,) -> Dict:\n",
        "  \n",
        "    inputs = tokenizer.encode_plus(\n",
        "            sample[\"text\"],\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            )\n",
        "    # Output of `tokenizer.encode_plus` is a dictionary.\n",
        "    if model_type == 'roberta-base':\n",
        "      token_type_ids = [] \n",
        "    else:\n",
        "      input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
        "    # For BERT, we need `attention_mask` along with `input_ids` as input.\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "    # We are going to pad sequences.\n",
        "    padding_length = MAX_LEN - len(input_ids)\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "    input_ids = input_ids + ([pad_id] * padding_length)\n",
        "    attention_mask = attention_mask + ([0] * padding_length)\n",
        "    token_type_ids = token_type_ids + ([pad_id] * padding_length)\n",
        "\n",
        "    assert len(input_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(input_ids), MAX_LEN)\n",
        "    assert len(attention_mask) == MAX_LEN, \"Error with input length {} vs {}\".format(len(attention_mask), MAX_LEN)\n",
        "    assert len(token_type_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(token_type_ids), MAX_LEN)\n",
        "\n",
        "    # Just a python list to `torch.tensor`\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "    token_type_ids = torch.tensor(token_type_ids)\n",
        "\n",
        "    # What we return will one instance in batch which `LightningModule.train_step` receives.\n",
        "    return {\n",
        "            \"label\": sample['label'],\n",
        "            \"embedding\": {\n",
        "                          \"input_ids\": input_ids,\n",
        "                          \"attention_mask\": attention_mask,\n",
        "                          \"token_type_ids\": token_type_ids\n",
        "                         }\n",
        "            }\n",
        "\n",
        "def generate_embeddings(\n",
        "                         hparams,\n",
        "                         tokenizer,\n",
        "                         embeder,\n",
        "                         sample):\n",
        "\n",
        "  embedding = torch.Tensor([token.vector for token in embeder(sample[\"text\"])])#torch.tensor(embeder.wv[tokens])\n",
        "\n",
        "  if embedding.size()[0] >= hparams['max_sentence_len']:\n",
        "    embedding = torch.narrow(embedding, 0, 0, hparams['max_sentence_len'])\n",
        "  else:\n",
        "    padding_length = hparams['max_sentence_len'] - len(embedding)\n",
        "    padding_vectors = torch.zeros((padding_length, hparams['embed_dim']))\n",
        "    embedding = torch.cat((embedding, padding_vectors)) \n",
        "\n",
        "  return {'label': sample['label'],\n",
        "          'embedding': embedding}\n",
        "\n",
        "def adjust_MNIST_to_pipeline(sample: Dict):\n",
        "  return {'label': sample['label'],\n",
        "          'embedding': sample['text']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7_n1xgQqHAq",
        "colab_type": "text"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH4XjCAXlRDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_global_metric(outputs, metric):\n",
        "    return sum([out[metric] for out in outputs]) / len(outputs)\n",
        "\n",
        "def create_metrics_dict(phase_type: str, loss, labels, labels_hat, max_probs) -> dict:\n",
        "\n",
        "    output_dict = {f'{phase_type}_loss': loss}\n",
        "    \n",
        "    metrics_keys = [\n",
        "                    f'{phase_type}_accuracy_error',\n",
        "                    f'{phase_type}_f1_error',\n",
        "                    f'{phase_type}_recall_error',\n",
        "                    f'{phase_type}_precision_error'\n",
        "                   ]\n",
        "\n",
        "    error_map = lambda x: 1-x\n",
        "\n",
        "    error_metrics_values = juxt(\n",
        "                                compose(error_map, partial(accuracy_score)),\n",
        "                                compose(error_map, partial(f1_score, **{'average':'micro'})),\n",
        "                                compose(error_map, partial(recall_score, **{'average':'micro'})),\n",
        "                                compose(error_map, partial(precision_score, **{'average':'micro'}))\n",
        "                               )(labels, labels_hat)\n",
        "\n",
        "\n",
        "    output_dict.update(dict(zip(metrics_keys, error_metrics_values)))\n",
        "\n",
        "    confident_keys = [\n",
        "                      f'{phase_type}_max_confident',\n",
        "                      f'{phase_type}_min_confident',\n",
        "                      f'{phase_type}_mean_confident',\n",
        "                      f'{phase_type}_std_confident'\n",
        "                     ]\n",
        "    confident_values = toolz.juxt(\n",
        "                                  np.max,\n",
        "                                  np.min, \n",
        "                                  np.mean,\n",
        "                                  np.std,\n",
        "                                 )(max_probs)\n",
        "\n",
        "    output_dict.update(dict(zip(confident_keys, confident_values)))\n",
        "\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def tfidf_tokenizer(text, token_pattern=r\"(?u)\\b\\w\\w+\\b\"):\n",
        "    token_pattern = re.compile(token_pattern)\n",
        "    return token_pattern.findall(text)\n",
        "\n",
        "def create_tfidf_model(df: pd.DataFrame):  \n",
        "  train_data = df['text'] \n",
        "  train_x = train_data.values\n",
        "    \n",
        "  train_x_tokens = [tfidf_tokenizer(x) for x in train_x]\n",
        "    \n",
        "  tfidf_model = nmw.TfIdf()\n",
        "  tfidf_model.train(train_x_tokens)\n",
        "  tfidf_model.save(os.environ['NLPAUG_PATH'])\n",
        "  os.environ['TFIDF_MODEL_PATH']  = os.path.join(os.environ['NLPAUG_PATH'], 'tfidfaug_w2tfidf.txt')\n",
        "  os.listdir(os.environ['NLPAUG_PATH'])\n",
        "\n",
        "class MetricsCallback(Callback):\n",
        "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.metrics = []\n",
        "\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        self.metrics.append(trainer.callback_metrics)\n",
        "\n",
        "# credits: https://github.com/galatolofederico/pytorch-balanced-batch/blob/master/sampler.py        \n",
        "class BalancedBatchSampler(torch.utils.data.sampler.Sampler):\n",
        "    def __init__(self, dataset, labels=None):\n",
        "        self.labels = labels\n",
        "        self.dataset = dict()\n",
        "        self.balanced_max = 0\n",
        "        # Save all the indices for all the classes\n",
        "        for idx in range(0, len(dataset)):\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label not in self.dataset:\n",
        "                self.dataset[label] = list()\n",
        "            self.dataset[label].append(idx)\n",
        "            self.balanced_max = len(self.dataset[label]) \\\n",
        "                if len(self.dataset[label]) > self.balanced_max else self.balanced_max\n",
        "        \n",
        "        # Oversample the classes with fewer elements than the max\n",
        "        for label in self.dataset:\n",
        "            while len(self.dataset[label]) < self.balanced_max:\n",
        "                self.dataset[label].append(random.choice(self.dataset[label]))\n",
        "        self.keys = list(self.dataset.keys())\n",
        "        self.currentkey = 0\n",
        "        self.indices = [-1]*len(self.keys)\n",
        "\n",
        "    def __iter__(self):\n",
        "        while self.indices[self.currentkey] < self.balanced_max - 1:\n",
        "            self.indices[self.currentkey] += 1\n",
        "            yield self.dataset[self.keys[self.currentkey]][self.indices[self.currentkey]]\n",
        "            self.currentkey = (self.currentkey + 1) % len(self.keys)\n",
        "        self.indices = [-1]*len(self.keys)\n",
        "    \n",
        "    def _get_label(self, dataset, idx, labels = None):\n",
        "        if self.labels is not None:\n",
        "            return self.labels[idx].item()\n",
        "        else:\n",
        "            raise Exception(\"You should pass the tensor of labels to the constructor as second argument\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.balanced_max*len(self.keys)\n",
        "\n",
        "def size_splits(tensor, split_sizes, dim=0):\n",
        "    \"\"\"Splits the tensor according to chunks of split_sizes.\n",
        "    \n",
        "    Arguments:\n",
        "        tensor (Tensor): tensor to split.\n",
        "        split_sizes (list(int)): sizes of chunks\n",
        "        dim (int): dimension along which to split the tensor.\n",
        "    \"\"\"\n",
        "    if dim < 0:\n",
        "        dim += tensor.dim()\n",
        "    \n",
        "    dim_size = tensor.size(dim)\n",
        "    if dim_size != torch.sum(torch.Tensor(split_sizes)):\n",
        "        raise KeyError(\"Sum of split sizes exceeds tensor dim\")\n",
        "    \n",
        "    splits = torch.cumsum(torch.Tensor([0] + split_sizes), dim=0)[:-1]\n",
        "\n",
        "    return tuple(tensor.narrow(int(dim), int(start), int(length)) \n",
        "        for start, length in zip(splits, split_sizes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShfV1EwDrX97",
        "colab_type": "text"
      },
      "source": [
        "### Composable Framework "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9CPFT7ormCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LitComposableFramework(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, hparams):\n",
        "\n",
        "    super().__init__()\n",
        "    self.hparams = hparams\n",
        "    self.num_classes = hparams['num_classes']\n",
        "    self.total_iterations = 0 \n",
        "    self.loss_fct = getattr(nn, hparams['loss_function'])()\n",
        "\n",
        "\n",
        "    if self.hparams['model_arch'] == \"Convolution\":\n",
        "\n",
        "      self.embeder_dict = {\n",
        "                          'fastText': (create_ft_embeder, gensim_tokenizer),\n",
        "                          'spaCy':(create_spacy_nlp_embeder, lambda x: x)\n",
        "                          }\n",
        "      embeder, self.tokenizer_fun = self.embeder_dict[hparams['embeder_type']]\n",
        "      self.embeder = nlp #embeder#nlp # hardcoded\n",
        "      self.D = hparams['embed_dim']\n",
        "      self.Ci = hparams['Ci'] \n",
        "      self.Co = hparams['kernel_num']\n",
        "      self.Ks = list(map(int, hparams['kernel_sizes'].split(','))) # (3,4,5)\n",
        "      self.convs1 = nn.ModuleList([nn.Conv2d(self.Ci, self.Co, (K, self.D)) for K in self.Ks])\n",
        "      self.dropout = nn.Dropout(hparams['dropout'])\n",
        "      self.fc1 = nn.Linear(len(self.Ks) * self.Co, self.num_classes) \n",
        "\n",
        "    elif self.hparams['model_arch'] == 'ConvolutionMNIST':\n",
        "\n",
        "      self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "      self.dropout1 = nn.Dropout2d(0.25)\n",
        "      self.dropout2 = nn.Dropout2d(0.5)\n",
        "      self.fc1 = nn.Linear(9216, 128)\n",
        "      self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "\n",
        "    elif self.hparams['model_arch'] == \"Transformer\":\n",
        "      self.model_class_dict = {\n",
        "            \"bert-base-uncased\": BertModel,\n",
        "            \"roberta-base\": RobertaModel\n",
        "            }\n",
        "              \n",
        "      self.tokenizer_dict = {\n",
        "              \"bert-base-uncased\":\n",
        "                BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
        "                                              do_lower_case=True),\n",
        "              \"roberta-base\":\n",
        "                RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "              }\n",
        "  \n",
        "      self.model = self.model_class_dict[self.hparams['model_type']].from_pretrained(self.hparams['model_type'],\n",
        "                                                                output_attentions=True)\n",
        "      self.encoder_features =  self.model.config.hidden_size \n",
        "      print('encoder features: ', self.encoder_features)\n",
        "      self.num_classes = self.hparams['num_classes']\n",
        "      self.classification_head = nn.Sequential(\n",
        "            nn.Linear(self.encoder_features, self.encoder_features * 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.encoder_features * 2, self.encoder_features),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.encoder_features, self.num_classes),\n",
        "        )\n",
        "      \n",
        "    else:\n",
        "      raise ValueError('Wrong model architecture type: {} \\\n",
        "       \\n Possible datasets: Transformer, Convolution'.format(self.hparams['model_arch']))\n",
        "\n",
        "  def forward(self, x, embeddings_only=False):\n",
        "\n",
        "      if self.hparams['model_arch'] == \"Convolution\":\n",
        "\n",
        "        if embeddings_only == True:\n",
        "          logits = x\n",
        "\n",
        "        else:\n",
        "          x = x.unsqueeze(self.Ci)  # (N, Ci, W, D)\n",
        "          x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
        "          x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "          x = torch.cat(x, 1)\n",
        "          x = self.dropout(x)  # (N, len(Ks)*Co)\n",
        "          logits = self.fc1(x)  # (N, C)\n",
        "\n",
        "      elif self.hparams['model_arch'] == 'ConvolutionMNIST':\n",
        "\n",
        "        if embeddings_only == True:\n",
        "          logits = x\n",
        "        else:\n",
        "          x = self.conv1(x)\n",
        "          x = F.relu(x)\n",
        "          x = self.conv2(x)\n",
        "          x = F.relu(x)\n",
        "          x = F.max_pool2d(x, 2)\n",
        "          x = self.dropout1(x)\n",
        "          x = torch.flatten(x, 1)\n",
        "          x = self.fc1(x)\n",
        "          x = F.relu(x)\n",
        "          x = self.dropout2(x)\n",
        "          logits = self.fc2(x)\n",
        "\n",
        "      elif self.hparams['model_arch'] == \"Transformer\":\n",
        "        h, _, _ = self.model(x['input_ids'],\n",
        "                             attention_mask=x['attention_mask'],\n",
        "                             token_type_ids=x['token_type_ids'] if self.hparams['model_type'] != \"roberta-base\" else None)\n",
        "        h_cls = h[:, 0]\n",
        "\n",
        "        if embeddings_only == True:\n",
        "          logits = h_cls\n",
        "        else:\n",
        "          logits = self.classification_head(h_cls)\n",
        "\n",
        "      else:\n",
        "        raise ValueError('Wrong model architecture type: {} \\\n",
        "         \\n Possible datasets: Transformer, Convolution, ConvolutionMNIST'.format(self.hparams['model_arch']))\n",
        "\n",
        "      return logits\n",
        "\n",
        "\n",
        "  def prepare_data(self):\n",
        "\n",
        "    if self.hparams['dataset'] == 'NLUHD':\n",
        "\n",
        "      not_none = lambda x: x[\"text\"] is not None \n",
        "      ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "      unique_labels = list(pd.DataFrame(ds).intent.unique())\n",
        "      self.le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      print(f\"Unique labels: {unique_labels}\")\n",
        "      print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                            int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                            seed=self.hparams['seed'])\n",
        "\n",
        "      nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "      dataset_preprocessor = partial(preprocess_NLUHD,\n",
        "                                     lowercase=True,\n",
        "                                     remove_stopwords=True,\n",
        "                                     with_ner_tags=False,\n",
        "                                     nlp=nlp,\n",
        "                                     label_encoder=le,\n",
        "      )\n",
        "\n",
        "      train_df, test_df = pd.DataFrame(train), pd.DataFrame(test)\n",
        "      # maybe here you can shuffle with different seed\n",
        "      x_train, y_train = train_df['text'].values, train_df['label'].values\n",
        "      self.x_test, self.y_test = test_df['text'].values, test_df['label'].values\n",
        "\n",
        "      dataset_preprocessor = partial(\n",
        "                                     preprocess_MR,\n",
        "                                     self.le,\n",
        "                                    )\n",
        "\n",
        "    elif self.hparams['dataset'] == 'MR':\n",
        "\n",
        "      not_none = lambda x: x[\"text\"] is not None \n",
        "      ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "      df = pd.DataFrame(ds)\n",
        "\n",
        "      if self.hparams['training_method'] == \"FixMatch\":\n",
        "        create_tfidf_model(df)\n",
        "\n",
        "      unique_labels = list(df.label.unique())\n",
        "      self.le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      print(f\"Unique labels: {unique_labels}\")\n",
        "      print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                            int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                            seed=self.hparams['seed'])\n",
        "      \n",
        "      train_df, test_df = pd.DataFrame(train), pd.DataFrame(test)\n",
        "      # maybe here you can shuffle with different seed\n",
        "      x_train, y_train = train_df['text'].values, train_df['label'].values\n",
        "      self.x_test, self.y_test = test_df['text'].values, test_df['label'].values\n",
        "\n",
        "      dataset_preprocessor = partial(\n",
        "                                     preprocess_MR,\n",
        "                                     self.le,\n",
        "                                    )\n",
        "      \n",
        "    elif self.hparams['dataset'] == 'IMDB':\n",
        "      not_none = lambda x: x[\"text\"] is not None \n",
        "      ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "      # ds = lfds.Imdb('train') + lfds.Imdb('test')\n",
        "      # ds = ds.map(lambda x: {'text': x[0], 'label': x[1]})\n",
        "      df = pd.DataFrame(ds)\n",
        "\n",
        "      if self.hparams['training_method'] == \"FixMatch\":\n",
        "        create_tfidf_model(df)\n",
        "\n",
        "      print(df.info(memory_usage=True))\n",
        "      unique_labels = list(df.label.unique())\n",
        "      print(f'unique_labels: {unique_labels}')\n",
        "      print(f'number_of_categories : {len(unique_labels)}')\n",
        "      self.le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                             int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                             seed=self.hparams['seed'])\n",
        "      dataset_preprocessor = partial(\n",
        "                                     preprocess_IMDB,\n",
        "                                     self.le,\n",
        "                                    )\n",
        "      \n",
        "      train_df, test_df = pd.DataFrame(train), pd.DataFrame(test)\n",
        "      # maybe here you can shuffle with different seed\n",
        "      x_train, y_train = train_df['text'].values, train_df['label'].values\n",
        "      self.x_test, self.y_test = test_df['text'].values, test_df['label'].values\n",
        "    \n",
        "    elif self.hparams['dataset'] == 'MNIST':\n",
        "\n",
        "      train_ds = torchvision.datasets.MNIST('./',\n",
        "                                          train=True, transform=None,\n",
        "                                          target_transform=None, download=True)\n",
        "      \n",
        "      test_ds = torchvision.datasets.MNIST('./',\n",
        "                                        train=False, transform=None,\n",
        "                                        target_transform=None, download=True)\n",
        "      \n",
        "      train_x, train_y  = train_ds.data, train_ds.targets\n",
        "      test_x, test_y  = test_ds.data, test_ds.targets\n",
        "      full_x, full_y = torch.cat((train_x, test_x), 0), torch.cat((train_y, test_y), 0)\n",
        "      \n",
        "      train_size = int(self.hparams['train_test_split'] * full_x.shape[0])\n",
        "      test_size = full_x.shape[0] - train_size\n",
        "      \n",
        "      x_train, self.x_test = size_splits(full_x,\n",
        "                                [train_size, test_size])\n",
        "      \n",
        "      y_train, self.y_test = size_splits(full_y,\n",
        "                                [train_size, test_size])\n",
        "      # check columns names x -> text, y -> label\n",
        "      unique_labels = list(np.array(torch.unique(train_y)))\n",
        "      print(f'unique_labels: {unique_labels}')\n",
        "      print(f'number_of_categories : {len(unique_labels)}')\n",
        "      self.le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "\n",
        "      dataset_preprocessor = partial(\n",
        "                                     preprocess_MNIST, # label encoding\n",
        "                                     self.le,\n",
        "                                    )\n",
        "      \n",
        "    else:\n",
        "      raise ValueError('Wrong dataset name : {} \\\n",
        "       \\n Possible datasets: IMDB, MR, NLUHD'.format(self.hparams['dataset']))\n",
        "\n",
        "    if self.hparams['model_arch'] == 'Transformer':\n",
        "\n",
        "      model_arch_preprocessor = partial(\n",
        "                                        transformer_preprocessing,\n",
        "                                        self.hparams['model_type'],\n",
        "                                        self.hparams['max_sentence_len'],\n",
        "                                        self.tokenizer_dict[self.hparams['model_type']],\n",
        "                                       )\n",
        "      \n",
        "    elif self.hparams['model_arch'] == 'Convolution':\n",
        "\n",
        "      model_arch_preprocessor = partial(\n",
        "                                        generate_embeddings,\n",
        "                                        self.hparams,\n",
        "                                        self.tokenizer_fun,\n",
        "                                        self.embeder,\n",
        "                                       )\n",
        "      \n",
        "    elif self.hparams['model_arch'] == 'ConvolutionMNIST':\n",
        "\n",
        "      model_arch_preprocessor = partial(adjust_MNIST_to_pipeline) # change text for embedding\n",
        "    \n",
        "    else:\n",
        "      raise ValueError('Wrong model architecture type: {} \\\n",
        "       \\n Possible architectures: Convolution, ConvolutionMNIST, Transformer'.format(self.hparams['model_arch']))\n",
        "      \n",
        "    preprocessor = toolz.compose(\n",
        "                                 model_arch_preprocessor,\n",
        "                                 dataset_preprocessor,\n",
        "                                )\n",
        "\n",
        "    # split's training parameters  \n",
        "    num_classes = len(unique_labels)\n",
        "    label_per_class = self.hparams['n_labeled'] // num_classes\n",
        "    valid_size = self.hparams['valid_size_per_class']\n",
        "\n",
        "    labeled_idx = []\n",
        "    unlabeled_idx = []\n",
        "    val_idx = []\n",
        "    \n",
        "    for label in unique_labels:\n",
        "        idx = np.where(y_train == label)[0]\n",
        "        np.random.shuffle(idx)\n",
        "        labeled_idx.extend(idx[:label_per_class])\n",
        "        val_idx.extend(idx[label_per_class: label_per_class + valid_size])\n",
        "        unlabeled_idx.extend(idx[label_per_class + valid_size:])\n",
        "\n",
        "    self.x_labeled, self.y_labeled  = x_train[labeled_idx], y_train[labeled_idx]\n",
        "    self.x_unlabeled, self.y_unlabeled = x_train[unlabeled_idx], y_train[unlabeled_idx]\n",
        "    self.x_val, self.y_val = x_train[val_idx], y_train[val_idx]\n",
        "\n",
        "    \n",
        "    self._train_labeled_dataset = SimpleTextDataset(self.x_labeled,\n",
        "                                                    self.y_labeled,\n",
        "                                                    transform=preprocessor)\n",
        "    if self.hparams['training_method'] == 'FixMatch' and self.hparams['dataset'] == 'MNIST' :\n",
        "\n",
        "      dataset_preprocessor = partial(\n",
        "                                     preprocess_MNIST_FixMatch, # label encoding\n",
        "                                     self.le,\n",
        "      )\n",
        "                                    \n",
        "      preprocessor_FixMatch = toolz.compose(\n",
        "                                   model_arch_preprocessor,\n",
        "                                   dataset_preprocessor,\n",
        "                                  )\n",
        "      \n",
        "      self._train_unlabeled_dataset = \\\n",
        "         FixMatchAugmentedImageDataset(x=self.x_unlabeled,\n",
        "                                       y=self.y_unlabeled,\n",
        "                                       model_preprocessing=preprocessor_FixMatch,\n",
        "                                       show=self.hparams['show_augmentation'],\n",
        "                                       fix_match_augmentation=TransformFixImage(\n",
        "                                              self.hparams['mean'],\n",
        "                                              self.hparams['std'],\n",
        "                                              self.hparams['n_strong']\n",
        "                                              )\n",
        "                                      )\n",
        "    elif self.hparams['training_method'] == 'FixMatch':\n",
        "      x_unlabeled_paraphrases = train_df['paraphrases'].values\n",
        "      ## implement showing weakly and strongly perturbated images\n",
        "      self._train_unlabeled_dataset = \\\n",
        "         FixMatchAugmentedTextDataset(self.x_unlabeled,\n",
        "                                      x_unlabeled_paraphrases[unlabeled_idx],\n",
        "                                      self.y_unlabeled,\n",
        "                                      model_preprocessing=preprocessor,\n",
        "                                      show=self.hparams['show_augmentation'],\n",
        "                                      fix_match_augmentation=TransformFixImage(\n",
        "                                        n_weak=self.hparams['n_weak'],\n",
        "                                        n_strong=self.hparams['n_strong'],\n",
        "                                        show=self.hparams['show_augmentation']),\n",
        "                                     )\n",
        "         \n",
        "    else:\n",
        "      self._train_unlabeled_dataset = SimpleTextDataset(self.x_unlabeled,\n",
        "                                                        self.y_unlabeled,\n",
        "                                                        transform=preprocessor)\n",
        "    \n",
        "    self._val_dataset = SimpleTextDataset(self.x_val,\n",
        "                                          self.y_val,\n",
        "                                          transform=preprocessor)\n",
        "    \n",
        "    self._test_dataset = SimpleTextDataset(self.x_test,\n",
        "                                           self.y_test,\n",
        "                                           transform=preprocessor)\n",
        "    \n",
        "    self.total_iterations = len(self._train_unlabeled_dataset) // self.hparams['unl_batch_size'] \n",
        "\n",
        "  # epoch = iterate over all train_labelled_examples\n",
        "  def train_dataloader(self):\n",
        "    encoded_label = torch.tensor(self.le.transform(self.y_labeled))\n",
        "\n",
        "    if self.hparams['epoch_over'] == 'labeled_train_dataset':\n",
        "\n",
        "      encoded_label = torch.tensor(self.le.transform(self.y_labeled))\n",
        "      self._train_unlabeled_dataloader = torch.utils.data.DataLoader(\n",
        "                              self._train_unlabeled_dataset,\n",
        "                              batch_size=self.hparams['unl_batch_size'],\n",
        "                              shuffle=True,\n",
        "                              num_workers=0,\n",
        "                              )\n",
        "        \n",
        "      self.train_unlabeled_dataloader_iterator = iter(self._train_unlabeled_dataloader)\n",
        "  \n",
        "      train_labeled_dataloader = DataLoader(\n",
        "                        self._train_labeled_dataset,\n",
        "                        batch_size=self.hparams['l_batch_size'],\n",
        "                        num_workers=8,\n",
        "                        # shuffle=True # without shuffle it want work cause\n",
        "                        # it need to create map index before __get_item__ function\n",
        "                        sampler=BalancedBatchSampler(self._train_labeled_dataset,\n",
        "                                                    encoded_label),\n",
        "                      )\n",
        "      epoch_over = train_labeled_dataloader\n",
        "\n",
        "    elif self.hparams['epoch_over'] == 'unlabeled_train_dataset':\n",
        "\n",
        "      self._train_labeled_dataloader = DataLoader(\n",
        "                                    self._train_labeled_dataset,\n",
        "                                    batch_size=self.hparams['l_batch_size'],\n",
        "                                    num_workers=0,\n",
        "                                    # shuffle=True # without shuffle it want work cause\n",
        "                                    # it need to create map index before __get_item__ function\n",
        "                                    sampler=BalancedBatchSampler(self._train_labeled_dataset,\n",
        "                                    encoded_label),\n",
        "                                    )\n",
        "      self.train_labeled_dataloader_iterator = iter(self._train_labeled_dataloader)\n",
        "\n",
        "      train_unlabeled_dataloader = DataLoader(\n",
        "                      self._train_unlabeled_dataset,\n",
        "                      batch_size=self.hparams['unl_batch_size'],\n",
        "                      shuffle=True,\n",
        "                      num_workers=8,\n",
        "                     )\n",
        "      epoch_over = train_unlabeled_dataloader\n",
        "\n",
        "    else:\n",
        "      raise ValueError('Wrong epoch_over type: {} \\\n",
        "       \\n Possibilities: unlabeled_train_dataset, labeled_train_dataset'.format(self.hparams['epoch_over']))\n",
        "\n",
        "    return epoch_over \n",
        "    \n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "                      self._val_dataset,\n",
        "                      batch_size=self.hparams['val_batch_size'],\n",
        "                      num_workers=8\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(\n",
        "                      self._test_dataset,\n",
        "                      batch_size=self.hparams['test_batch_size'],\n",
        "                      num_workers=8\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "\n",
        "    if self.hparams['optimizer_type'] == 'AdamW':\n",
        "      param_optimizer = list(self.model.named_parameters())\n",
        "      no_decay = [\"bias\", 'LayerNorm.weight']\n",
        "      optimizer_grouped_parameters = [\n",
        "              {\n",
        "                  \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                  \"weight_decay_rate\": 0.01\n",
        "                  },\n",
        "              {\n",
        "                  \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                  \"weight_decay_rate\": 0.0\n",
        "                  },\n",
        "              ]\n",
        "      print('total_iterations: ', self.total_iterations)\n",
        "      optimizer = AdamW(\n",
        "                        optimizer_grouped_parameters,\n",
        "                        lr=self.hparams['lr'],\n",
        "                      )\n",
        "\n",
        "    elif self.hparams['optimizer_type'] == 'Adam':\n",
        "      optimizer =torch.optim.Adam(self.parameters(), lr=self.hparams['lr'])\n",
        "\n",
        "    elif self.hparams['optimizer_type'] == 'SGD':\n",
        "      optimizer = torch.optim.SGD(self.parameters(),\n",
        "                                  lr=self.hparams['lr'],\n",
        "                                  momentum=self.hparams['momentum'],\n",
        "                                  weight_decay=self.hparams['weight_decay'],\n",
        "                                  nesterov=self.hparams['nesterov'],\n",
        "                                  )\n",
        "    else:\n",
        "      raise ValueError('Wrong optimizer type: {} \\\n",
        "   \\n Possible types: AdamW, Adam, SGD '.format(self.hparams['optimizer_type']))\n",
        "\n",
        "\n",
        "    if self.hparams['scheduler_type'] == 'ExponentialLR':            \n",
        "      scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer,\n",
        "                                       gamma=self.hparams['exponential_decay'])\n",
        "\n",
        "    elif self.hparams['scheduler_type'] == 'StepLR':                    \n",
        "      scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                  self.hparams['decay_step_size'],\n",
        "                                                  self.hparams['decay_gamma'])\n",
        "\n",
        "    elif self.hparams['scheduler_type'] == 'CosineAnnealingLR':                    \n",
        "      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "                                                             optimizer,\n",
        "                                                             self.hparams['T_max'])\n",
        "    elif self.hparams['scheduler_type'] == 'LinearWarmUpLR':\n",
        "      scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                  self.hparams['warmup_steps'],\n",
        "                                                  self.total_iterations,\n",
        "                                                  -1)\n",
        "    else:\n",
        "      print('Scheduler was not specified!')\n",
        "      return [optimizer]\n",
        "\n",
        "    return [optimizer], [scheduler]\n",
        "\n",
        "  def supervised(self, texts, labels, logs):\n",
        "\n",
        "    if self.hparams['epoch_over'] == 'unlabeled_train_dataset':\n",
        "      texts = texts.cuda()\n",
        "      labels = labels.cuda()\n",
        "\n",
        "    logits = self.forward(texts)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "    return logits, loss, logs\n",
        "  \n",
        "  def pseudo_labelling(self, l_embeddings, labels, unl_embeddings, logs):\n",
        "\n",
        "    if self.hparams['epoch_over'] == 'unlabeled_train_dataset':\n",
        "      l_embeddings = l_embeddings.cuda()\n",
        "      labels = labels.cuda()\n",
        "      unl_embeddings = unl_embeddings['embedding']\n",
        "    elif self.hparams['epoch_over'] == 'labeled_train_dataset':\n",
        "      unl_embeddings = unl_embeddings['embedding'].cuda()\n",
        "    else:\n",
        "      raise ValueError('Wrong epoch_over type: {} \\\n",
        "       \\n Possibilities: unlabeled_train_dataset, labeled_train_dataset'.format(self.hparams['epoch_over']))\n",
        "\n",
        "    x = torch.cat((l_embeddings,\n",
        "                   unl_embeddings))\n",
        "\n",
        "    logits = self.forward(x)\n",
        "    batch_size = self.hparams['l_batch_size'] \n",
        "    logits_l_x = logits[:batch_size]\n",
        "    logits_unl_x = logits[batch_size:]\n",
        "\n",
        "    del logits\n",
        "     \n",
        "    Lx = F.cross_entropy(logits_l_x, labels, reduction='mean')\n",
        "\n",
        "    pseudo_distribution = torch.softmax(logits_unl_x.detach(), dim=-1)\n",
        "    max_probs, targets_u = torch.max(pseudo_distribution, dim=-1)\n",
        "\n",
        "    confident_keys = ['unl_max_confident', 'unl_min_confident',\n",
        "                      'unl_mean_confident', 'unl_std_confident']\n",
        "    confident_values = juxt(torch.max, torch.min, torch.mean, torch.std) \\\n",
        "                           (max_probs)\n",
        "    confident_dict = dict(zip(confident_keys, confident_values))\n",
        "    logs.update(confident_dict)\n",
        "\n",
        "    Lu = F.cross_entropy(logits_unl_x, targets_u, reduction='none').mean()\n",
        "    ### here should be some kind of scheduler for lambda_u\n",
        "\n",
        "    def unlabeled_weight(self):\n",
        "        alpha = 0.0\n",
        "        if self.current_epoch > self.hparams['T1']:\n",
        "            alpha = (self.current_epoch-self.hparams['T1']) /\n",
        "             (self.hparams['T2']-self.hparams['T1']) * self.['af']\n",
        "            if self.current_epoch > self.T2:\n",
        "                alpha = af\n",
        "        return alpha\n",
        "\n",
        "    alpha = unlabeled_weight()\n",
        "    loss = Lx + alpha * Lu\n",
        "    logs.update({\"Lu\": Lu, 'Lx': Lx, 'alpha': alpha})\n",
        "\n",
        "    return logits_l_x, loss, logs\n",
        "\n",
        "  def vat(self, l_embeddings, labels, unl_embeddings, logs):\n",
        "    if self.hparams['epoch_over'] == 'unlabeled_train_dataset':\n",
        "      l_embeddings = l_embeddings.cuda()\n",
        "      labels = labels.cuda()\n",
        "      unl_embeddings = unl_embeddings['embedding']\n",
        "    elif self.hparams['epoch_over'] == 'labeled_train_dataset':\n",
        "      unl_embeddings = unl_embeddings['embedding'].cuda()\n",
        "    else:\n",
        "      raise ValueError('Wrong epoch_over type: {} \\\n",
        "       \\n Possibilities: unlabeled_train_dataset, labeled_train_dataset'.format(self.hparams['epoch_over']))\n",
        "      \n",
        "    vat_loss = VATLoss(xi=self.hparams['xi'],\n",
        "                       eps=self.hparams['eps'],\n",
        "                       ip=self.hparams['ip'])\n",
        "\n",
        "    # from pdb import set_trace as st\n",
        "    # st()\n",
        "    logits = self.forward(l_embeddings)\n",
        "    supervised_loss = self.loss_fct(logits, labels)\n",
        "    logs.update({'supervised_loss': supervised_loss})\n",
        "    # need it for transformer\n",
        "    embeddings = self.forward(unl_embeddings, embeddings_only=True)\n",
        "    lds = vat_loss(self, embeddings)\n",
        "    logs.update({'lds_loss': lds})\n",
        "\n",
        "    loss = supervised_loss + self.hparams['alpha'] * lds  \n",
        "\n",
        "    return logits, loss, logs\n",
        "\n",
        "  def fixmatch(self, l_embeddings, labels, unl_embeddings, logs):\n",
        "\n",
        "    unl_w_dict, unl_s_dict = unl_embeddings\n",
        "    unl_w, unl_s = unl_w_dict['embedding'], unl_s_dict['embedding']\n",
        "\n",
        "    if self.hparams['epoch_over'] == 'unlabeled_train_dataset':\n",
        "\n",
        "      if self.hparams['model_arch'] == 'transformer':\n",
        "        l_embeddings = toolz.dicttoolz.valmap(torch.Tensor.cuda, l_embeddings)\n",
        "        labels = labels.cuda()\n",
        "      else:\n",
        "        l_embeddings = l_embeddings.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "    elif self.hparams['epoch_over'] == 'labeled_train_dataset':\n",
        "  \n",
        "      if self.hparams['model_arch'] == 'transformer':\n",
        "        unl_w = toolz.dicttoolz.valmap(torch.Tensor.cuda, unl_w)\n",
        "        unl_s = toolz.dicttoolz.valmap(torch.Tensor.cuda, unl_s)\n",
        "      else:\n",
        "        unl_w = unl_w.cuda() \n",
        "        unl_s = unl_s.cuda() \n",
        "    else:\n",
        "      raise ValueError('Wrong epoch_over type: {} \\\n",
        "       \\n Possibilities: unlabeled_train_dataset, labeled_train_dataset'.format(self.hparams['epoch_over']))\n",
        "\n",
        "    if self.hparams['model_arch'] == 'transformer':\n",
        "\n",
        "      stacked_inputs_ids = torch.cat((l_embeddings[\"input_ids\"],\n",
        "                                      unl_w['input_ids'],\n",
        "                                      unl_s['input_ids']))\n",
        "      \n",
        "      stacked_attention_mask = torch.cat((l_embeddings[\"attention_mask\"],\n",
        "                                          unl_w['attention_mask'],\n",
        "                                          unl_s['attention_mask']))\n",
        "      \n",
        "      stacked_token_type_ids = torch.cat((l_embeddings[\"token_type_ids\"],\n",
        "                                          unl_w['token_type_ids'],\n",
        "                                          unl_s['token_type_ids']))\n",
        "    \n",
        "      x = {\n",
        "           \"inputs_ids\": stacked_inputs_ids,\n",
        "           \"attention_mask\": stacked_attention_mask,\n",
        "           \"token_type_ids\": stacked_token_type_ids\n",
        "          }\n",
        "    else:\n",
        "      # from pdb import set_trace as st;\n",
        "      # st()\n",
        "      x = torch.cat((l_embeddings,\n",
        "                     unl_w,\n",
        "                     unl_s))\n",
        "\n",
        "    logits = self.forward(x)\n",
        "    batch_size = self.hparams['l_batch_size'] \n",
        "    logits_x = logits[:batch_size]\n",
        "    logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
        "\n",
        "    del logits\n",
        "     \n",
        "    Lx = F.cross_entropy(logits_x, labels, reduction='mean')\n",
        "\n",
        "    pseudo_label = torch.softmax(logits_u_w.detach_(), dim=-1)\n",
        "    max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
        "\n",
        "    confident_keys = ['unl_max_confident', 'unl_min_confident',\n",
        "                      'unl_mean_confident', 'unl_std_confident']\n",
        "    confident_values = juxt(torch.max, torch.min, torch.mean, torch.std) \\\n",
        "                           (max_probs)\n",
        "    confident_dict = dict(zip(confident_keys, confident_values))\n",
        "    logs.update(confident_dict)\n",
        "\n",
        "    mask = max_probs.ge(self.hparams['threshold']).float()\n",
        "    Lu = (F.cross_entropy(logits_u_s, targets_u,\n",
        "                          reduction='none') * mask).mean()\n",
        "\n",
        "    loss = Lx + self.hparams['lambda_u'] * Lu\n",
        "    logs.update({\"Lu\": Lu, 'Lx': Lx})\n",
        "\n",
        "    return logits_x, loss, logs\n",
        "\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "\n",
        "    if self.hparams['epoch_over'] == 'unlabeled_train_dataset':\n",
        "\n",
        "      try:\n",
        "        # it it automatically that values are unpacked?\n",
        "        l_batch = next(self.train_labeled_dataloader_iterator)\n",
        "  \n",
        "      except StopIteration:\n",
        "        self.train_labeled_dataloader_iterator = iter(self._train_labeled_dataloader)\n",
        "        l_batch = next(self.train_labeled_dataloader_iterator)\n",
        "      \n",
        "      l_embeddings, labels = l_batch['embedding'], l_batch['label']\n",
        "      unlabeled = batch\n",
        "\n",
        "    elif self.hparams['epoch_over'] == 'labeled_train_dataset':\n",
        "\n",
        "      try:\n",
        "        # unlabeled[embedding]???\n",
        "        unlabeled = next(self.train_unlabeled_dataloader_iterator)\n",
        "\n",
        "      except StopIteration:\n",
        "        self.train_unlabeled_dataloader_iterator = iter(self._train_unlabeled_dataloader)\n",
        "        unlabeled = next(self.train_unlabeled_dataloader_iterator)\n",
        "      \n",
        "      l_embeddings, labels = batch['embedding'], batch['label']\n",
        "\n",
        "    else:\n",
        "      raise ValueError('Wrong epoch_over type: {} \\\n",
        "       \\n Possibilities: unlabeled_train_dataset, labeled_train_dataset'.format(self.hparams['epoch_over']))\n",
        "\n",
        "    logs = dict()\n",
        "    # from pdb import set_trace as st\n",
        "    # st()\n",
        "    \n",
        "    if self.hparams['training_method'] == 'VAT':\n",
        "      logits, loss, logs = self.vat(l_embeddings, labels, unlabeled, logs)\n",
        "\n",
        "    elif self.hparams['training_method'] == \"FixMatch\":\n",
        "      logits, loss, logs = self.fixmatch(l_embeddings, labels, unlabeled, logs)\n",
        "\n",
        "    elif self.hparams['training_method'] == \"PseudoLabelling\":\n",
        "      logits, loss, logs = self.pseudo_labelling(l_embeddings, labels, unlabeled, logs)\n",
        "\n",
        "    elif self.hparams['training_method'] == \"Supervised\":\n",
        "      logits, loss, logs = self.supervised(l_embeddings, labels, logs)\n",
        "\n",
        "    else:\n",
        "      raise ValueError('Wrong training method type: {} \\n \\\n",
        "        Possible methods : VAT, FixMatch, Supervised'.format(self.hparams['traning_method']))\n",
        "\n",
        "    probabilities = torch.softmax(logits.detach(), dim=-1)\n",
        "    gpu_max_probs, gpu_labels_hat = torch.max(probabilities, dim=-1)\n",
        "\n",
        "    cpu_labels = labels.detach().cpu().numpy()\n",
        "    cpu_labels_hat = gpu_labels_hat.detach().cpu().numpy()\n",
        "    cpu_max_probs = gpu_max_probs.detach().cpu().numpy()\n",
        "    cpu_loss = loss.clone().detach().cpu().numpy()\n",
        "\n",
        "\n",
        "    metrics_dict = create_metrics_dict('train',\n",
        "                                       cpu_loss,\n",
        "                                       cpu_labels,\n",
        "                                       cpu_labels_hat,\n",
        "                                       cpu_max_probs,\n",
        "                                      )\n",
        "\n",
        "    logs.update(metrics_dict)    \n",
        "\n",
        "    return {\n",
        "            'loss': loss,\n",
        "            # \"progress_bar\": logs,\n",
        "            'log': logs}\n",
        "\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "\n",
        "    embeddings = batch['embedding']\n",
        "    labels = batch['label']\n",
        "  \n",
        "    logits = self.forward(embeddings)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "    max_probs, labels_hat = torch.max(probabilities, dim=-1)\n",
        "\n",
        "    cpu_labels = labels.detach().cpu().numpy()\n",
        "    cpu_labels_hat = labels_hat.detach().cpu().numpy()\n",
        "    cpu_max_probs = max_probs.detach().cpu().numpy()\n",
        "    cpu_loss = loss.clone().detach().cpu().numpy()\n",
        "\n",
        "    output = create_metrics_dict('val',\n",
        "                                 cpu_loss,\n",
        "                                 cpu_labels,\n",
        "                                 cpu_labels_hat,\n",
        "                                 cpu_max_probs)\n",
        "    output['val_gpu_loss'] = loss\n",
        "    \n",
        "    return output\n",
        "\n",
        "  def validation_epoch_end(self, outputs):\n",
        "    val_loss_mean = torch.mean(torch.tensor([output.pop('val_gpu_loss') for output in outputs]))\n",
        "    val_accuracy_error_mean = torch.mean(torch.Tensor([output['val_accuracy_error'] for output in outputs]))\n",
        "    tqdm_dict = toolz.merge_with(np.mean, outputs)\n",
        "    return {\n",
        "            \"progress_bar\": tqdm_dict,\n",
        "            \"log\": tqdm_dict,\n",
        "            'val_loss': val_loss_mean,\n",
        "            'val_accuracy_error': val_accuracy_error_mean\n",
        "           }\n",
        "\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "\n",
        "    embeddings = batch[\"embedding\"]\n",
        "    labels = batch[\"label\"]\n",
        "  \n",
        "    logits = self.forward(embeddings)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "    max_probs, labels_hat = torch.max(probabilities, dim=-1)\n",
        "\n",
        "    cpu_labels = labels.detach().cpu().numpy()\n",
        "    cpu_labels_hat = labels_hat.detach().cpu().numpy()\n",
        "    cpu_max_probs = max_probs.detach().cpu().numpy()\n",
        "    cpu_loss = loss.detach().cpu().numpy()\n",
        "    \n",
        "    output = create_metrics_dict('test',\n",
        "                                 cpu_loss,\n",
        "                                 cpu_labels,\n",
        "                                 cpu_labels_hat,\n",
        "                                 cpu_max_probs)\n",
        "    output['test_loss_gpu'] = loss\n",
        "    \n",
        "    return output \n",
        "\n",
        "\n",
        "  def test_epoch_end(self, outputs):\n",
        "    test_loss_mean = torch.mean(torch.tensor([output.pop('test_loss_gpu') for output in outputs]))\n",
        "    tqdm_dict = toolz.merge_with(np.mean, outputs)\n",
        "\n",
        "    return {\n",
        "            \"progress_bar\": tqdm_dict,\n",
        "            \"log\": tqdm_dict,\n",
        "            'test_loss': test_loss_mean \n",
        "           }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aYX9ZJVIoEs",
        "colab_type": "text"
      },
      "source": [
        "### Configure experiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC9zE6cKkbPi",
        "colab_type": "text"
      },
      "source": [
        "##### Choose dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEwfu7pNkDJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = 'MNIST'\n",
        "\n",
        "if dataset == 'IMDB':\n",
        "  dataset_path = str(Path()/\n",
        "                        'gdrive'/\n",
        "                        'My Drive'/\n",
        "                        'praca_magisterska'/\n",
        "                        'pytorch_lightning'/\n",
        "                        'datasets'/\n",
        "                        'imdb_with_bt.csv')\n",
        "  \n",
        "  hparams = {\"dataset\": 'IMDB',\n",
        "            \"num_classes\": 2,\n",
        "            \"dataset_path\": dataset_path,\n",
        "            }\n",
        "\n",
        "\n",
        "elif dataset == 'MR':\n",
        "  dataset_path = str(Path()/\n",
        "                        'gdrive'/\n",
        "                        'My Drive'/\n",
        "                        'praca_magisterska'/\n",
        "                        'pytorch_lightning'/\n",
        "                        'datasets'/\n",
        "                        'mr_with_bt2.csv')\n",
        "  \n",
        "  hparams = {\"dataset\": 'MR',\n",
        "            \"num_classes\": 2,\n",
        "            \"dataset_path\": dataset_path,\n",
        "            }\n",
        "\n",
        "elif dataset == 'MNIST':\n",
        "  dataset_path = str(Path()/\n",
        "                        'gdrive'/\n",
        "                        'My Drive'/\n",
        "                        'praca_magisterska'/\n",
        "                        'pytorch_lightning'/\n",
        "                        'datasets'\n",
        "                        )\n",
        "  \n",
        "  hparams = {\"dataset\": 'MNIST',\n",
        "            \"num_classes\": 10,\n",
        "            \"dataset_path\": dataset_path,\n",
        "            'mean': 0.1307,\n",
        "            'std': 0.3081,\n",
        "            }\n",
        "\n",
        "elif dataset == 'NLUHD':\n",
        "  dataset_path = str(Path()/\n",
        "                    'gdrive'/\n",
        "                    'My Drive'/\n",
        "                    'praca_magisterska'/\n",
        "                    'pytorch_lightning'/\n",
        "                    'datasets'/\n",
        "                    'NLU-Data-Home-Domain-preprocessed-without-ner.csv')\n",
        "  \n",
        "  hparams = {\"dataset\": 'NLUHD',\n",
        "            \"num_classes\": 68, # ???\n",
        "            \"dataset_path\": dataset_path}\n",
        "            \n",
        "else:\n",
        "  raise ValueError('Wrong dataset name : {} \\\n",
        "   \\n Possible datasets: IMDB, MR, NLUHD'.format(hparams['dataset']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uf3R7m6kd6z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##### Choose traning method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxhbdqoNkeGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_method = 'Supervised' \n",
        "# training_method = 'VAT' \n",
        "# training_method = 'FixMatch' \n",
        "# training_method = 'PseudoLabelling' \n",
        "\n",
        "if training_method == 'VAT':\n",
        "  hparams.update({\"training_method\": 'VAT',\n",
        "                  \n",
        "                  'l_batch_size': 32,\n",
        "                  'unl_batch_size': 32, \n",
        "                  'xi':1,\n",
        "                  'eps':4.5, \n",
        "                  'ip':1, \n",
        "                  'alpha':1,\n",
        "                  'optimizer_type': 'Adam',\n",
        "                  'lr': 5e-05,\n",
        "\n",
        "                  'scheduler_type': 'ExponentialLR',\n",
        "                  'exponential_decay':0.9998,\n",
        "                })\n",
        "elif training_method == 'FixMatch':\n",
        "  hparams.update({\"training_method\": 'FixMatch',\n",
        "                  \n",
        "                  'l_batch_size': 16,\n",
        "                  'unl_batch_size': 112, # mu = 7\n",
        "\n",
        "                  'mu': 7, \n",
        "                  'threshold': 0.7,\n",
        "                  'lambda_u': 0.5,\n",
        "                  'n_weak': 1,\n",
        "                  'n_strong': 2,\n",
        "\n",
        "                  'optimizer_type': 'SGD',\n",
        "                  'lr': 3e-03,\n",
        "                  'momentum':0.9,\n",
        "                  'weight_decay':0.0005,\n",
        "                  'nesterov': True,\n",
        "\n",
        "                  'scheduler_type': \"CosineAnnealingLR\",\n",
        "\n",
        "                  'show_augmentation': False,\n",
        "                })\n",
        "  \n",
        "elif training_method == 'PseudoLabelling':\n",
        "  hparams.update({\"training_method\": 'PseudoLabelling',\n",
        "                  \n",
        "                  'T1': 100,\n",
        "                  'T2': 600,\n",
        "                  'af': 0.3,                  \n",
        "\n",
        "                  'l_batch_size': 32,\n",
        "                  'unl_batch_size': 32, \n",
        "                  \n",
        "                  'optimizer_type': 'Adam',\n",
        "                  'lr': 1e-05,\n",
        "\n",
        "                  'scheduler_type': 'None',\n",
        "                  'scheduler_T': 'Linear',\n",
        "                  'decay_step_size': 10000,\n",
        "                  'decay_gamma':0.5,\n",
        "  })\n",
        "\n",
        "elif training_method == 'Supervised':\n",
        "  hparams.update({\"training_method\": 'Supervised',\n",
        "                  \n",
        "                  'l_batch_size': 32,\n",
        "                  'unl_batch_size': 32, \n",
        "                  \n",
        "                  'optimizer_type': 'Adam',\n",
        "                  'lr': 1e-05,\n",
        "                  \n",
        "                  'scheduler_type': 'None',\n",
        "                  'decay_step_size': 10000,\n",
        "                  'decay_gamma':0.5,\n",
        "\n",
        "  })\n",
        "\n",
        "\n",
        "else:\n",
        "  raise ValueError('Wrong training method type: {} \\\n",
        "     \\n Possible methods : VAT, FixMatch, Supervised'.format(self.hparams['training_method']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FownSMlUmIzo",
        "colab_type": "text"
      },
      "source": [
        "#### Choose model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH4lZGqlmJAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_arch = 'ConvolutionMNIST'\n",
        "\n",
        "if model_arch == \"Transformer\":\n",
        "  hparams.update({\n",
        "      'model_arch': model_arch,\n",
        "      'model_type': 'bert-base-uncased',\n",
        "      'max_sentence_len': 156,\n",
        "\n",
        "      'xi': 0.00001,\n",
        "      'lr': 5e-05,\n",
        "      'weight_decay': 0.01,\n",
        "      'adam_eps': 1e-06,\n",
        "      'warmup_steps': 150,\n",
        "      })\n",
        "\n",
        "elif model_arch == 'Convolution':\n",
        "  hparams.update({\n",
        "           'model_arch': model_arch,\n",
        "           'max_sentence_len': 700,\n",
        "           'embeder_type': \"fastText\",\n",
        "           'embed_dim': 300,\n",
        "           'Ci': 1,\n",
        "           'kernel_num': 100,\n",
        "           'kernel_sizes': '3,4,5',\n",
        "           'dropout':0.0, \n",
        "  })\n",
        "\n",
        "elif model_arch == 'ConvolutionMNIST':\n",
        "  hparams.update({\n",
        "           'model_arch': model_arch,\n",
        "           'dropout':0.0,\n",
        "  })\n",
        "else:\n",
        "  raise ValueError('Wrong model architecture type: {} \\\n",
        "   \\n Possible datasets: Transformer, Convolution'.format(self.hparams['model_arch']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st6sL-cOwym2",
        "colab_type": "text"
      },
      "source": [
        "#### Choose training params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8YWsCR-jGM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " hparams.update({\n",
        "           'train_test_split': 0.9,\n",
        "           'seed': 42,\n",
        "           'val_batch_size': 16,\n",
        "           'test_batch_size': 16,\n",
        "           'n_labeled': 160, # number of labeled samples  # must be a multiplication of l_batch_size\n",
        "           'valid_size_per_class': 100, # 68 class => n_val_samples = 68 * 10 \n",
        "\n",
        "           'loss_function':'CrossEntropyLoss',\n",
        "\n",
        "           'test_run': False,\n",
        "           'save_best_model': False,\n",
        "           'epoch_over': 'labeled_train_dataset',           \n",
        "           'max_epochs': 3,\n",
        "           'min_epochs': 1,\n",
        "           'val_check_interval': 1, # change interval if epoch is on unlabeled \n",
        "           'patience': 15, # early stopping callback parameter\n",
        " }) \n",
        "\n",
        "if hparams['scheduler_type'] == \"CosineAnnealingLR\":\n",
        "  hparams.update({'T_max': hparams['max_epochs'] * hparams['n_labeled'] / hparams['l_batch_size']})\n",
        "\n",
        " \n",
        "tags = []\n",
        "tags.append(hparams['dataset'])\n",
        "tags.append(hparams['model_arch'])\n",
        "tags.append(hparams['training_method'])\n",
        "tags.append(str(hparams['n_labeled']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjha3vit9omj",
        "colab_type": "text"
      },
      "source": [
        "### Tests "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DWq_Q17ENpc",
        "colab_type": "text"
      },
      "source": [
        "#### Test if lf.cv.split splits in stratified fashion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e-bcGspDUnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ds = lf.CsvDataset(os.path.join(os.environ['DATASETS_PATH'], 'NLU-Data-Home-Domain-preprocessed-without-ner.csv'), header=True)\n",
        "# ds = ds.filter(lambda x: x['text'] is not None)\n",
        "\n",
        "# train, test = lf.cross_validation.split_dataset_random(ds, int(len(ds) * 0.8), seed=42)\n",
        "# df_train = pd.DataFrame(train)\n",
        "# df_test = pd.DataFrame(test)\n",
        "# # df_train['intent'].value_counts().plot(kind=\"bar\", figsize= (21,20))\n",
        "# df_test['intent'].value_counts().plot(kind=\"bar\", figsize= (21,20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vhpT-QpSOrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pd.set_option('display.max_rows', None)\n",
        "# df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfAShMKL9oH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
        "# uter = '[greetings : Hello] there what is [swear_word: fuck] up ?'\n",
        "# # uter =  '[greetings : Hello] there what is fuck up ?'\n",
        "# # uter = \"hello adfafdasfsaf sfsafdsafsa\"\n",
        "# comment = nlp(uter)\n",
        "# processed_comment = prepare_NLUHD(comment, nlp=nlp,ner_abstract_tag=False)\n",
        "# processed_comment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isgALgNl86Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train = lfds.MsrParaphrase(\"train\")\n",
        "# print(len(train))\n",
        "# test = lfds.MsrParaphrase(\"test\")\n",
        "# train.first()\n",
        "# def nonefilter(dataset):\n",
        "#   filtered = []\n",
        "#   for x in dataset:\n",
        "#       if x[\"string1\"] is None:\n",
        "#           continue\n",
        "#       if x[\"string2\"] is None:\n",
        "#           continue\n",
        "#       filtered.append(x)\n",
        "#   return lf.Dataset(filtered)\n",
        "# # train = nonefilter(train)\n",
        "# train = train.filter(lambda x: x[\"string1\"] is not None and x[\"string2\"] is not None)\n",
        "# print(len(train))\n",
        "# train.take(3)\n",
        "# unique = list(['ale', 'beka'])\n",
        "# le = preprocessing.LabelEncoder().fit(unique)\n",
        "# torch.tensor(int(le.transform(['ale'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9cu0GG3qjHz",
        "colab_type": "text"
      },
      "source": [
        "#### Test IMDB preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOH9NEJHhTyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hparams = {'max_sentence_len': 200,\n",
        "#            'embed_dim': 300,\n",
        "#            'seed': 42,\n",
        "#            'train_test_split': 0.8}\n",
        "\n",
        "# train = lfds.Imdb('train')\n",
        "# test = lfds.Imdb('test')\n",
        "# ds = train + test\n",
        "# ds = ds.map(lambda x: {'text':x[0] , 'label': x[1]})\n",
        "# embeder = nlp \n",
        "# tokenizer_fun = lambda x: x#gensim_tokenizer\n",
        "# unique_labels = list(pd.DataFrame(ds).label.unique())\n",
        "# le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "\n",
        "# preprocessor = partial(\n",
        "#                       preprocess_IMDB,\n",
        "#                       hparams,\n",
        "#                       tokenizer_fun, \n",
        "#                       embeder,\n",
        "#                       le,\n",
        "#                       )\n",
        "# s = ds.first()\n",
        "# print(s)\n",
        "# preprocessor(s)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YDThFsqiBYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# train = lfds.Imdb('train')\n",
        "# test = lfds.Imdb('test')\n",
        "# ds = train + test\n",
        "# ds = ds.map(lambda x: {'text':x[0] , 'label': x[1]})\n",
        "# df = pd.DataFrame(ds)\n",
        "# pattern1 = re.compile(r'<.*?>')\n",
        "# # pattern2 = re.compile('[\\W_]+')\n",
        "# # text = pattern1.sub('', sample['text'])\n",
        "# # print('text after p1: ', text)\n",
        "# # text = text.replace('_', '').lower()\n",
        "# func = partial(pattern1.sub,\n",
        "#                '')\n",
        "# df['text'] = df['text'].apply(func)\n",
        "# # df\n",
        "# texts = list(df['text'])\n",
        "# texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ac1FViOh833",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ft.build_vocab(texts, update=True)\n",
        "# ft.train(new_sentences, total_examples=len(texts), epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHn7X8Oc2IJN",
        "colab_type": "text"
      },
      "source": [
        "#### Test FixMatchTransform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIOUS2Fr2NLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf = TransformFix(1,show=True)\n",
        "# s = tf(\"what will be the weather like tomorrow, please tell me\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0Yw_j3YaDTg",
        "colab_type": "text"
      },
      "source": [
        "#### Test balanced sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-29zVc12aDe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not_none = lambda x: x[\"text\"] is not None \n",
        "# # ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "# ds = lfds.Imdb('train') + lfds.Imdb('test')\n",
        "# ds = ds.map(lambda x: {'text': x[0], 'label': x[1]})\n",
        "# df = pd.DataFrame(ds)\n",
        "\n",
        "# create_tfidf_model(df)\n",
        "\n",
        "# print(df.info(memory_usage=True))\n",
        "# unique_labels = list(df.label.unique())\n",
        "# print(f'unique_labels: {unique_labels}')\n",
        "# print(f'number_of_categories : {len(unique_labels)}')\n",
        "# le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "# train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "#                                                         int(len(ds) * 0.9),\n",
        "#                                                         seed=42)\n",
        "\n",
        "# dataset_preprocessor = partial(\n",
        "#                                 preprocess_IMDB,\n",
        "#                                 le,\n",
        "#                               )\n",
        "\n",
        "# tokenizer_dict = {\n",
        "#         \"bert-base-uncased\":\n",
        "#           BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
        "#                                         do_lower_case=True),\n",
        "#         \"roberta-base\":\n",
        "#           RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "#         }\n",
        "\n",
        "\n",
        "# model_arch_preprocessor = partial(\n",
        "#                                   transformer_preprocessing,\n",
        "#                                   'bert-base-uncased',\n",
        "#                                   156,\n",
        "#                                   tokenizer_dict['bert-base-uncased'],\n",
        "#                                   )\n",
        "\n",
        "\n",
        "# preprocessor = toolz.compose(\n",
        "#                             model_arch_preprocessor,\n",
        "#                             dataset_preprocessor,\n",
        "#                             )\n",
        "\n",
        "# train_df, test_df = pd.DataFrame(train), pd.DataFrame(test)\n",
        "# x_train, y_train = train_df['text'].values, train_df['label'].values\n",
        "# x_test, y_test = test_df['text'].values, test_df['label'].values\n",
        "\n",
        "# # split's training parameters  \n",
        "# num_classes = len(unique_labels)\n",
        "# label_per_class = 1000 // num_classes\n",
        "# valid_size = 1000 \n",
        "\n",
        "# labeled_idx = []\n",
        "# unlabeled_idx = []\n",
        "# val_idx = []\n",
        "\n",
        "# for label in unique_labels:\n",
        "#   idx = np.where(y_train == label)[0]\n",
        "#   np.random.shuffle(idx)\n",
        "#   labeled_idx.extend(idx[:label_per_class])\n",
        "#   val_idx.extend(idx[label_per_class: label_per_class + valid_size])\n",
        "#   unlabeled_idx.extend(idx[label_per_class + valid_size:])\n",
        "\n",
        "# x_labeled, y_labeled  = x_train[labeled_idx], y_train[labeled_idx]\n",
        "# x_unlabeled, y_unlabeled = x_train[unlabeled_idx], y_train[unlabeled_idx]\n",
        "# x_val, y_val = x_train[val_idx], y_train[val_idx]\n",
        "\n",
        "\n",
        "# train_labeled_dataset = SimpleTextDataset(x_labeled,\n",
        "#                                               y_labeled,\n",
        "#                                               transform=preprocessor)\n",
        "# train_unlabeled_dataset = SimpleTextDataset(x_unlabeled,\n",
        "#                                                   y_unlabeled,\n",
        "#                                                   transform=preprocessor)\n",
        "\n",
        "# val_dataset = SimpleTextDataset(x_val,\n",
        "#                                     y_val,\n",
        "#                                     transform=preprocessor)\n",
        "\n",
        "# test_dataset = SimpleTextDataset(x_test,\n",
        "#                                       y_test,\n",
        "#                                       transform=preprocessor)\n",
        "\n",
        "\n",
        "\n",
        "# train_labeled_dataloader = torch.utils.data.DataLoader(\n",
        "#                       train_labeled_dataset,\n",
        "#                       batch_size=64,\n",
        "#                       # shuffle=True,\n",
        "#                       num_workers=0,\n",
        "#                       sampler=BalancedBatchSampler(train_labeled_dataset, y_labeled),\n",
        "#                       )\n",
        "\n",
        "# train_labeled_dataloader_iterator = iter(train_labeled_dataloader)\n",
        "# train_unlabeled_dataloader = DataLoader(\n",
        "#                 train_unlabeled_dataset,\n",
        "#                 batch_size=64,\n",
        "#                 num_workers=8,\n",
        "#                 shuffle=True # without shuffle it want work cause it need to create map index before __get_item__ function\n",
        "#                 )\n",
        "\n",
        "# train_labeled_dataloader_iterator = iter(train_labeled_dataloader)\n",
        "# batch = next(train_labeled_dataloader_iterator)\n",
        "# print(torch.sum(batch['label']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMARxMrnTMzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# epochs = 3\n",
        "# size = 20\n",
        "# features = 5\n",
        "# classes_prob = torch.tensor([0.1, 0.4, 0.5])\n",
        "\n",
        "# dataset_X = torch.randn(size, features)\n",
        "# dataset_Y = torch.distributions.categorical.Categorical(classes_prob.repeat(size, 1)).sample()\n",
        "# print(dataset_Y)\n",
        "\n",
        "# dataset = torch.utils.data.TensorDataset(dataset_X, dataset_Y)\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(dataset, sampler=BalancedBatchSampler(dataset, dataset_Y), batch_size=6)\n",
        "\n",
        "# for epoch in range(0, epochs):\n",
        "#     for batch_x, batch_y in train_loader:\n",
        "#         print(\"epoch: %d labels: %s\\ninputs: %s\\n\" % (epoch, batch_y, batch_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVxlMTAX9hUH",
        "colab_type": "text"
      },
      "source": [
        "#### Test embeder if exist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfDzoDan9gkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokens = nlp('213213dsf ma kota')\n",
        "# tokenlist = [token.vector for token in tokens]\n",
        "# t = torch.Tensor(tokenlist)\n",
        "# t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCpso0jN-jKw",
        "colab_type": "text"
      },
      "source": [
        "#### Test weights and biases changes in model architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN-ernJqP0dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torchtest import assert_vars_change, assert_vars_same\n",
        "# from torch.autograd import Variable\n",
        "\n",
        "# model = LitComposableFramework(hparams)\n",
        "# model.prepare_data()\n",
        "# model.train_dataloader()\n",
        "\n",
        "# # what are the variables?\n",
        "# print('Our list of parameters', [ np[0] for np in model.named_parameters() ])\n",
        "\n",
        "# # do they change after a training step?\n",
        "\n",
        "# batch = next(model.train_unlabeled_dataloader_iterator)\n",
        "\n",
        "# class Wrapper():\n",
        "#   def __init__(self, values, model_arch):\n",
        "#     self.values = values \n",
        "#     self.model_arch = model_arch\n",
        "\n",
        "#   def to(self, device):\n",
        "#     if self.model_arch == 'Transformer':\n",
        "#       for k, v in self.values.items():\n",
        "#           self.values[k] = v.to(device)\n",
        "#       return self.values\n",
        "#     else:\n",
        "#       return self.values.to(device) \n",
        "\n",
        "# def convert_batch_format(embedding, label):\n",
        "#   return [Wrapper(embedding, hparams['model_arch']),\n",
        "#           label, hparams['model_arch']]\n",
        "\n",
        "# converted_batch = convert_batch_format(**batch)\n",
        "\n",
        "# if hparams['model_arch'] == 'Transformer':\n",
        "#   assert_vars_same(\n",
        "#       model=model,\n",
        "#       loss_fn=F.cross_entropy,\n",
        "#       optim=torch.optim.Adam(model.parameters()),\n",
        "#       device= torch.cuda.current_device(),\n",
        "#       batch=converted_batch,\n",
        "#       params=[('model.pooler.dense.weight', model.model.pooler.dense.weight)])\n",
        "# else:\n",
        "#   assert_vars_change(\n",
        "#       model=model,\n",
        "#       loss_fn=F.cross_entropy,\n",
        "#       optim=torch.optim.Adam(model.parameters()),\n",
        "#       device= torch.cuda.current_device(),\n",
        "#       batch=converted_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvLcy17JrmCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gradients\n",
        "# for n, p in named_parameters:\n",
        "#         if(p.requires_grad) and (\"bias\" not in n):\n",
        "#             layers.append(n)\n",
        "#             ave_grads.append(p.grad.abs().mean())\n",
        "#             max_grads.append(p.grad.abs().max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1K0b38hTGez",
        "colab_type": "text"
      },
      "source": [
        "#### Test split_size of pytorch tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-LRJLFlTHgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_ds = torchvision.datasets.MNIST('./',\n",
        "#                                     train=True, transform=None,\n",
        "#                                     target_transform=None, download=True)\n",
        "\n",
        "# test_ds = torchvision.datasets.MNIST('./',\n",
        "#                                   train=False, transform=None,\n",
        "#                                   target_transform=None, download=True)\n",
        "# train_x, train_y  = train_ds.data, train_ds.targets\n",
        "# test_x, test_y  = test_ds.data, test_ds.targets\n",
        "# full_x, full_y = torch.cat((train_x, test_x), 0), torch.cat((train_y, test_y), 0)\n",
        "\n",
        "# train_size = int(0.9 * full_x.shape[0])\n",
        "# test_size = full_x.shape[0] - train_size\n",
        "\n",
        "# train_x, test_x = size_splits(full_x,\n",
        "#                           [train_size, test_size])\n",
        "\n",
        "# train_y, test_y = size_splits(full_y,\n",
        "#                           [train_size, test_size])\n",
        "# train_y.shape\n",
        "# list(np.array(torch.unique(train_y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4n0cvZNw5Ec",
        "colab_type": "text"
      },
      "source": [
        "### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "velwnCrgNCeb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cfed226301d94af7a5e000078a9ffbab",
            "9c46a3f1d0d541c28e56d31a166a79ee",
            "ef4898e3f13541409b7f5cabd40c3837",
            "7f19cb52705f4156ac4ec85ab6451986",
            "1eda2aa271f64a0eadc6d020e42cd283",
            "1539425a17c64d05a554ea73a7eac9ab",
            "11efffd4fa2d44438ad8b32c6df198fe",
            "39a17e62633644b4afa30acdb46542ef",
            "9169f11300d04f4b81d2f2d9cf058d58",
            "87b71385f7c14b939e60f867f0596078",
            "10d8b881e8614d8faf02f04eb500f12c",
            "d256c7b03ccc489dbcdc5477fea46f7c",
            "22f51b6fc1b04322b6857fb233045b3b",
            "68ceb60e8a2e4c39a42536dd2cfa2157",
            "4f189eeed7da46c6b8423f0a4b51b186",
            "494dac7506c2477aa4c615ed10ed56ef",
            "ab4bfa30558143a4b66dd98a84fd6496",
            "7aae98c9160b41398e0833e42f7a0202",
            "4805c7ea211b4e3b9425b963506d7633",
            "9e6069b656b9415e82984737f3abc665",
            "d99b8c62284c498c9ba538cee6137ad0",
            "c40e234318f14ddfb06ebf5462d8eb30",
            "dbaae0c90b944c37ac85e5d58121ae7a",
            "ef1cfe9b13d240d3ad92d44c7bf81f24",
            "9d800d2110f9487f863bc81a33697172",
            "b6a3944ed0d74cccbd4c6a7f602c86d4",
            "53e35c39c84f486b99fe593448bc8743",
            "a6c7279d3e4e4904b92ec2b6cf299fd1",
            "e25239ebdc6440338e8a75dfd72ff3a0",
            "eedf6a6712f3429e9a112ac3bd82c6aa",
            "07f6670f8f9a4a9197c194f13c6ae451",
            "8a5156a9436f4865a1b9c68b5b61d8a6",
            "a610463fb75f484990975077bd4c424f",
            "b92a50a9d47e417b9305f4c716e5fc58",
            "dde33337eb774a47be27c91459c5a2c4",
            "83c70e39a7864253b1d365de25ab5383",
            "ff3f247988524c92ab9153ac88f11860",
            "b11fd17dffd44b468b5b52423d6558c2",
            "0214107e92db4314b1e63135b7d4965a",
            "4de75f08b42b46e9b5ab4ff5310e3ec9",
            "d0f7a77f25404d8895d52702f7e90ab2",
            "cd658230e5f74900863c45669d919d6a",
            "85b2230322494354ae5065efa9f2e60e",
            "b5a39566ba1a43b98ed3ad179e79c251",
            "f3f8034931a64c8b85233d9d728769be",
            "4bfdf04e124d4ab9af7b5835c39bd55a",
            "124f8c23a5f44327848f677e38a97516",
            "1ee2046cbc984b209a6d063b5f32ac23"
          ]
        },
        "outputId": "c49f432f-cf8b-447b-f90d-04a5cf2883c4"
      },
      "source": [
        "set_seed(hparams['seed'])\n",
        "experiment_name = training_method + \"-\" + model_arch + \"-\" + dataset \n",
        "project_name = \"textframework\"\n",
        "\n",
        "# logers\n",
        "\n",
        "# neptune_logger = NeptuneLogger(\n",
        "#                                project_name=\"m1f1/lightning-exps-text,\n",
        "#                                close_after_fit=False,\n",
        "#                                experiment_name=experiment_name,  # Optional,\n",
        "#                                params=hparams, # Optional,\n",
        "#                                tags=tags # Optional,\n",
        "#                               )\n",
        "\n",
        "wandb_logger = WandbLogger(project=project_name,\n",
        "                           name=experiment_name,\n",
        "                           tags=tags)\n",
        "\n",
        "wandb_logger.log_hyperparams(hparams)\n",
        "\n",
        "\n",
        "# callbacks\n",
        "early_stop_callback = EarlyStopping(\n",
        "                        monitor=\"val_loss\",\n",
        "                        min_delta=0.0,\n",
        "                        patience=hparams['patience'],\n",
        "                        verbose=True,\n",
        "                        mode='min'\n",
        "                      )\n",
        "lr_logger = LearningRateLogger()\n",
        "# Path(\"./checkpoints\").mkdir(parents=True, exist_ok=True)\n",
        "# model_checkpoint = pl.callbacks.ModelCheckpoint(filepath='./checkpoints') # check if it overwrite last checkpoint\n",
        "\n",
        "[print(f'{k}: {v}') for k, v in hparams.items()]\n",
        "\n",
        "\n",
        "if hparams['save_best_model']:\n",
        "  checkpoint_path = os.path.join(os.environ['MODEL_CHECKPOINT_PATH'], '{epoch}-{val_accuracy_error:.2f}')\n",
        "  checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                        mode='min',\n",
        "                                        monitor='val_accuracy_error',\n",
        "                                        save_top_k=1)\n",
        "else:\n",
        "  checkpoint_callback = None\n",
        "\n",
        "# training and evaluating model\n",
        "trainer = pl.Trainer(\n",
        "                gpus=1,\n",
        "                # logger=neptune_logger,\n",
        "                logger=wandb_logger,\n",
        "                checkpoint_callback=checkpoint_callback,\n",
        "                # checkpoint_callback=model_checkpoint,\n",
        "                # early_stop_callback=early_stop_callback,\n",
        "                # val_check_interval=hparams['val_check_interval'],\n",
        "                # distributed_backend=hparams['distributed_backend'],\n",
        "                # default_root_dir=\"./test_run_logs\",\n",
        "                callbacks=[lr_logger],\n",
        "                fast_dev_run=hparams['test_run'],\n",
        "              #  train_percent_check=0.001,\n",
        "              #  val_percent_check=0.001,\n",
        "                min_epochs=hparams['min_epochs'],\n",
        "                max_epochs=hparams['max_epochs'],\n",
        "          )\n",
        "\n",
        "model = LitComposableFramework(hparams)\n",
        "\n",
        "wandb_logger.watch(model, log='all', log_freq=10)\n",
        "\n",
        "trainer.fit(model)\n",
        "\n",
        "if hparams['save_best_model']:\n",
        "  model_loaded= LitComposableFramework.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
        "  trainer.test(model_loaded)\n",
        "else:\n",
        "  trainer.test(model)\n",
        "\n",
        "# neptune_logger.experiment.log_artifact('./checkpoints')\n",
        "# neptune_logger.experiment.log_artifact(os.environ['REQUIREMENTS_PATH'])\n",
        "# neptune_logger.experiment.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/mifi/textframework\" target=\"_blank\">https://app.wandb.ai/mifi/textframework</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/mifi/textframework/runs/2gma0cof\" target=\"_blank\">https://app.wandb.ai/mifi/textframework/runs/2gma0cof</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "dataset: MNIST\n",
            "num_classes: 10\n",
            "dataset_path: gdrive/My Drive/praca_magisterska/pytorch_lightning/datasets\n",
            "mean: 0.1307\n",
            "std: 0.3081\n",
            "model_arch: ConvolutionMNIST\n",
            "dropout: 0.0\n",
            "train_test_split: 0.9\n",
            "seed: 42\n",
            "val_batch_size: 16\n",
            "test_batch_size: 16\n",
            "n_labeled: 160\n",
            "valid_size_per_class: 100\n",
            "loss_function: CrossEntropyLoss\n",
            "test_run: False\n",
            "save_best_model: False\n",
            "epoch_over: labeled_train_dataset\n",
            "max_epochs: 3\n",
            "min_epochs: 1\n",
            "val_check_interval: 1\n",
            "patience: 15\n",
            "training_method: Supervised\n",
            "l_batch_size: 32\n",
            "unl_batch_size: 32\n",
            "mu: 7\n",
            "threshold: 0.7\n",
            "lambda_u: 1\n",
            "n_weak: 1\n",
            "n_strong: 2\n",
            "optimizer_type: Adam\n",
            "lr: 1e-05\n",
            "momentum: 0.9\n",
            "weight_decay: 0.0005\n",
            "nesterov: True\n",
            "scheduler_type: None\n",
            "show_augmentation: False\n",
            "T_max: 30.0\n",
            "xi: 1\n",
            "eps: 4.5\n",
            "ip: 1\n",
            "alpha: 1\n",
            "exponential_decay: 0.9998\n",
            "scheduler_T: Linear\n",
            "decay_step_size: 10000\n",
            "decay_gamma: 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "unique_labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "number_of_categories : 10\n",
            "Scheduler was not specified!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name     | Type             | Params\n",
            "----------------------------------------------\n",
            "0 | loss_fct | CrossEntropyLoss | 0     \n",
            "1 | conv1    | Conv2d           | 320   \n",
            "2 | conv2    | Conv2d           | 18 K  \n",
            "3 | dropout1 | Dropout2d        | 0     \n",
            "4 | dropout2 | Dropout2d        | 0     \n",
            "5 | fc1      | Linear           | 1 M   \n",
            "6 | fc2      | Linear           | 1 K   \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfed226301d94af7a5e000078a9ffbab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layoutâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning:\n",
            "\n",
            "You are using LearningRateLogger callback with models that have no learning rate schedulers. Please see documentation for `configure_optimizers` method.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9169f11300d04f4b81d2f2d9cf058d58",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab4bfa30558143a4b66dd98a84fd6496",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), mâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d800d2110f9487f863bc81a33697172",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), mâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a610463fb75f484990975077bd4c424f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), mâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "unique_labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "number_of_categories : 10\n",
            "Scheduler was not specified!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0f7a77f25404d8895d52702f7e90ab2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "TEST RESULTS\n",
            "{'test_accuracy_error': 0.8000856164383562,\n",
            " 'test_f1_error': 0.8000856164383562,\n",
            " 'test_loss': 2.7039173,\n",
            " 'test_max_confident': 0.82243544,\n",
            " 'test_mean_confident': 0.48174092,\n",
            " 'test_min_confident': 0.24676304,\n",
            " 'test_precision_error': 0.8000856164383562,\n",
            " 'test_recall_error': 0.8000856164383562,\n",
            " 'test_std_confident': 0.16613899}\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8Loooc9iAcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = pd.read_csv(str(Path(os.environ['DATASETS_PATH'])/'imdb_with_bt.csv'))\n",
        "# df\n",
        "# df = df.rename(columns={'intent': 'label', 'bt': 'paraphrases'})\n",
        "# df.to_csv(str(Path(os.environ['DATASETS_PATH'])/'imdb_with_bt.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfaD-N-RFmpH",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhTVU9r32eyI",
        "colab_type": "text"
      },
      "source": [
        "#### System spec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucbrH_0LsQ8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class LitYKConv_HPO(pl.LightningModule):\n",
        "\n",
        "#   def __init__(self, hparams, trial):\n",
        "\n",
        "#     super().__init__()\n",
        "#     self.hparams = hparams\n",
        "\n",
        "#     if self.hparams['with_VAT']:\n",
        "#       xi_interval = list(map(float, hparams['xi'].split(',')))\n",
        "#       eps_interval = list(map(float, hparams['eps'].split(',')))\n",
        "#       ip_interval = list(map(int, hparams['ip'].split(',')))\n",
        "#       alpha_interval = list(map(float, hparams['alpha'].split(',')))\n",
        "\n",
        "#       self.xi = trial.suggest_uniform('xi', *xi_interval)\n",
        "#       print('xi: ', self.xi)\n",
        "#       self.eps = trial.suggest_uniform('eps', *eps_interval)\n",
        "#       print('eps: ', self.eps)\n",
        "#       self.ip = trial.suggest_int('ip', *ip_interval)\n",
        "#       print('ip: ', self.ip)\n",
        "#       self.alpha = trial.suggest_uniform('alpha', *alpha_interval)\n",
        "#       print('alpha: ', self.alpha)\n",
        "\n",
        "#     lr_interval = list(map(float, hparams['lr'].split(',')))\n",
        "#     print(lr_interval)\n",
        "#     kernel_num_interval = list(map(int, hparams['kernel_num'].split(','))) \n",
        "#     print(kernel_num_interval)\n",
        "#     dropout_interval = list(map(float, hparams['dropout'].split(','))) \n",
        "#     print(dropout_interval)\n",
        "\n",
        "#     self.lr = trial.suggest_loguniform('learning_rate', *lr_interval)\n",
        "#     print('lr: ', self.lr)\n",
        "#     self.Co = trial.suggest_int('kernel_num', *kernel_num_interval) #hparams['kernel_num']\n",
        "#     print('kernel_num: ', self.Co)\n",
        "#     dropout = trial.suggest_uniform('dropout', *dropout_interval)\n",
        "#     print('dropout: ', dropout)\n",
        "#     self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "#     self.embeder_dict = {\n",
        "#                          'fastText': (create_ft_embeder, gensim_tokenizer),\n",
        "#                          'spaCy':(create_spacy_nlp_embeder, lambda x: x)\n",
        "#                         }\n",
        "#     embeder, self.tokenizer_fun = self.embeder_dict[hparams['embeder_type']]\n",
        "#     self.embeder = nlp #embeder()\n",
        "#     self.D = hparams['embed_dim']\n",
        "#     self.Ci = hparams['Ci'] \n",
        "    \n",
        "#     self.loss_fct = getattr(nn, hparams['loss_function'])()\n",
        "#     self.num_classes = hparams['num_classes']\n",
        "\n",
        "#     self.Ks = list(map(int, hparams['kernel_sizes'].split(','))) # (3,4,5)\n",
        "#     self.convs1 = nn.ModuleList([nn.Conv2d(self.Ci, self.Co, (K, self.D)) for K in self.Ks])\n",
        "#     self.fc1 = nn.Linear(len(self.Ks) * self.Co, self.num_classes) \n",
        "\n",
        "#     self.total_iterations = 0 \n",
        "\n",
        "#   def forward(self, x):\n",
        "#       # print('org: ', x.size())\n",
        "#       x = x.unsqueeze(self.Ci)  # (N, Ci, W, D)\n",
        "#       # from pdb import set_trace as st\n",
        "#       # st() \n",
        "#       # print(f'unsqueeze {self.Ci}: {x.size()}')\n",
        "#       x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
        "#       # print(f'conv, relu, squeeze : {x.size()}')\n",
        "#       x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "#       # print(f'max_pool1d, squeeze : {x.size()}')\n",
        "#       x = torch.cat(x, 1)\n",
        "#       # print(f' cat: {x.size()}')\n",
        "#       x = self.dropout(x)  # (N, len(Ks)*Co)\n",
        "#       logit = self.fc1(x)  # (N, C)\n",
        "#       # print(f' logit: {logit.size()}')\n",
        "#       return logit\n",
        "\n",
        "\n",
        "#   def prepare_data(self):\n",
        "\n",
        "#     if self.hparams['dataset'] == 'NLUHD':\n",
        "\n",
        "#       not_none = lambda x: x[\"text\"] is not None \n",
        "#       ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "#       unique_labels = list(pd.DataFrame(ds).intent.unique())\n",
        "#       le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "#       print(f\"Unique labels: {unique_labels}\")\n",
        "#       print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "#       train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "#                                                             int(len(ds) * self.hparams['train_test_split']),\n",
        "#                                                             seed=self.hparams['seed'])\n",
        "#       preprocessor = partial(\n",
        "#                             preprocess_NLUHD,\n",
        "#                             self.hparams['model_type'],\n",
        "#                             self.hparams['max_sentence_len'],\n",
        "#                             self.tokenizer_dict[self.hparams['model_type']],\n",
        "#                             le,\n",
        "#                             )\n",
        "      \n",
        "#     elif self.hparams['dataset'] == 'MR':\n",
        "\n",
        "#       preprocessor = partial(\n",
        "#                             preprocess_MR,\n",
        "#                             self.hparams,\n",
        "#                             self.tokenizer_fun, \n",
        "#                             self.embeder,\n",
        "#                             le,\n",
        "#                             )\n",
        "\n",
        "#     elif self.hparams['dataset'] == 'IMDB':\n",
        "\n",
        "#       ds = lfds.Imdb('train') + lfds.Imdb('test')\n",
        "#       ds = ds.map(lambda x: {'text': x[0], 'label': x[1]})\n",
        "#       df = pd.DataFrame(ds)\n",
        "#       # self.embeder.build_vocab(new_sentences, update=True)\n",
        "#       # self.embeder.train(new_sentences, total_examples=len(new_sentences), epochs=)\n",
        "#       print(df.info(memory_usage=True))\n",
        "#       unique_labels = list(df.label.unique())\n",
        "#       print(f'unique_labels: {unique_labels}')\n",
        "#       print(f'number_of_categories : {len(unique_labels)}')\n",
        "#       le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "#       train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "#                                                              int(len(ds) * self.hparams['train_test_split']),\n",
        "#                                                              seed=self.hparams['seed'])\n",
        "#       preprocessor = partial(\n",
        "#                              preprocess_IMDB,\n",
        "#                              self.hparams,\n",
        "#                              self.tokenizer_fun, \n",
        "#                              self.embeder,\n",
        "#                              le,\n",
        "#                             )\n",
        "#     else:\n",
        "#       raise ValueError('Wrong dataset name : {}'.format(self.hparams['dataset']))\n",
        "\n",
        "    \n",
        "#     train_df, test_df = pd.DataFrame(train), pd.DataFrame(test)\n",
        "#     x_train, y_train = train_df['text'].values, train_df['label'].values\n",
        "#     x_test, y_test = test_df['text'].values, test_df['label'].values\n",
        "\n",
        "#     # split's parameters  \n",
        "#     num_classes = len(unique_labels)\n",
        "#     label_per_class = self.hparams['n_labeled'] // num_classes\n",
        "#     valid_size = self.hparams['valid_size_per_class']\n",
        "\n",
        "#     labeled_idx = []\n",
        "#     unlabeled_idx = []\n",
        "#     val_idx = []\n",
        "    \n",
        "#     for label in unique_labels:\n",
        "#         idx = np.where(y_train == label)[0]\n",
        "#         np.random.shuffle(idx)\n",
        "#         labeled_idx.extend(idx[:label_per_class])\n",
        "#         val_idx.extend(idx[label_per_class: label_per_class + valid_size])\n",
        "#         unlabeled_idx.extend(idx[label_per_class + valid_size:])\n",
        "\n",
        "#     x_labeled, y_labeled  = x_train[labeled_idx], y_train[labeled_idx]\n",
        "#     x_unlabeled, y_unlabeled = x_train[unlabeled_idx], y_train[unlabeled_idx]\n",
        "#     x_val, y_val = x_train[val_idx], y_train[val_idx]\n",
        "    \n",
        "#     train_labeled_dataset = SimpleTextDataset(x_labeled,\n",
        "#                                               y_labeled,\n",
        "#                                               transform=preprocessor)\n",
        "    \n",
        "#     train_unlabeled_dataset = SimpleTextDataset(x_unlabeled,\n",
        "#                                                 y_unlabeled,\n",
        "#                                                 transform=preprocessor)\n",
        "    \n",
        "#     self._train_dataset = TwoInOneDataset([train_labeled_dataset,\n",
        "#                                             train_unlabeled_dataset])\n",
        "    \n",
        "#     self._val_dataset = SimpleTextDataset(x_val,\n",
        "#                                           y_val,\n",
        "#                                           transform=preprocessor)\n",
        "    \n",
        "#     self._test_dataset = SimpleTextDataset(x_test,\n",
        "#                                            y_test,\n",
        "#                                            transform=preprocessor)\n",
        "    \n",
        "#     self.total_iterations = len(train_unlabeled_dataset) // self.hparams['batch_size'] \n",
        "\n",
        "\n",
        "#   def train_dataloader(self):\n",
        "#     return DataLoader(\n",
        "#                       self._train_dataset,\n",
        "#                       batch_size=self.hparams['batch_size'],\n",
        "#                       num_workers=8,\n",
        "#                       shuffle=True # without shuffle it want work cause it need to create map index before __get_item__ function\n",
        "#                      )\n",
        "    \n",
        "  \n",
        "#   def val_dataloader(self):\n",
        "#     return DataLoader(\n",
        "#                       self._val_dataset,\n",
        "#                       batch_size=self.hparams['batch_size'],\n",
        "#                       num_workers=8\n",
        "#                      )\n",
        "    \n",
        "  \n",
        "#   def test_dataloader(self):\n",
        "#     return DataLoader(\n",
        "#                       self._test_dataset,\n",
        "#                       batch_size=self.hparams['batch_size'],\n",
        "#                       num_workers=8\n",
        "#                      )\n",
        "    \n",
        "  \n",
        "#   def configure_optimizers(self):\n",
        "\n",
        "#     optimizers = [\n",
        "#                   torch.optim.Adam(self.parameters(), lr=self.lr),\n",
        "#                  ]\n",
        "#     schedulers = [\n",
        "#                   {\n",
        "#                     'scheduler': ReduceLROnPlateau(optimizers[0],'min', verbose=True), \n",
        "#                     'monitor': 'val_loss', # Default: val_loss\n",
        "#                     'interval': 'epoch',\n",
        "#                     'frequency': 1\n",
        "#                   },\n",
        "#                  ]\n",
        "\n",
        "#     return optimizers, schedulers\n",
        "\n",
        "\n",
        "#   def training_step(self, batch, batch_idx):\n",
        "\n",
        "#     l_batch = batch[0]\n",
        "#     l_texts = l_batch['embedding']\n",
        "#     labels = l_batch['label']\n",
        "#     unl_texts = batch[1]['embedding']\n",
        "\n",
        "#     if self.hparams['with_VAT']:\n",
        "#       vat_loss = VATLoss(xi=self.xi,\n",
        "#                          eps=self.eps,\n",
        "#                          ip=self.ip)\n",
        "#       lds = vat_loss(self, unl_texts)\n",
        "\n",
        "#     logits = self.forward(l_texts)\n",
        "#     loss = self.loss_fct(logits, labels)\n",
        "\n",
        "#     if self.hparams['with_VAT']:\n",
        "#       loss += self.alpha * lds \n",
        "\n",
        "#     labels_hat = logits.max(dim=1)[1]\n",
        "\n",
        "#     labels = labels.detach().cpu()\n",
        "#     labels_hat = labels_hat.detach().cpu()\n",
        "    \n",
        "#     accuracy_error = torch.tensor(1 - accuracy_score(labels, labels_hat))\n",
        "#     f1_error = torch.tensor(1 - f1_score(labels, labels_hat, average='micro'))\n",
        "#     recall_error = torch.tensor(1 - recall_score(labels, labels_hat, average='micro'))\n",
        "#     precision_error = torch.tensor(1 - precision_score(labels, labels_hat, average='micro'))\n",
        "\n",
        "#     logs = {'train_loss': loss,\n",
        "#             'train_accuracy_error': accuracy_error,\n",
        "#             'train_f1_error': f1_error,\n",
        "#             'train_recall_error': recall_error,\n",
        "#             'train_precision_error': precision_error,\n",
        "#            }  \n",
        "\n",
        "#     if self.hparams['with_VAT']:\n",
        "#       logs.update({'lds_loss': lds.item()})\n",
        "\n",
        "\n",
        "#     return {'loss': loss,\n",
        "#             'log': logs}\n",
        "\n",
        "\n",
        "#   def validation_step(self, batch, batch_idx):\n",
        "#     texts = batch['embedding']\n",
        "#     labels = batch['label']\n",
        "  \n",
        "#     logits = self.forward(texts)\n",
        "#     loss = self.loss_fct(logits, labels)\n",
        "#     labels_hat = torch.argmax(logits, dim=1)\n",
        "\n",
        "#     labels = labels.cpu()\n",
        "#     labels_hat = labels_hat.cpu()\n",
        "\n",
        "#     accuracy_error = torch.tensor(1 - accuracy_score(labels, labels_hat))\n",
        "#     f1_error = torch.tensor(1 - f1_score(labels, labels_hat, average='micro'))\n",
        "#     recall_error = torch.tensor(1 - recall_score(labels, labels_hat, average='micro'))\n",
        "#     precision_error = torch.tensor(1 - precision_score(labels, labels_hat, average='micro'))\n",
        "  \n",
        "#     output = {\n",
        "#             \"val_loss\": loss,\n",
        "#             'accuracy_error': accuracy_error,\n",
        "#             'f1_error': f1_error,\n",
        "#             'recall_error': recall_error,\n",
        "#             'precision_error': precision_error,\n",
        "#             }\n",
        "  \n",
        "#     return output\n",
        "\n",
        "\n",
        "#   def validation_epoch_end(self, outputs):\n",
        "#     # CHANGE FOR TENSORS!!!!\n",
        "#     val_acc = compute_global_metric(outputs, 'accuracy_error')\n",
        "#     val_f1 = compute_global_metric(outputs, 'f1_error')\n",
        "#     val_recall = compute_global_metric(outputs, 'recall_error')\n",
        "#     val_precision = compute_global_metric(outputs, 'precision_error')\n",
        "#     val_loss = compute_global_metric(outputs, \"val_loss\")\n",
        "\n",
        "#     tqdm_dict = {\n",
        "#                  \"val_loss\": val_loss,\n",
        "#                  \"val_acc\": val_acc,\n",
        "#                  \"val_f1\": val_f1,\n",
        "#                  \"val_recall\": val_recall,\n",
        "#                  \"val_precision\": val_precision,\n",
        "#                 }\n",
        "#     return {\n",
        "#             \"progress_bar\": tqdm_dict,\n",
        "#             \"log\": tqdm_dict,\n",
        "#             \"val_loss\": val_loss,\n",
        "#             'val_acc': val_acc,\n",
        "#             'val_f1': val_f1\n",
        "#            }\n",
        "\n",
        "\n",
        "#   def test_step(self, batch, batch_idx):\n",
        "\n",
        "#     texts = batch[\"embedding\"]\n",
        "#     labels = batch[\"label\"]\n",
        "  \n",
        "#     logits = self.forward(texts)\n",
        "#     loss = self.loss_fct(logits, labels)\n",
        "#     labels_hat = torch.argmax(logits, dim=1)\n",
        "\n",
        "#     labels = labels.cpu()\n",
        "#     labels_hat = labels_hat.cpu()\n",
        "\n",
        "\n",
        "#     accuracy_error = torch.tensor(1 - accuracy_score(labels, labels_hat))\n",
        "#     f1_error = torch.tensor(1 - f1_score(labels, labels_hat, average='micro'))\n",
        "#     recall_error = torch.tensor(1 - recall_score(labels, labels_hat, average='micro'))\n",
        "#     precision_error = torch.tensor(1 - precision_score(labels, labels_hat, average='micro'))\n",
        "  \n",
        "#     return {\n",
        "#             \"test_loss\": loss,\n",
        "#             'accuracy_error': accuracy_error,\n",
        "#             'f1_error': f1_error,\n",
        "#             'recall_error': recall_error,\n",
        "#             'precision_error': precision_error,\n",
        "#            }\n",
        "\n",
        "\n",
        "#   def test_epoch_end(self, outputs):\n",
        "\n",
        "#     test_acc = compute_global_metric(outputs, 'accuracy_error') \n",
        "#     test_f1 = compute_global_metric(outputs, 'f1_error')\n",
        "#     test_recall = compute_global_metric(outputs, 'recall_error')\n",
        "#     test_precision = compute_global_metric(outputs, 'precision_error')\n",
        "#     test_loss = compute_global_metric(outputs, \"test_loss\")\n",
        "\n",
        "#     tqdm_dict = {\n",
        "#                  \"test_loss\": test_loss,\n",
        "#                  \"test_acc\": test_acc,\n",
        "#                  \"test_f1\": test_f1,\n",
        "#                  \"test_recall\": test_recall,\n",
        "#                  \"test_precision\": test_precision,\n",
        "#                 }\n",
        "#     return {\n",
        "#             \"progress_bar\": tqdm_dict,\n",
        "#             \"log\": tqdm_dict,\n",
        "#             \"test_loss\": test_loss,\n",
        "#             'test_acc': test_acc,\n",
        "#             'test_f1': test_f1\n",
        "#            }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8blDj0ji58J4",
        "colab_type": "text"
      },
      "source": [
        "#### Define objective func\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPAMdmP6F-DM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def objective(trial):\n",
        "#   # nluhd_dataset_path = str(Path()/\n",
        "#   #                         'gdrive'/\n",
        "#   #                         'My Drive'/\n",
        "#   #                         'praca_magisterska'/\n",
        "#   #                         'pytorch_lightning'/\n",
        "#   #                         'datasets'/\n",
        "#   #                         'NLU-Data-Home-Domain-preprocessed-without-ner.csv')\n",
        "#   hparams = {\n",
        "#             # model architecture\n",
        "#             'model_type': 'YoonKimConvNN',\n",
        "#             'dropout':'0.2,0.7', \n",
        "#             'kernel_sizes': '3,4,5', # (3,4,5)\n",
        "#             'kernel_num': '60,120', # interval\n",
        "#             'Ci': 1,\n",
        "#             'loss_function':'CrossEntropyLoss',\n",
        "#             # pl trainer params\n",
        "#             'seed': 42,\n",
        "#             'monitor_value': 'val_acc',\n",
        "#             'percent_valid_examples': 0.5, \n",
        "#             'test_run': False,\n",
        "#             'with_VAT': True,\n",
        "#             'max_epochs': 2,\n",
        "#             'min_epochs': 1,\n",
        "#             'val_check_interval': 0.5, \n",
        "#             'patience': 3, # early stopping callback parameter\n",
        "#             'distributed_backend': 'dp',\n",
        "#             # embeddings params\n",
        "#             'embeder_type': \"fastText\",\n",
        "#             'embed_dim': 300,\n",
        "#             'max_sentence_len': 400,\n",
        "#             # dataset params\n",
        "#             'train_test_split': 0.8,\n",
        "#             'batch_size': 32,\n",
        "#             'n_labeled': 1000, # number of labeled samples \n",
        "#             'valid_size_per_class': 1000, # 68 class => n_val_samples = 68 * 10 \n",
        "#             # optimizer params\n",
        "#             'lr': '0.00001, 10',\n",
        "#             # VAT params\n",
        "#             'xi':'6,12',\n",
        "#             'eps':'1,3',\n",
        "#             'ip':'1,3',\n",
        "#             'alpha':'1,3',\n",
        "#             }\n",
        "  \n",
        "#   set_seed(hparams['seed'])\n",
        "  \n",
        "#   hparams.update({'dataset':'IMDB',\n",
        "#                   'num_classes': 2})\n",
        "  \n",
        "  \n",
        "#   # training and evaluating model\n",
        "  \n",
        "          \n",
        "#   metrics_callback = MetricsCallback()\n",
        "  \n",
        "#   checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "#           os.path.join(os.environ['RESULT_PATH'],\n",
        "#                        \"trial_{}\".format(trial.number),\n",
        "#                        \"{epoch}\"), monitor=hparams['monitor_value']\n",
        "#       )\n",
        "  \n",
        "#   trainer = pl.Trainer(\n",
        "#                   gpus=1 if torch.cuda.is_available() else None,\n",
        "#                   logger=False,\n",
        "#                   # val_percent_check=hparams['percent_valid_examples'],\n",
        "#                   checkpoint_callback=checkpoint_callback,\n",
        "#                   max_epochs=hparams['max_epochs'],\n",
        "#                   fast_dev_run=hparams['test_run'],\n",
        "#                   callbacks=[metrics_callback],\n",
        "#                   early_stop_callback=PyTorchLightningPruningCallback(trial, monitor=hparams['monitor_value'])\n",
        "#             )\n",
        "  \n",
        "#   model = LitYKConv_HPO(hparams, trial=trial)\n",
        "  \n",
        "#   trainer.fit(model)\n",
        "#   return metrics_callback.metrics[-1][\"val_acc\"]\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjAj0j4L5vTG",
        "colab_type": "text"
      },
      "source": [
        "#### Run trails"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuNsLhst6BPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pruning = True\n",
        "\n",
        "# pruner = optuna.pruners.MedianPruner() if pruning else optuna.pruners.NopPruner()\n",
        "\n",
        "# study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
        "# study.optimize(objective, n_trials=20, timeout=None)\n",
        "\n",
        "# print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "# print(\"Best trial:\")\n",
        "# trial = study.best_trial\n",
        "\n",
        "# print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "# print(\"  Params: \")\n",
        "# for key, value in trial.params.items():\n",
        "#     print(\"    {}: {}\".format(key, value))\n",
        "# # shutil.rmtree(os.environ['RESULT_PATH'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8LrwpD19Y3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optuna.visualization.plot_intermediate_values(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y5eSN9y9Xqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optuna.visualization.plot_param_importances(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRChtuTLMfxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}