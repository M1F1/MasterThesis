{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemiSupervisedComposableFramework.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_DWq_Q17ENpc",
        "_9cu0GG3qjHz",
        "aHn7X8Oc2IJN",
        "AhTVU9r32eyI",
        "8blDj0ji58J4",
        "AjAj0j4L5vTG"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M1F1/MasterThesis/blob/master/SemiSupervisedComposableFramework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmIolHgLI6Yn",
        "colab_type": "text"
      },
      "source": [
        "### Setup env "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE7eYLcuXnX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ff7bc8e-b057-4391-dd99-658fe0a8d313"
      },
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "import pkg_resources\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "if importlib.util.find_spec('neptune') is None:\n",
        "  !pip install neptune-client\n",
        "\n",
        "if importlib.util.find_spec('pytorch_lightning') is None:\n",
        "  !pip install pytorch-lightning\n",
        "\n",
        "if importlib.util.find_spec('logzero') is None:\n",
        "  !pip install logzero \n",
        "\n",
        "if importlib.util.find_spec('tensorboardX') is None:\n",
        "  !pip install tensorboardX \n",
        "\n",
        "if importlib.util.find_spec('lineflow') is None:\n",
        "  !pip install lineflow\n",
        "\n",
        "if importlib.util.find_spec('optuna') is None:\n",
        "  !pip install optuna\n",
        "\n",
        "#if importlib.util.find_spec('gdown') is None:\n",
        "!pip install gdown==3.11.0\n",
        "  \n",
        "if importlib.util.find_spec('transformers') is None:\n",
        "  !pip install transformers \n",
        "  \n",
        "if importlib.util.find_spec('nlpaug') is None:\n",
        "  !pip install nlpaug \n",
        "\n",
        "import gdown\n",
        "import contextlib\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from typing import Dict\n",
        "import re\n",
        "import time\n",
        "\n",
        "import lineflow as lf\n",
        "import lineflow.datasets as lfds\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset, sampler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "if pkg_resources.parse_version(pl.__version__) < pkg_resources.parse_version(\"0.7.1\"):\n",
        "  raise RuntimeError(\"PyTorch Lightning>=0.7.1 is required for this code.\")\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.logging.neptune import NeptuneLogger \n",
        "from pytorch_lightning import Callback\n",
        "\n",
        "from gensim.utils import tokenize as gensim_tokenizer\n",
        "import gensim\n",
        "from gensim.models.fasttext import FastText as FT_gensim\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, RobertaTokenizer, RobertaModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup \n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "import toolz\n",
        "from nltk.corpus import stopwords\n",
        "import optuna\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.model.word_stats as nmw\n",
        "import nlpaug.flow as nafc\n",
        "from nlpaug.util import Action\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "!pip freeze > requirements.txt\n",
        "\n",
        "os.environ['PROJECT_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning')\n",
        "os.environ['DATASETS_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'datasets')\n",
        "os.environ['REQUIREMENTS_PATH'] = str(Path()/'requirements.txt')\n",
        "# Watch out for this path\n",
        "os.environ['RESULT_PATH'] = str(Path()/'result')\n",
        "os.environ['SPELLING_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'nlpaug'/'spelling_en.txt') \n",
        "os.environ['NLPAUG_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'nlpaug')\n",
        "artefacts_temp_dir = os.path.join(os.environ['PROJECT_PATH'], 'parametrized_nbs')\n",
        "\n",
        "neptune_api_token_key_file = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'neptune_api_token.txt')\n",
        "with open (neptune_api_token_key_file, 'r') as f:\n",
        "  os.environ['NEPTUNE_API_TOKEN'] = f.readlines()[0]\n",
        "\n",
        "if not os.path.exists(artefacts_temp_dir):\n",
        "  os.makedirs(artefacts_temp_dir)\n",
        "\n",
        "if not os.path.exists(os.environ['RESULT_PATH']):\n",
        "  os.makedirs(os.environ['RESULT_PATH'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Collecting neptune-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/c7/bfd27803f482f58b150fa2145ba1fa28797d674ee9ce7468f992e580ffb1/neptune-client-0.4.117.tar.gz (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.6MB/s \n",
            "\u001b[?25hCollecting bravado\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/cc/b3c8dadc3f51fa184db10172f031c1c5206b0e67f3207217bbdd326e81a4/bravado-10.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 24.7MB/s \n",
            "\u001b[?25hCollecting py3nvml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/b3/cb30dd8cc1198ae3fdb5a320ca7986d7ca76e23d16415067eafebff8685f/py3nvml-0.2.6-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from neptune-client) (1.0.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (7.0.0)\n",
            "Collecting PyJWT\n",
            "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (1.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (1.12.0)\n",
            "Collecting websocket-client>=0.35.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 51.6MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/f9/c315aa88e51fabdc08e91b333cfefb255aff04a2ee96d632c32cb19180c9/GitPython-3.1.3-py3-none-any.whl (451kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 55.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from neptune-client) (20.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client) (3.6.6)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client) (2.8.1)\n",
            "Collecting monotonic\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Collecting simplejson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/a7b98aa9256c8843f92878966dc3d8d914c14aad97e2c5ce4798d5743e07/simplejson-3.17.0.tar.gz (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.6MB/s \n",
            "\u001b[?25hCollecting msgpack-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/20/6eca772d1a5830336f84aca1d8198e5a3f4715cd1c7fc36d3cc7f7185091/msgpack-python-0.5.6.tar.gz (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 53.0MB/s \n",
            "\u001b[?25hCollecting bravado-core>=5.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/11/18e9d28a156c33f2d5f15a5e155dc7130250acb0a569255a2b6b307b596d/bravado_core-5.17.0-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client) (3.13)\n",
            "Collecting xmltodict\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client) (1.18.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client) (2020.4.5.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client) (1.24.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->neptune-client) (2.4.7)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (1.0.0)\n",
            "Collecting swagger-spec-validator>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/09/de/e78cefbf5838b434b63a789264b79821cb2267f1498fbed23ef8590133e4/swagger_spec_validator-2.7.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2.6.0)\n",
            "Collecting jsonref\n",
            "  Downloading https://files.pythonhosted.org/packages/07/92/f8e4ac824b14af77e613984e480fa818397c72d4141fc466decb26752749/jsonref-0.2-py3-none-any.whl\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Collecting strict-rfc3339; extra == \"format\"\n",
            "  Downloading https://files.pythonhosted.org/packages/56/e4/879ef1dbd6ddea1c77c0078cd59b503368b0456bcca7d063a870ca2119d3/strict-rfc3339-0.7.tar.gz\n",
            "Collecting rfc3987; extra == \"format\"\n",
            "  Downloading https://files.pythonhosted.org/packages/65/d4/f7407c3d15d5ac779c3dd34fbbc6ea2090f77bd7dd12f207ccf881551208/rfc3987-1.3.8-py2.py3-none-any.whl\n",
            "Collecting webcolors; extra == \"format\"\n",
            "  Downloading https://files.pythonhosted.org/packages/12/05/3350559de9714b202e443a9e6312937341bd5f79f4e4f625744295e7dd17/webcolors-1.11.1-py3-none-any.whl\n",
            "Building wheels for collected packages: neptune-client, future, simplejson, msgpack-python, strict-rfc3339\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.4.117-py2.py3-none-any.whl size=149569 sha256=b866124ed005d951e0b7ba380835e15ea39a5cd7622f8ca4f8eec147417828ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/d6/14/b0ff025b804bd28f3580e5832a82e9aaa0296ad47742fa2c0c\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=5298b81b979065c90e2397b31826fe78f504d7031cd0d7e6d79033c4d36e9445\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for simplejson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplejson: filename=simplejson-3.17.0-cp36-cp36m-linux_x86_64.whl size=114207 sha256=411a36e430e33cb3feaf2b2dc21dc92d2a6e3b0d2a808a8ba4b61f5f7bd4e61a\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/c0/83/dcd0339abb2640544bb8e0938aab2d069cef55e5647ce6e097\n",
            "  Building wheel for msgpack-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack-python: filename=msgpack_python-0.5.6-cp36-cp36m-linux_x86_64.whl size=304224 sha256=5450507889aac9b3b57b1b5cc59097f96bb012b77805c3e50d0887d437a7e130\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/de/86/7fa56fda12511be47ea0808f3502bc879df4e63ab168ec0406\n",
            "  Building wheel for strict-rfc3339 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strict-rfc3339: filename=strict_rfc3339-0.7-cp36-none-any.whl size=18120 sha256=f0fa2e6b5d78f706a342d295ffa1b1f741080e8cb074ab43ec93f1453f8b5a73\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/af/c9/b6e9fb5f9b2470e4ed2a7241c9ab3a8cdd3bc8555ae02ca2e6\n",
            "Successfully built neptune-client future simplejson msgpack-python strict-rfc3339\n",
            "Installing collected packages: monotonic, simplejson, msgpack-python, swagger-spec-validator, jsonref, bravado-core, bravado, future, xmltodict, py3nvml, PyJWT, websocket-client, smmap, gitdb, GitPython, neptune-client, strict-rfc3339, rfc3987, webcolors\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed GitPython-3.1.3 PyJWT-1.7.1 bravado-10.6.2 bravado-core-5.17.0 future-0.18.2 gitdb-4.0.5 jsonref-0.2 monotonic-1.5 msgpack-python-0.5.6 neptune-client-0.4.117 py3nvml-0.2.6 rfc3987-1.3.8 simplejson-3.17.0 smmap-3.0.4 strict-rfc3339-0.7 swagger-spec-validator-2.7.3 webcolors-1.11.1 websocket-client-0.57.0 xmltodict-0.12.0\n",
            "Collecting pytorch-lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/8e/d1bb6f3696aaed40bf8263c0d9be95593dbc6a63921cca68f0e7f60e7893/pytorch_lightning-0.8.1-py3-none-any.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.2.2)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 34.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (4.41.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (3.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (47.3.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.6.0.post3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (0.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (3.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.29.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (1.17.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning) (0.34.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2020.4.5.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning) (1.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.1.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.4.8)\n",
            "Building wheels for collected packages: PyYAML\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=6e1cdc4064a6a64bf96c8724b227210cb535e08762321dc54f529bb859b1faf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built PyYAML\n",
            "Installing collected packages: PyYAML, pytorch-lightning\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1 pytorch-lightning-0.8.1\n",
            "Collecting logzero\n",
            "  Downloading https://files.pythonhosted.org/packages/97/24/27295d318ea8976b12cf9cc51d82e7c7129220f6a3cc9e3443df3be8afdb/logzero-1.5.0-py2.py3-none-any.whl\n",
            "Installing collected packages: logzero\n",
            "Successfully installed logzero-1.5.0\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (47.3.1)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.0\n",
            "Collecting lineflow\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/09/6e8842e1c8ace250d352cf59503e3939169d8a3abe0d17115d5916054930/lineflow-0.6.4.tar.gz\n",
            "Collecting arrayfiles\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/be/297c365c7f8304ffa949bb252eac9d67e5c708a3615c9d5b18a7613e9006/arrayfiles-0.0.1.tar.gz\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from lineflow) (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown->lineflow) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown->lineflow) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown->lineflow) (1.12.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->lineflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->lineflow) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->lineflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->lineflow) (2.9)\n",
            "Building wheels for collected packages: lineflow, arrayfiles\n",
            "  Building wheel for lineflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lineflow: filename=lineflow-0.6.4-cp36-none-any.whl size=23202 sha256=8f29cb89dc7b00ec8ece3b99c7a332d9410f5cf511f058cff19312e445e1557d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/11/32/a6120f98d7d11ed8cf1b28b265a12a4b72842da341c13384c1\n",
            "  Building wheel for arrayfiles (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for arrayfiles: filename=arrayfiles-0.0.1-cp36-none-any.whl size=5434 sha256=0bc520812a3c92bcb87d534bf960995a14107b9edf925e77d7ec0b5610a2cff3\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/43/9b/fb9049f9c3931d703182bd45418c3e8d67a6b6eded6325e16e\n",
            "Successfully built lineflow arrayfiles\n",
            "Installing collected packages: arrayfiles, lineflow\n",
            "Successfully installed arrayfiles-0.0.1 lineflow-0.6.4\n",
            "Collecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/32/266d4afd269e3ecd7fcc595937c1733f65eae6c09c3caea74c0de0b88d78/optuna-1.5.0.tar.gz (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 11.7MB/s \n",
            "\u001b[?25hCollecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1e/cabc75a189de0fbb2841d0975243e59bde8b7822bacbb95008ac6fe9ad47/alembic-1.4.2.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 31.1MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/59/4db149d8962dc29a37c8bc08cd79185935527af9a27259a2d80cac707212/cliff-3.3.0-py3-none-any.whl (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.1MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0e/7f/ebba8a7950487c760c245168f7ba318b35bf0cac9c0eba30b9fb50150a20/cmaes-0.5.1-py3-none-any.whl\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/00/0d/22c73c2eccb21dd3498df7d22c0b1d4a30f5a5fb3feb64e1ce06bc247747/colorlog-4.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.15.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.17)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.12.0)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (5.3.1)\n",
            "Collecting cmd2!=0.8.3,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/b2/440d8d89b432b250c8f4db4368301aa7c6b49ab48a87b42e2a36f60c104d/cmd2-1.1.0-py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 47.4MB/s \n",
            "\u001b[?25hCollecting stevedore>=1.20.0\n",
            "  Downloading https://files.pythonhosted.org/packages/45/62/aa4c77e0f0899b7697445d8126fd099473452488d70f877426812c2ce982/stevedore-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/ba/aa953a11ec014b23df057ecdbc922fdb40ca8463466b1193f3367d2711a6/pbr-5.4.5-py2.py3-none-any.whl (110kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: setuptools>=34.4 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (47.3.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.4)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (19.3.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/5b/55866e1cde0f86f5eec59dab5de8a66628cb0d53da74b8dbc15ad8dabda3/pyperclip-1.8.0.tar.gz\n",
            "Building wheels for collected packages: alembic\n",
            "  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.2-cp36-none-any.whl size=159543 sha256=6855c61700fb863cb99f0d5dc0467ff1c51c16c675f47e480a214e74f0999670\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/04/83/76023f7a4c14688c0b5c2682a96392cfdd3ee4449eaaa287ef\n",
            "Successfully built alembic\n",
            "Building wheels for collected packages: optuna, pyperclip\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-1.5.0-cp36-none-any.whl size=276145 sha256=4424963a84b9fb5a9ce2207e301d3eaf7fcbc3035c283c585cbf3187340d579f\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/21/78/4f5529e0c757ababc4217eb9adf1886d21eb22bb1ab98c33c5\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.0-cp36-none-any.whl size=8693 sha256=3b6571c2c2f07ef4aaa15c5d5d52f63dabd9eeb69f60b697ec0c3923c4170b25\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/ac/0a/b784f0afe26eaf52e88a7e15c7369090deea0354fa1c6fc689\n",
            "Successfully built optuna pyperclip\n",
            "Installing collected packages: Mako, python-editor, alembic, colorama, pyperclip, cmd2, pbr, stevedore, cliff, cmaes, colorlog, optuna\n",
            "Successfully installed Mako-1.1.3 alembic-1.4.2 cliff-3.3.0 cmaes-0.5.1 cmd2-1.1.0 colorama-0.4.3 colorlog-4.1.0 optuna-1.5.0 pbr-5.4.5 pyperclip-1.8.0 python-editor-1.0.4 stevedore-2.0.1\n",
            "Collecting gdown==3.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/db/f9/757abd4b0ebf60f3d276b599046c515c070fab5161b22abb952e35f3c0a4/gdown-3.11.0.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (1.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (3.0.12)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (2020.4.5.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.11.0-cp36-none-any.whl size=9619 sha256=8745c09caa68091fdfa6c71a490db7ab793792066cc5774f3615f527b067e767\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/a6/67/ebb80360fc57bb0ddd5da77f57b275084cd8838bf7d5b91685\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "Successfully installed gdown-3.11.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 11.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 22.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=7208f9fb5f93bd952ac74abfb1487cb52274edc47255279f310344b130f35bf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n",
            "Collecting nlpaug\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/6c/ca85b6bd29926561229e8c9f677c36c65db9ef1947bfc175e6641bc82ace/nlpaug-0.0.14-py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.7MB/s \n",
            "\u001b[?25hInstalling collected packages: nlpaug\n",
            "Successfully installed nlpaug-0.0.14\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu76gSLTpagD",
        "colab_type": "text"
      },
      "source": [
        "### Dataset statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3KMkjEDKPoE",
        "colab_type": "text"
      },
      "source": [
        "#### IMDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isBfou21d5f7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train = lfds.Imdb('train')\n",
        "# test = lfds.Imdb('test')\n",
        "# dataset = train + test\n",
        "# ds = dataset.map(lambda x: {'text': x[0], 'label':x[1], 'tokens_len': len(list(gensim_tokenizer(x[0])))})\n",
        "# df = pd.DataFrame(ds)\n",
        "# df['tokens_len'].describe()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IPyAeQnwtjk",
        "colab_type": "text"
      },
      "source": [
        "### Embeders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj41DiVYK6ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_ft_embeder():\n",
        "  my_file = Path(\"./cc.en.300.bin\")\n",
        "  # my_file = Path(\"./wiki-news-300d-1M.vec.zip\")\n",
        "  if not my_file.is_file():\n",
        "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "    # !https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "    !gunzip cc.en.300.bin.gz\n",
        "  print('Loading fastText model into memory, it can take a while...')\n",
        "  start = time.time()\n",
        "  # ft = gensim.models.fasttext.load_text_format()  # Original fasttext embeddings from https://fasttext.cc/\n",
        "  ft = gensim.models.FastText.load_fasttext_format(\"./cc.en.300.bin\")\n",
        "  # ft = FT_gensim.load(\"./cc.en.300.bin\")\n",
        "  # ft = fasttext.load_model(\"./cc.en.300.bin\")\n",
        "  end = time.time()\n",
        "  duration =  end - start\n",
        "  print(f'Loading took: {duration} s')\n",
        "  return ft\n",
        "\n",
        "def create_spacy_nlp_embeder():\n",
        "  my_file = Path(\"./crawl-300d-2M.vec.zip\")\n",
        "  if not my_file.is_file():\n",
        "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip \n",
        "  my_file = Path(\"./en_vectors_wiki_lg\")\n",
        "  if not my_file.is_file():\n",
        "    !python -m spacy init-model en /en_vectors_wiki_lg --vectors-loc crawl-300d-2M.vec.zip\n",
        "  print('Loading fastText vectors into spaCy model into memory, it can take a while...')\n",
        "  start = time.time()\n",
        "  nlp = spacy.load(\"/en_vectors_wiki_lg\")\n",
        "  end = time.time()\n",
        "  duration =  end - start\n",
        "  print(f'Loading took: {duration} s')\n",
        "  return nlp \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbMKpkCwWlXj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "997e681f-8862-4eb6-83f1-d31e2d21cf0c"
      },
      "source": [
        "nlp = create_spacy_nlp_embeder()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-25 17:16:47--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: ‘crawl-300d-2M.vec.zip’\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G  7.99MB/s    in 7m 17s  \n",
            "\n",
            "2020-06-25 17:24:05 (3.33 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
            "\n",
            "✔ Successfully created model\n",
            "⠙ Reading vectors from crawl-300d-2M.vec.ziptcmalloc: large alloc 2400002048 bytes == 0x3dac000 @  0x7fc358bf5001 0x7fc356759765 0x7fc3567bdbb0 0x7fc3567bfa4f 0x7fc356856048 0x50a635 0x50cd96 0x509758 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x588d41 0x59fc4e 0x50d356 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x516345 0x50a2bf 0x50bfb4 0x507d64\n",
            "1999995it [03:03, 10899.95it/s]\n",
            "✔ Loaded vectors from crawl-300d-2M.vec.zip\n",
            "✔ Sucessfully compiled vocab\n",
            "2000191 entries, 1999995 vectors\n",
            "Loading fastText vectors into spaCy model into memory, it can take a while...\n",
            "Loading took: 16.535643577575684 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wggG_1T2gT_y",
        "colab_type": "text"
      },
      "source": [
        "### VAT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEXOJ_dmgWOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@contextlib.contextmanager\n",
        "def _disable_tracking_bn_stats(model):\n",
        "\n",
        "    def switch_attr(m):\n",
        "        if hasattr(m, 'track_running_stats'):\n",
        "            m.track_running_stats ^= True\n",
        "    model.apply(switch_attr)\n",
        "    yield\n",
        "    model.apply(switch_attr)\n",
        "\n",
        "class VATLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, xi=10.0, eps=1.0, ip=1):\n",
        "        \"\"\"VAT loss\n",
        "        :param xi: hyperparameter of VAT (default: 10.0)\n",
        "        :param eps: hyperparameter of VAT (default: 1.0)\n",
        "        :param ip: iteration times of computing adv noise (default: 1)\n",
        "        \"\"\"\n",
        "        super(VATLoss, self).__init__()\n",
        "\n",
        "        self.xi = xi\n",
        "        self.eps = eps\n",
        "        self.ip = ip\n",
        "\n",
        "    def forward(self, model, x):\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            pred = F.softmax(logits, dim=1)\n",
        "\n",
        "        # prepare random unit tensor\n",
        "\n",
        "        d = torch.rand(x.shape).sub(0.5).to(x.device)\n",
        "        def _l2_normalize(d):\n",
        "          d_reshaped = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))\n",
        "          d /= torch.norm(d_reshaped, dim=1, keepdim=True) + 1e-8\n",
        "          return d\n",
        "\n",
        "        d = _l2_normalize(d)\n",
        "        # self.forward as model it may crush?\n",
        "        with _disable_tracking_bn_stats(model):\n",
        "            # calc adversarial direction\n",
        "          for _ in range(self.ip):\n",
        "              d.requires_grad_()\n",
        "              pred_hat = model(x + self.xi * d)\n",
        "              logp_hat = F.log_softmax(pred_hat, dim=1)\n",
        "              adv_distance = F.kl_div(logp_hat, pred, reduction='batchmean')\n",
        "              adv_distance.backward(retain_graph=True)\n",
        "              d = _l2_normalize(d.grad)\n",
        "              model.zero_grad()\n",
        "\n",
        "        # calc LDS\n",
        "        r_adv = d * self.eps\n",
        "        pred_hat = model(x + r_adv)\n",
        "        logp_hat = F.log_softmax(pred_hat, dim=1)\n",
        "        lds = F.kl_div(logp_hat, pred, reduction='batchmean')\n",
        "\n",
        "        return lds "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfNoF2dG1yGl",
        "colab_type": "text"
      },
      "source": [
        "### FixMatch Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nIV2Bly11Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------------    \n",
        "# augmentations on char level \n",
        "# ---------------------------------------------    \n",
        "def print_augmentations(func, text):\n",
        "  def wrapper():\n",
        "    print(\"Augmentation function: \", func.__name__)\n",
        "    print(\"Original: \")\n",
        "    print(text)\n",
        "    augmented_text = func(text)\n",
        "    print(\"Augmention result: \")\n",
        "    print(augmented_text)\n",
        "    return augmented_text\n",
        "\n",
        "  return wrapper() \n",
        "\n",
        "def substitute_character_by_keyboard_distance(text):\n",
        "  aug = nac.KeyboardAug()\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "\n",
        "def insert_character_randomly(text):\n",
        "  aug = nac.RandomCharAug(action=\"insert\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def substitute_character_randomly(text):\n",
        "  aug = nac.RandomCharAug(action=\"substitute\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def delete_char_randomly(text):\n",
        "  aug = nac.RandomCharAug(action=\"delete\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def swap_character_randomly(text):\n",
        "  aug = nac.RandomCharAug(action=\"swap\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "\n",
        "# ---------------------------------------------    \n",
        "# augmentations on word level \n",
        "# ---------------------------------------------    \n",
        "# models - spelling_en.txt\n",
        "# model_dir with fasttext or word2vec or glove \n",
        "# model dir with tf-idf\n",
        "\n",
        "# its consume to much RAM\n",
        "# def insert_word_randomly_by_word_embeddings_similarity(text):\n",
        "#   # model_type: word2vec, glove or fasttext\n",
        "#   aug = naw.WordEmbsAug(\n",
        "#       model_type='word2vec', model_path=os.environ('WORD2VEC_MODEL_PATH'),\n",
        "#       action=\"insert\")\n",
        "#   augmented_text = aug.augment(text)\n",
        "#   print(\"Original:\")\n",
        "#   print(text)\n",
        "#   print(\"Augmented Text:\")\n",
        "#   print(augmented_text)\n",
        "#   return augmented_text\n",
        "\n",
        "\n",
        "def insert_word_by_tf_idf_similarity(text):\n",
        "  aug = naw.TfIdfAug(\n",
        "      model_path=os.environ['NLPAUG_PATH'],\n",
        "      action=\"insert\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "\n",
        "def split_word_to_two_tokens_randomly(text):\n",
        "  aug = naw.SplitAug()\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def swap_word_randomly(text):\n",
        "  aug = naw.RandomWordAug(action=\"swap\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def substitute_word_by_antonym(text):\n",
        "  aug = naw.AntonymAug()\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def substitute_word_by_spelling_mistake_words_dictionary(text):\n",
        "  aug = naw.SpellingAug(os.environ['SPELLING_PATH'])\n",
        "  augmented_text = aug.augment(text, n=1)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def insert_word_by_contextual_word_embeddings(text):\n",
        "  aug = naw.ContextualWordEmbsAug(\n",
        "      model_path='bert-base-uncased', action=\"insert\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "\n",
        "\n",
        "def subtitute_word_by_contextual_word_embeddings(text):\n",
        "  aug = naw.ContextualWordEmbsAug(\n",
        "           model_path='bert-base-uncased', action=\"substitute\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "\n",
        "\n",
        "def substitute_word_by_WordNets_synonym(text):\n",
        "  aug = naw.SynonymAug(aug_src='wordnet')\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "  \n",
        "def fixmatch_weak_augment_pool():\n",
        "    augs = [\n",
        "            substitute_character_by_keyboard_distance,\n",
        "            insert_character_randomly,\n",
        "            substitute_character_randomly,\n",
        "            delete_char_randomly,\n",
        "            swap_character_randomly,\n",
        "            # insert_word_randomly_by_word_embeddings_similarity,\n",
        "            insert_word_by_tf_idf_similarity,\n",
        "            split_word_to_two_tokens_randomly,\n",
        "            swap_word_randomly,\n",
        "            substitute_word_by_antonym,\n",
        "            substitute_word_by_spelling_mistake_words_dictionary,\n",
        "            insert_word_by_contextual_word_embeddings,\n",
        "            subtitute_word_by_contextual_word_embeddings,\n",
        "            substitute_word_by_WordNets_synonym,\n",
        "           ]\n",
        "\n",
        "    return augs\n",
        "\n",
        "# def fixmatch_strong_augment_pool():\n",
        "#     augs = [\n",
        "#             insert_word_by_contextual_word_embeddings,\n",
        "#             subtitute_word_by_contextual_word_embeddings,\n",
        "#             substitute_word_by_WordNets_synonym,\n",
        "#            ]\n",
        "\n",
        "#     return augs\n",
        "\n",
        "\n",
        "class WeakRandAugment(object):\n",
        "  def __init__(self, n, show=False):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    n (int): number of operations\n",
        "\n",
        "    \"\"\"\n",
        "    assert n >= 0\n",
        "    self.n = n\n",
        "    self.augment_pool = fixmatch_weak_augment_pool()\n",
        "    self.show=show\n",
        "\n",
        "  def __call__(self, text):\n",
        "    if self.n <= 0:\n",
        "      return text\n",
        "    ops = random.choices(self.augment_pool, k=self.n)\n",
        "    for op in ops:\n",
        "        if random.random() < 1.:\n",
        "          if self.show:\n",
        "            text = print_augmentations(op, text)\n",
        "          else:\n",
        "            text = op(text)\n",
        "    return text \n",
        "\n",
        "# not necessery\n",
        "# class StrongRandAugment(object):\n",
        "#   def __init__(self, n, show=False):\n",
        "#     assert n >= 1\n",
        "#     self.n = n\n",
        "#     self.augment_pool = fixmatch_strong_augment_pool()\n",
        "#     self.show= show\n",
        "\n",
        "#   def __call__(self, text):\n",
        "#     ops = random.choices(self.augment_pool, k=self.n)\n",
        "#     for op in ops:\n",
        "#       if random.random() < 1.:\n",
        "#         if self.show:\n",
        "#           text = print_augmentations(op, text)\n",
        "#         else:\n",
        "#           text = op(text)\n",
        "#     return text \n",
        "\n",
        "\n",
        "class TransformFix(object):\n",
        "  def __init__(self, n_weak=3, show=False):\n",
        "  # def __init__(self, n_weak=3, n_strong=2, show=False):\n",
        "    self.weak = WeakRandAugment(n=n_weak, show=show) \n",
        "    # self.strong = StrongRandAugment(n=n_strong, show=show)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    weak = self.weak(x)\n",
        "    # strong = self.strong(x)\n",
        "    return weak #, strong\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKpMf7Gz3CpR",
        "colab_type": "text"
      },
      "source": [
        "### Datasets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkdrQH7a3FKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoInOneDataset(Dataset):\n",
        "    def __init__(self, datasets):\n",
        "        self.datasets = datasets\n",
        "\n",
        "        self.map_indexes = [[] for _ in self.datasets]\n",
        "\n",
        "        self.min_length = min(len(d) for d in self.datasets)\n",
        "        self.max_length = max(len(d) for d in self.datasets)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return tuple(d[m[i]] for d, m in zip(self.datasets, self.map_indexes))\n",
        "\n",
        "    def construct_map_index(self):\n",
        "        def update_indices(original_indexes, target_len, max_len):\n",
        "            # map max_len to target_len (large to small)\n",
        "\n",
        "            # return: a list, which maps the range(max_len) to the valid index in the dataset\n",
        "            \n",
        "            original_indexes = original_indexes[max_len:] # remove used indices\n",
        "            fill_num = max_len - len(original_indexes)\n",
        "            batch = fill_num // target_len\n",
        "\n",
        "            if fill_num % target_len != 0:\n",
        "                # to let the fill_num + len(original_indexes) greater than max_len\n",
        "                batch += 1\n",
        "\n",
        "            additional_indexes = list(range(target_len)) * batch\n",
        "            random.shuffle(additional_indexes)\n",
        "\n",
        "            original_indexes += additional_indexes\n",
        "\n",
        "            assert len(original_indexes) >= max_len, \"the length of matcing indexes is too small\"\n",
        "\n",
        "            return original_indexes\n",
        "\n",
        "        self.map_indexes = [update_indices(m, len(d), self.max_length) \n",
        "            for m, d in zip(self.map_indexes, self.datasets)]\n",
        "\n",
        "    def __len__(self):\n",
        "        # will be called every epoch\n",
        "        self.construct_map_index()\n",
        "        return self.max_length\n",
        "        \n",
        "class SimpleTextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, x, y, transform=None):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # special dict convention for f: process_NLUHD \n",
        "        data_dict = { 'text': self.x[index], 'label': self.y[index]}\n",
        "        if self.transform is not None:\n",
        "          return self.transform(data_dict)\n",
        "        return tuple(data_dict.values()) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "class FixMatchAugmentedTextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, x, x_paraphrases, y,\n",
        "                 model_preprocessing = None,\n",
        "                 fix_match_augmentation = None, show=False):\n",
        "        self.x = x\n",
        "        self.x_paraphrases = x_paraphrases\n",
        "        self.y = y\n",
        "        self.model_preprocessing = model_preprocessing\n",
        "        self.fix_match_augmentation = fix_match_augmentation\n",
        "        self.show = show\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # special dict convention for f: process_NLUHD \n",
        "        if self.fix_match_augmentation is not None:\n",
        "          weak_augmented, strong_augmented = \\\n",
        "           self.fix_match_augmentation(self.x[index]), self.x_paraphrases[index] \n",
        "          if self.show:\n",
        "            def back_translation(text):\n",
        "              return strong_augmented \n",
        "            print_augmentations(back_translation, self.x_paraphrases[index])\n",
        "        \n",
        "\n",
        "        weak_aug_data_dict = { 'text': weak_augmented , 'label': self.y[index]}\n",
        "        strong_aug_data_dict = { 'text': strong_augmented , 'label': self.y[index]}\n",
        "\n",
        "        if self.model_preprocessing is not None:\n",
        "          return self.model_preprocessing(weak_aug_data_dict), self.model_preprocessing(strong_aug_data_dict)\n",
        "\n",
        "        return tuple(weak_aug_data_dict.values()), tuple(strong_aug_data_dict.values()) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA1ZSYrtFVK6",
        "colab_type": "text"
      },
      "source": [
        "### Datasets related processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsGvdNvB90p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_NLUHD(comment, nlp,  ner_abstract_tag: bool=True):\n",
        "  expression = r\"\\[.*?\\]\"\n",
        "  matches = []\n",
        "  for match in re.finditer(expression, comment.text):\n",
        "      start, end = match.span()\n",
        "      span = comment.char_span(start, end)\n",
        "      # This is a Span object or None if match doesn't map to valid token sequence\n",
        "      if span is not None:\n",
        "          # print(\"Found match:\", span.text)\n",
        "          if ner_abstract_tag:\n",
        "            expression_scd = r\"\\[.*?\\:\"\n",
        "          else:\n",
        "            expression_scd = r\"\\:.*?\\]\"\n",
        "\n",
        "          temp_doc = nlp(span.text)\n",
        "          scd_match = next(re.finditer(expression_scd, temp_doc.text))\n",
        "          start1, end1 = scd_match.span()\n",
        "          # print(start1, end1)\n",
        "          s1 = int(start1) + 1\n",
        "          e1 = int(end1) - 1\n",
        "          # print(type(e1))\n",
        "          replace_str = temp_doc.text[s1:e1].strip()\n",
        "          # scd_doc = temp_doc.char_span(start1 + 1, end1 - 2) \n",
        "          matches += [((start, end), replace_str)]\n",
        "\n",
        "  start_line = 0\n",
        "  new_comment = \"\"\n",
        "  for match in matches:\n",
        "    s = match[0][0]\n",
        "    e = match[0][1]\n",
        "    replace_word = match[1]\n",
        "    new_comment += comment.text[start_line:s] + replace_word \n",
        "    start_line = e\n",
        "  new_comment += comment.text[start_line:]\n",
        "  ret_val = nlp(new_comment)\n",
        "  return ret_val\n",
        "\n",
        "def preprocess_NLUHD(lowercase,\n",
        "                     remove_stopwords,\n",
        "                     with_ner_tags,\n",
        "                     nlp,\n",
        "                     label_encoder,\n",
        "                     sample):\n",
        "  \n",
        "  stops = stopwords.words(\"english\")\n",
        "  comment = sample['text']\n",
        "  if lowercase:\n",
        "      comment = comment.lower()\n",
        "  comment = nlp(comment)\n",
        "  if with_ner_tags is True:\n",
        "    comment = prepare_NLUHD(comment, ner_abstract_tag=True, nlp=nlp)\n",
        "  else:\n",
        "    comment = prepare_NLUHD(comment, ner_abstract_tag=False, nlp=nlp)\n",
        "  lemmatized = list()\n",
        "  if remove_stopwords:\n",
        "    for word in comment:\n",
        "        lemma = word.lemma_.strip()\n",
        "        if lemma:\n",
        "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
        "                lemmatized.append(lemma)\n",
        "    processed_text = \" \".join(lemmatized) \n",
        "  processed_text = comment.text\n",
        "  encoded_label = torch.tensor(int(label_encoder.transform([sample[\"label\"]])))\n",
        "  return {\"text\": processed_text,\n",
        "          \"label\": encoded_label}\n",
        "\n",
        "def preprocess_IMDB(label_encoder, sample: Dict):\n",
        "  pattern1 = re.compile(r'<.*?>')\n",
        "  # pattern2 = re.compile('[\\W_]+ ')\n",
        "  # text = pattern2.sub(' ', text)\n",
        "  text = pattern1.sub('', sample['text']).lower()\n",
        "  encoded_label = torch.tensor(int(label_encoder.transform([sample[\"label\"]])))\n",
        "  return {\"text\": text,\n",
        "          \"label\": encoded_label}\n",
        "          \n",
        "def preprocess_MR(label_encoder, sample: Dict):\n",
        "  pattern1 = re.compile(r'<.*?>')\n",
        "  # pattern2 = re.compile('[\\W_]+ ')\n",
        "  # text = pattern2.sub(' ', text)\n",
        "  text = pattern1.sub('', sample['text']).lower()\n",
        "  encoded_label = torch.tensor(int(label_encoder.transform([sample[\"label\"]])))\n",
        "  return {\"text\": text,\n",
        "          \"label\": encoded_label}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGCcDLd_-udN",
        "colab_type": "text"
      },
      "source": [
        "### Model related processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvkT6fi64bEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transformer_preprocessing(model_type: str,\n",
        "                              MAX_LEN: int,\n",
        "                              tokenizer: BertTokenizer,\n",
        "                              sample:Dict,) -> Dict:\n",
        "  \n",
        "    inputs = tokenizer.encode_plus(\n",
        "            sample[\"text\"],\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            )\n",
        "    # Output of `tokenizer.encode_plus` is a dictionary.\n",
        "    if model_type == 'roberta-base':\n",
        "      token_type_ids = [] \n",
        "    else:\n",
        "      input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
        "    # For BERT, we need `attention_mask` along with `input_ids` as input.\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "    # We are going to pad sequences.\n",
        "    padding_length = MAX_LEN - len(input_ids)\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "    input_ids = input_ids + ([pad_id] * padding_length)\n",
        "    attention_mask = attention_mask + ([0] * padding_length)\n",
        "    token_type_ids = token_type_ids + ([pad_id] * padding_length)\n",
        "\n",
        "    assert len(input_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(input_ids), MAX_LEN)\n",
        "    assert len(attention_mask) == MAX_LEN, \"Error with input length {} vs {}\".format(len(attention_mask), MAX_LEN)\n",
        "    assert len(token_type_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(token_type_ids), MAX_LEN)\n",
        "\n",
        "    # Just a python list to `torch.tensor`\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "    token_type_ids = torch.tensor(token_type_ids)\n",
        "\n",
        "    # What we return will one instance in batch which `LightningModule.train_step` receives.\n",
        "    return {\n",
        "            \"label\": sample['label'],\n",
        "            \"embedding\": {\n",
        "                          \"input_ids\": input_ids,\n",
        "                          \"attention_mask\": attention_mask,\n",
        "                          \"token_type_ids\": token_type_ids\n",
        "                         }\n",
        "            }\n",
        "\n",
        "def generate_embeddings(\n",
        "                         hparams,\n",
        "                         tokenizer,\n",
        "                         embeder,\n",
        "                         sample):\n",
        "\n",
        "  embedding = torch.Tensor([token.vector for token in embeder(sample[\"text\"])])#torch.tensor(embeder.wv[tokens])\n",
        "\n",
        "  if embedding.size()[0] >= hparams['max_sentence_len']:\n",
        "    embedding = torch.narrow(embedding, 0, 0, hparams['max_sentence_len'])\n",
        "  else:\n",
        "    padding_length = hparams['max_sentence_len'] - len(embedding)\n",
        "    padding_vectors = torch.zeros((padding_length, hparams['embed_dim']))\n",
        "    embedding = torch.cat((embedding, padding_vectors)) \n",
        "\n",
        "  return {'label': sample['label'],\n",
        "          'embedding': embedding}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_ppWCPTVnLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# datasets_path = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'datasets')\n",
        "# NLU_HD_path = os.path.join(datasets_path,'NLU-Data-Home-Domain-Annotated-All.csv')\n",
        "# print(NLU_HD_path)\n",
        "# # df = pd.read_csv(str(NLU_HD_path), delimiter=';')[['intent', 'answer_annotation']]\n",
        "# df = pd.read_csv(str(NLU_HD_path), delimiter=';')[['intent', 'answer_annotation', 'scenario']]\n",
        "# df['intent'] = df[['scenario', 'intent']].agg('-'.join, axis=1) \n",
        "# del df['scenario']\n",
        "# df = df[df['answer_annotation'].notna()]\n",
        "# df = df.rename(columns={\"answer_annotation\": \"text\"})\n",
        "# nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "# df['text'] = df['text'].apply(normalize,\n",
        "#                               lowercase=True,\n",
        "#                               remove_stopwords=False,\n",
        "#                               with_ner_tags=False,\n",
        "#                               nlp=nlp) \n",
        "# # df.to_csv(os.path.join(datasets_path,'NLU-Data-Home-Domain-preprocessed-without-ner_no-scenario.csv'))\n",
        "# df.to_csv(os.path.join(datasets_path,'NLU-Data-Home-Domain-preprocessed-without-ner.csv'))\n",
        "# df['intent'].value_counts().plot(kind=\"bar\", figsize= (21,20))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPAoAdd3Fm2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pd.set_option('display.max_rows', None)\n",
        "# df['intent'].value_counts()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr0K3ed_C6hV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df.head(10)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk1pzfVafNFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(df['intent'].unique())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7_n1xgQqHAq",
        "colab_type": "text"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH4XjCAXlRDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_global_metric(outputs, metric):\n",
        "    return sum([out[metric] for out in outputs]) / len(outputs)\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def tfidf_tokenizer(text, token_pattern=r\"(?u)\\b\\w\\w+\\b\"):\n",
        "    token_pattern = re.compile(token_pattern)\n",
        "    return token_pattern.findall(text)\n",
        "\n",
        "def create_tfidf_model(df: pd.DataFrame):  \n",
        "  train_data = df['text'] \n",
        "  train_x = train_data.values\n",
        "    \n",
        "  train_x_tokens = [tfidf_tokenizer(x) for x in train_x]\n",
        "    \n",
        "  tfidf_model = nmw.TfIdf()\n",
        "  tfidf_model.train(train_x_tokens)\n",
        "  tfidf_model.save(os.environ['NLPAUG_PATH'])\n",
        "  os.environ['TFIDF_MODEL_PATH']  = os.path.join(os.environ['NLPAUG_PATH'], 'tfidfaug_w2tfidf.txt')\n",
        "  os.listdir(os.environ['NLPAUG_PATH'])\n",
        "\n",
        "class MetricsCallback(Callback):\n",
        "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.metrics = []\n",
        "\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        self.metrics.append(trainer.callback_metrics)\n",
        "\n",
        "# credits: https://github.com/galatolofederico/pytorch-balanced-batch/blob/master/sampler.py        \n",
        "class BalancedBatchSampler(torch.utils.data.sampler.Sampler):\n",
        "    def __init__(self, dataset, labels=None):\n",
        "        self.labels = labels\n",
        "        self.dataset = dict()\n",
        "        self.balanced_max = 0\n",
        "        # Save all the indices for all the classes\n",
        "        for idx in range(0, len(dataset)):\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label not in self.dataset:\n",
        "                self.dataset[label] = list()\n",
        "            self.dataset[label].append(idx)\n",
        "            self.balanced_max = len(self.dataset[label]) \\\n",
        "                if len(self.dataset[label]) > self.balanced_max else self.balanced_max\n",
        "        \n",
        "        # Oversample the classes with fewer elements than the max\n",
        "        for label in self.dataset:\n",
        "            while len(self.dataset[label]) < self.balanced_max:\n",
        "                self.dataset[label].append(random.choice(self.dataset[label]))\n",
        "        self.keys = list(self.dataset.keys())\n",
        "        self.currentkey = 0\n",
        "        self.indices = [-1]*len(self.keys)\n",
        "\n",
        "    def __iter__(self):\n",
        "        while self.indices[self.currentkey] < self.balanced_max - 1:\n",
        "            self.indices[self.currentkey] += 1\n",
        "            yield self.dataset[self.keys[self.currentkey]][self.indices[self.currentkey]]\n",
        "            self.currentkey = (self.currentkey + 1) % len(self.keys)\n",
        "        self.indices = [-1]*len(self.keys)\n",
        "    \n",
        "    def _get_label(self, dataset, idx, labels = None):\n",
        "        if self.labels is not None:\n",
        "            return self.labels[idx].item()\n",
        "        else:\n",
        "            raise Exception(\"You should pass the tensor of labels to the constructor as second argument\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.balanced_max*len(self.keys)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjha3vit9omj",
        "colab_type": "text"
      },
      "source": [
        "### Tests "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DWq_Q17ENpc",
        "colab_type": "text"
      },
      "source": [
        "#### Check if lf.cv.split splits in stratified fashion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e-bcGspDUnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ds = lf.CsvDataset(os.path.join(os.environ['DATASETS_PATH'], 'NLU-Data-Home-Domain-preprocessed-without-ner.csv'), header=True)\n",
        "# ds = ds.filter(lambda x: x['text'] is not None)\n",
        "\n",
        "# train, test = lf.cross_validation.split_dataset_random(ds, int(len(ds) * 0.8), seed=42)\n",
        "# df_train = pd.DataFrame(train)\n",
        "# df_test = pd.DataFrame(test)\n",
        "# # df_train['intent'].value_counts().plot(kind=\"bar\", figsize= (21,20))\n",
        "# df_test['intent'].value_counts().plot(kind=\"bar\", figsize= (21,20))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vhpT-QpSOrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pd.set_option('display.max_rows', None)\n",
        "# df_train.info()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfAShMKL9oH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
        "# uter = '[greetings : Hello] there what is [swear_word: fuck] up ?'\n",
        "# # uter =  '[greetings : Hello] there what is fuck up ?'\n",
        "# # uter = \"hello adfafdasfsaf sfsafdsafsa\"\n",
        "# comment = nlp(uter)\n",
        "# processed_comment = prepare_NLUHD(comment, nlp=nlp,ner_abstract_tag=False)\n",
        "# processed_comment"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isgALgNl86Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train = lfds.MsrParaphrase(\"train\")\n",
        "# print(len(train))\n",
        "# test = lfds.MsrParaphrase(\"test\")\n",
        "# train.first()\n",
        "# def nonefilter(dataset):\n",
        "#   filtered = []\n",
        "#   for x in dataset:\n",
        "#       if x[\"string1\"] is None:\n",
        "#           continue\n",
        "#       if x[\"string2\"] is None:\n",
        "#           continue\n",
        "#       filtered.append(x)\n",
        "#   return lf.Dataset(filtered)\n",
        "# # train = nonefilter(train)\n",
        "# train = train.filter(lambda x: x[\"string1\"] is not None and x[\"string2\"] is not None)\n",
        "# print(len(train))\n",
        "# train.take(3)\n",
        "# unique = list(['ale', 'beka'])\n",
        "# le = preprocessing.LabelEncoder().fit(unique)\n",
        "# torch.tensor(int(le.transform(['ale'])))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9cu0GG3qjHz",
        "colab_type": "text"
      },
      "source": [
        "#### Check IMDB preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOH9NEJHhTyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hparams = {'max_sentence_len': 200,\n",
        "#            'embed_dim': 300,\n",
        "#            'seed': 42,\n",
        "#            'train_test_split': 0.8}\n",
        "\n",
        "# train = lfds.Imdb('train')\n",
        "# test = lfds.Imdb('test')\n",
        "# ds = train + test\n",
        "# ds = ds.map(lambda x: {'text':x[0] , 'label': x[1]})\n",
        "# embeder = nlp \n",
        "# tokenizer_fun = lambda x: x#gensim_tokenizer\n",
        "# unique_labels = list(pd.DataFrame(ds).label.unique())\n",
        "# le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "\n",
        "# preprocessor = partial(\n",
        "#                       preprocess_IMDB,\n",
        "#                       hparams,\n",
        "#                       tokenizer_fun, \n",
        "#                       embeder,\n",
        "#                       le,\n",
        "#                       )\n",
        "# s = ds.first()\n",
        "# print(s)\n",
        "# preprocessor(s)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YDThFsqiBYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# train = lfds.Imdb('train')\n",
        "# test = lfds.Imdb('test')\n",
        "# ds = train + test\n",
        "# ds = ds.map(lambda x: {'text':x[0] , 'label': x[1]})\n",
        "# df = pd.DataFrame(ds)\n",
        "# pattern1 = re.compile(r'<.*?>')\n",
        "# # pattern2 = re.compile('[\\W_]+')\n",
        "# # text = pattern1.sub('', sample['text'])\n",
        "# # print('text after p1: ', text)\n",
        "# # text = text.replace('_', '').lower()\n",
        "# func = partial(pattern1.sub,\n",
        "#                '')\n",
        "# df['text'] = df['text'].apply(func)\n",
        "# # df\n",
        "# texts = list(df['text'])\n",
        "# texts"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ac1FViOh833",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ft.build_vocab(texts, update=True)\n",
        "# ft.train(new_sentences, total_examples=len(texts), epochs=10)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHn7X8Oc2IJN",
        "colab_type": "text"
      },
      "source": [
        "#### Test FixMatchTransform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIOUS2Fr2NLF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0856289f-08a2-448d-bc90-464f5075754a"
      },
      "source": [
        "tf = TransformFix(1,show=True)\n",
        "s = tf(\"what will be the weather like tomorrow, please tell me\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Augmentation function:  substitute_word_by_WordNets_synonym\n",
            "Original: \n",
            "what will be the weather like tomorrow, please tell me\n",
            "Augmention result: \n",
            "what volition embody the weather like tomorrow , please severalise me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0Yw_j3YaDTg",
        "colab_type": "text"
      },
      "source": [
        "#### Test balanced sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-29zVc12aDe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not_none = lambda x: x[\"text\"] is not None \n",
        "# # ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "# ds = lfds.Imdb('train') + lfds.Imdb('test')\n",
        "# ds = ds.map(lambda x: {'text': x[0], 'label': x[1]})\n",
        "# df = pd.DataFrame(ds)\n",
        "\n",
        "# create_tfidf_model(df)\n",
        "\n",
        "# print(df.info(memory_usage=True))\n",
        "# unique_labels = list(df.label.unique())\n",
        "# print(f'unique_labels: {unique_labels}')\n",
        "# print(f'number_of_categories : {len(unique_labels)}')\n",
        "# le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "# train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "#                                                         int(len(ds) * 0.9),\n",
        "#                                                         seed=42)\n",
        "# dataset_preprocessor = partial(\n",
        "#                                 preprocess_IMDB,\n",
        "#                                 le,\n",
        "#                               )\n",
        "\n",
        "# tokenizer_dict = {\n",
        "#         \"bert-base-uncased\":\n",
        "#           BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
        "#                                         do_lower_case=True),\n",
        "#         \"roberta-base\":\n",
        "#           RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "#         }\n",
        "\n",
        "# model_arch_preprocessor = partial(\n",
        "#                                   transformer_preprocessing,\n",
        "#                                   'bert-base-uncased',\n",
        "#                                   156,\n",
        "#                                   tokenizer_dict['bert-base-uncased'],\n",
        "#                                   )\n",
        "\n",
        "\n",
        "# preprocessor = toolz.compose(\n",
        "#                             model_arch_preprocessor,\n",
        "#                             dataset_preprocessor,\n",
        "#                             )\n",
        "\n",
        "# train_df, test_df = pd.DataFrame(train), pd.DataFrame(test)\n",
        "# x_train, y_train = train_df['text'].values, train_df['label'].values\n",
        "# x_test, y_test = test_df['text'].values, test_df['label'].values\n",
        "\n",
        "# # split's training parameters  \n",
        "# num_classes = len(unique_labels)\n",
        "# label_per_class = 1000 // num_classes\n",
        "# valid_size = 1000 \n",
        "\n",
        "# labeled_idx = []\n",
        "# unlabeled_idx = []\n",
        "# val_idx = []\n",
        "\n",
        "# for label in unique_labels:\n",
        "#   idx = np.where(y_train == label)[0]\n",
        "#   np.random.shuffle(idx)\n",
        "#   labeled_idx.extend(idx[:label_per_class])\n",
        "#   val_idx.extend(idx[label_per_class: label_per_class + valid_size])\n",
        "#   unlabeled_idx.extend(idx[label_per_class + valid_size:])\n",
        "\n",
        "# x_labeled, y_labeled  = x_train[labeled_idx], y_train[labeled_idx]\n",
        "# x_unlabeled, y_unlabeled = x_train[unlabeled_idx], y_train[unlabeled_idx]\n",
        "# x_val, y_val = x_train[val_idx], y_train[val_idx]\n",
        "\n",
        "\n",
        "# train_labeled_dataset = SimpleTextDataset(x_labeled,\n",
        "#                                               y_labeled,\n",
        "#                                               transform=preprocessor)\n",
        "# train_unlabeled_dataset = SimpleTextDataset(x_unlabeled,\n",
        "#                                                   y_unlabeled,\n",
        "#                                                   transform=preprocessor)\n",
        "\n",
        "# val_dataset = SimpleTextDataset(x_val,\n",
        "#                                     y_val,\n",
        "#                                     transform=preprocessor)\n",
        "\n",
        "# test_dataset = SimpleTextDataset(x_test,\n",
        "#                                       y_test,\n",
        "#                                       transform=preprocessor)\n",
        "\n",
        "\n",
        "\n",
        "# train_labeled_dataloader = torch.utils.data.DataLoader(\n",
        "#                       train_labeled_dataset,\n",
        "#                       batch_size=64,\n",
        "#                       # shuffle=True,\n",
        "#                       num_workers=0,\n",
        "#                       sampler=BalancedBatchSampler(train_labeled_dataset, y_labeled),\n",
        "#                       )\n",
        "\n",
        "# train_labeled_dataloader_iterator = iter(train_labeled_dataloader)\n",
        "# train_unlabeled_dataloader = DataLoader(\n",
        "#                 train_unlabeled_dataset,\n",
        "#                 batch_size=64,\n",
        "#                 num_workers=8,\n",
        "#                 shuffle=True # without shuffle it want work cause it need to create map index before __get_item__ function\n",
        "#                 )\n",
        "\n",
        "# train_labeled_dataloader_iterator = iter(train_labeled_dataloader)\n",
        "# b = next(train_labeled_dataloader_iterator)\n",
        "# torch.sum(b['label'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMARxMrnTMzq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5df078b6-6870-4243-ca3a-fa9bcb6140b7"
      },
      "source": [
        "epochs = 3\n",
        "size = 20\n",
        "features = 5\n",
        "classes_prob = torch.tensor([0.1, 0.4, 0.5])\n",
        "\n",
        "dataset_X = torch.randn(size, features)\n",
        "dataset_Y = torch.distributions.categorical.Categorical(classes_prob.repeat(size, 1)).sample()\n",
        "print(dataset_Y)\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(dataset_X, dataset_Y)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, sampler=BalancedBatchSampler(dataset, dataset_Y), batch_size=6)\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        print(\"epoch: %d labels: %s\\ninputs: %s\\n\" % (epoch, batch_y, batch_x))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1])\n",
            "epoch: 0 labels: tensor([2, 1, 2, 1, 2, 1])\n",
            "inputs: tensor([[-1.3425, -1.1678,  0.4695,  0.0196, -0.2692],\n",
            "        [-0.0417, -0.7213,  0.6492,  1.2039, -0.2315],\n",
            "        [-2.6826, -0.1335,  0.9670,  0.8894, -0.3225],\n",
            "        [ 0.9057, -0.0724,  0.4375, -0.4172, -0.9839],\n",
            "        [ 1.1450,  0.9629,  0.7119,  0.1725, -0.3991],\n",
            "        [ 0.6788,  0.7069,  0.0963,  0.3965, -1.7663]])\n",
            "\n",
            "epoch: 0 labels: tensor([2, 1, 2, 1, 2, 1])\n",
            "inputs: tensor([[ 0.8420,  1.2591,  2.1289, -0.5433, -0.1115],\n",
            "        [ 0.0876, -1.4769,  0.7769,  0.1552,  0.3016],\n",
            "        [ 0.3609,  1.0560,  1.4031, -0.3382,  1.3304],\n",
            "        [-0.6375, -0.5138, -0.8491, -0.5406,  1.3031],\n",
            "        [-2.1542,  1.2397,  0.3655, -0.2560, -0.5063],\n",
            "        [ 0.9213,  0.7111,  0.5607, -0.6517, -1.2219]])\n",
            "\n",
            "epoch: 0 labels: tensor([2, 1, 2, 1, 2, 1])\n",
            "inputs: tensor([[-0.8478, -0.3303,  0.1322,  0.0686, -2.3124],\n",
            "        [-0.9840, -0.6970, -0.3769, -0.4791, -0.3102],\n",
            "        [ 0.0138,  0.3575, -1.0219,  0.2367, -0.5325],\n",
            "        [-0.6446, -0.4806,  1.2690, -0.4634,  0.3466],\n",
            "        [ 0.9832, -2.0310, -0.2349, -0.0319, -1.9407],\n",
            "        [ 1.6032,  0.2124, -1.6760,  0.2292, -0.2747]])\n",
            "\n",
            "epoch: 0 labels: tensor([2, 1])\n",
            "inputs: tensor([[ 1.0210,  1.6278, -0.7793,  0.7563,  0.4828],\n",
            "        [-0.4026,  2.2607, -1.3106, -0.6569, -0.8748]])\n",
            "\n",
            "epoch: 1 labels: tensor([2, 1, 2, 1, 2, 1])\n",
            "inputs: tensor([[-1.3425, -1.1678,  0.4695,  0.0196, -0.2692],\n",
            "        [-0.0417, -0.7213,  0.6492,  1.2039, -0.2315],\n",
            "        [-2.6826, -0.1335,  0.9670,  0.8894, -0.3225],\n",
            "        [ 0.9057, -0.0724,  0.4375, -0.4172, -0.9839],\n",
            "        [ 1.1450,  0.9629,  0.7119,  0.1725, -0.3991],\n",
            "        [ 0.6788,  0.7069,  0.0963,  0.3965, -1.7663]])\n",
            "\n",
            "epoch: 1 labels: tensor([2, 1, 2, 1, 2, 1])\n",
            "inputs: tensor([[ 0.8420,  1.2591,  2.1289, -0.5433, -0.1115],\n",
            "        [ 0.0876, -1.4769,  0.7769,  0.1552,  0.3016],\n",
            "        [ 0.3609,  1.0560,  1.4031, -0.3382,  1.3304],\n",
            "        [-0.6375, -0.5138, -0.8491, -0.5406,  1.3031],\n",
            "        [-2.1542,  1.2397,  0.3655, -0.2560, -0.5063],\n",
            "        [ 0.9213,  0.7111,  0.5607, -0.6517, -1.2219]])\n",
            "\n",
            "epoch: 1 labels: tensor([2, 1, 2, 1, 2, 1])\n",
            "inputs: tensor([[-0.8478, -0.3303,  0.1322,  0.0686, -2.3124],\n",
            "        [-0.9840, -0.6970, -0.3769, -0.4791, -0.3102],\n",
            "        [ 0.0138,  0.3575, -1.0219,  0.2367, -0.5325],\n",
            "        [-0.6446, -0.4806,  1.2690, -0.4634,  0.3466],\n",
            "        [ 0.9832, -2.0310, -0.2349, -0.0319, -1.9407],\n",
            "        [ 1.6032,  0.2124, -1.6760,  0.2292, -0.2747]])\n",
            "\n",
            "epoch: 1 labels: tensor([2, 1])\n",
            "inputs: tensor([[ 1.0210,  1.6278, -0.7793,  0.7563,  0.4828],\n",
            "        [-0.4026,  2.2607, -1.3106, -0.6569, -0.8748]])\n",
            "\n",
            "epoch: 2 labels: tensor([2, 1, 2, 1, 2, 1])\n",
            "inputs: tensor([[-1.3425, -1.1678,  0.4695,  0.0196, -0.2692],\n",
            "        [-0.0417, -0.7213,  0.6492,  1.2039, -0.2315],\n",
            "        [-2.6826, -0.1335,  0.9670,  0.8894, -0.3225],\n",
            "        [ 0.9057, -0.0724,  0.4375, -0.4172, -0.9839],\n",
            "        [ 1.1450,  0.9629,  0.7119,  0.1725, -0.3991],\n",
            "        [ 0.6788,  0.7069,  0.0963,  0.3965, -1.7663]])\n",
            "\n",
            "epoch: 2 labels: tensor([2, 1, 2, 1, 2, 1])\n",
            "inputs: tensor([[ 0.8420,  1.2591,  2.1289, -0.5433, -0.1115],\n",
            "        [ 0.0876, -1.4769,  0.7769,  0.1552,  0.3016],\n",
            "        [ 0.3609,  1.0560,  1.4031, -0.3382,  1.3304],\n",
            "        [-0.6375, -0.5138, -0.8491, -0.5406,  1.3031],\n",
            "        [-2.1542,  1.2397,  0.3655, -0.2560, -0.5063],\n",
            "        [ 0.9213,  0.7111,  0.5607, -0.6517, -1.2219]])\n",
            "\n",
            "epoch: 2 labels: tensor([2, 1, 2, 1, 2, 1])\n",
            "inputs: tensor([[-0.8478, -0.3303,  0.1322,  0.0686, -2.3124],\n",
            "        [-0.9840, -0.6970, -0.3769, -0.4791, -0.3102],\n",
            "        [ 0.0138,  0.3575, -1.0219,  0.2367, -0.5325],\n",
            "        [-0.6446, -0.4806,  1.2690, -0.4634,  0.3466],\n",
            "        [ 0.9832, -2.0310, -0.2349, -0.0319, -1.9407],\n",
            "        [ 1.6032,  0.2124, -1.6760,  0.2292, -0.2747]])\n",
            "\n",
            "epoch: 2 labels: tensor([2, 1])\n",
            "inputs: tensor([[ 1.0210,  1.6278, -0.7793,  0.7563,  0.4828],\n",
            "        [-0.4026,  2.2607, -1.3106, -0.6569, -0.8748]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVxlMTAX9hUH",
        "colab_type": "text"
      },
      "source": [
        "#### Test embeder if exist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfDzoDan9gkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokens = nlp('213213dsf ma kota')\n",
        "# tokenlist = [token.vector for token in tokens]\n",
        "# t = torch.Tensor(tokenlist)\n",
        "# t"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShfV1EwDrX97",
        "colab_type": "text"
      },
      "source": [
        "### Composable Framework "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9CPFT7ormCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LitComposableFramework(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, hparams):\n",
        "\n",
        "    super().__init__()\n",
        "    self.hparams = hparams\n",
        "    self.num_classes = hparams['num_classes']\n",
        "    self.total_iterations = 0 \n",
        "    self.loss_fct = getattr(nn, hparams['loss_function'])()\n",
        "\n",
        "\n",
        "    if self.hparams['model_arch'] == \"Convolution\":\n",
        "\n",
        "      self.embeder_dict = {\n",
        "                          'fastText': (create_ft_embeder, gensim_tokenizer),\n",
        "                          'spaCy':(create_spacy_nlp_embeder, lambda x: x)\n",
        "                          }\n",
        "      embeder, self.tokenizer_fun = self.embeder_dict[hparams['embeder_type']]\n",
        "      self.embeder = nlp #embeder#nlp # hardcoded\n",
        "      self.D = hparams['embed_dim']\n",
        "      self.Ci = hparams['Ci'] \n",
        "      self.Co = hparams['kernel_num']\n",
        "      self.Ks = list(map(int, hparams['kernel_sizes'].split(','))) # (3,4,5)\n",
        "      self.convs1 = nn.ModuleList([nn.Conv2d(self.Ci, self.Co, (K, self.D)) for K in self.Ks])\n",
        "      self.dropout = nn.Dropout(hparams['dropout'])\n",
        "      self.fc1 = nn.Linear(len(self.Ks) * self.Co, self.num_classes) \n",
        "\n",
        "\n",
        "    elif self.hparams['model_arch'] == \"Transformer\":\n",
        "      self.model_class_dict = {\n",
        "            \"bert-base-uncased\": BertModel,\n",
        "            \"roberta-base\": RobertaModel\n",
        "            }\n",
        "              \n",
        "      self.tokenizer_dict = {\n",
        "              \"bert-base-uncased\":\n",
        "                BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
        "                                              do_lower_case=True),\n",
        "              \"roberta-base\":\n",
        "                RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "              }\n",
        "  \n",
        "      self.model = self.model_class_dict[self.hparams['model_type']].from_pretrained(self.hparams['model_type'],\n",
        "                                                                output_attentions=True)\n",
        "      self.encoder_features =  self.model.config.hidden_size \n",
        "      print(self.encoder_features)\n",
        "      self.num_classes = self.hparams['num_classes']\n",
        "      self.classification_head = nn.Sequential(\n",
        "            nn.Linear(self.encoder_features, self.encoder_features * 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.encoder_features * 2, self.encoder_features),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.encoder_features, self.num_classes),\n",
        "        )\n",
        "      \n",
        "    else:\n",
        "      raise ValueError('Wrong model architecture type: {} \\n Possible datasets: Transformer, Convolution'.format(self.hparams['model_arch']))\n",
        "\n",
        "  def forward(self, x, embeddings_only=False):\n",
        "\n",
        "      if self.hparams['model_arch'] == \"Convolution\":\n",
        "\n",
        "        if embeddings_only == True:\n",
        "          logits = x\n",
        "        else:\n",
        "          x = x.unsqueeze(self.Ci)  # (N, Ci, W, D)\n",
        "          x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
        "          x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "          x = torch.cat(x, 1)\n",
        "          x = self.dropout(x)  # (N, len(Ks)*Co)\n",
        "          logits = self.fc1(x)  # (N, C)\n",
        "\n",
        "      elif self.hparams['model_arch'] == \"Transformer\":\n",
        "        # print('input_x: ', x['input_ids'].size())\n",
        "        h, _, _ = self.model(x['input_ids'],\n",
        "                                attention_mask=x['attention_mask'],\n",
        "                                token_type_ids=x['token_type_ids'] if self.hparams['model_type'] != \"roberta-base\" else None)\n",
        "        h_cls = h[:, 0]\n",
        "\n",
        "        if embeddings_only == True:\n",
        "          return h_cls\n",
        "        # print('h_cls', h_cls.size())\n",
        "        logits = self.classification_head(h_cls)\n",
        "      else:\n",
        "        raise ValueError('Wrong model architecture type: {} \\n Possible datasets: Transformer, Convolution'.format(self.hparams['model_arch']))\n",
        "\n",
        "      return logits\n",
        "\n",
        "\n",
        "  def prepare_data(self):\n",
        "\n",
        "    if self.hparams['dataset'] == 'NLUHD':\n",
        "\n",
        "      not_none = lambda x: x[\"text\"] is not None \n",
        "      ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "      unique_labels = list(pd.DataFrame(ds).intent.unique())\n",
        "      self.le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      print(f\"Unique labels: {unique_labels}\")\n",
        "      print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                            int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                            seed=self.hparams['seed'])\n",
        "\n",
        "      nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "      dataset_preprocessor = partial(preprocess_NLUHD,\n",
        "                                     lowercase=True,\n",
        "                                     remove_stopwords=True,\n",
        "                                     with_ner_tags=False,\n",
        "                                     nlp=nlp,\n",
        "                                     label_encoder=le,\n",
        "      )\n",
        "\n",
        "    elif self.hparams['dataset'] == 'MR':\n",
        "\n",
        "      not_none = lambda x: x[\"text\"] is not None \n",
        "      ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "      df = pd.DataFrame(ds)\n",
        "\n",
        "      if self.hparams['training_method'] == \"FixMatch\":\n",
        "        create_tfidf_model(df)\n",
        "\n",
        "      unique_labels = list(df.intent.unique())\n",
        "      self.le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      print(f\"Unique labels: {unique_labels}\")\n",
        "      print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                            int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                            seed=self.hparams['seed'])\n",
        "\n",
        "      dataset_preprocessor = partial(\n",
        "                                     preprocess_MR,\n",
        "                                     le,\n",
        "                                    )\n",
        "      \n",
        "    elif self.hparams['dataset'] == 'IMDB':\n",
        "      not_none = lambda x: x[\"text\"] is not None \n",
        "      ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "      # ds = lfds.Imdb('train') + lfds.Imdb('test')\n",
        "      # ds = ds.map(lambda x: {'text': x[0], 'label': x[1]})\n",
        "      df = pd.DataFrame(ds)\n",
        "\n",
        "      if self.hparams['training_method'] == \"FixMatch\":\n",
        "        create_tfidf_model(df)\n",
        "\n",
        "      print(df.info(memory_usage=True))\n",
        "      unique_labels = list(df.label.unique())\n",
        "      print(f'unique_labels: {unique_labels}')\n",
        "      print(f'number_of_categories : {len(unique_labels)}')\n",
        "      self.le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                             int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                             seed=self.hparams['seed'])\n",
        "      dataset_preprocessor = partial(\n",
        "                                     preprocess_IMDB,\n",
        "                                     self.le,\n",
        "                                    )\n",
        "                            \n",
        "    else:\n",
        "      raise ValueError('Wrong dataset name : {} \\n Possible datasets: IMDB, NLUHD'.format(self.hparams['dataset']))\n",
        "\n",
        "    if self.hparams['model_arch'] == 'Transformer':\n",
        "\n",
        "      model_arch_preprocessor = partial(\n",
        "                                        transformer_preprocessing,\n",
        "                                        self.hparams['model_type'],\n",
        "                                        self.hparams['max_sentence_len'],\n",
        "                                        self.tokenizer_dict[self.hparams['model_type']],\n",
        "                                       )\n",
        "      \n",
        "    elif self.hparams['model_arch'] == 'Convolution':\n",
        "\n",
        "      model_arch_preprocessor = partial(\n",
        "                                        generate_embeddings,\n",
        "                                        self.hparams,\n",
        "                                        self.tokenizer_fun,\n",
        "                                        self.embeder,\n",
        "                                       )\n",
        "    \n",
        "    else:\n",
        "      raise ValueError('Wrong model architecture type: {} \\n Possible architectures: Convolution, Transformer'.format(self.hparams['model_arch']))\n",
        "      \n",
        "    preprocessor = toolz.compose(\n",
        "                                 model_arch_preprocessor,\n",
        "                                 dataset_preprocessor,\n",
        "                                 )\n",
        "\n",
        "    train_df, test_df = pd.DataFrame(train), pd.DataFrame(test)\n",
        "    x_train, y_train = train_df['text'].values, train_df['label'].values\n",
        "    self.x_test, self.y_test = test_df['text'].values, test_df['label'].values\n",
        "\n",
        "    # split's training parameters  \n",
        "    num_classes = len(unique_labels)\n",
        "    label_per_class = self.hparams['n_labeled'] // num_classes\n",
        "    valid_size = self.hparams['valid_size_per_class']\n",
        "\n",
        "    labeled_idx = []\n",
        "    unlabeled_idx = []\n",
        "    val_idx = []\n",
        "    \n",
        "    for label in unique_labels:\n",
        "        idx = np.where(y_train == label)[0]\n",
        "        np.random.shuffle(idx)\n",
        "        labeled_idx.extend(idx[:label_per_class])\n",
        "        val_idx.extend(idx[label_per_class: label_per_class + valid_size])\n",
        "        unlabeled_idx.extend(idx[label_per_class + valid_size:])\n",
        "\n",
        "    self.x_labeled, self.y_labeled  = x_train[labeled_idx], y_train[labeled_idx]\n",
        "    self.x_unlabeled, self.y_unlabeled = x_train[unlabeled_idx], y_train[unlabeled_idx]\n",
        "    self.x_val, self.y_val = x_train[val_idx], y_train[val_idx]\n",
        "\n",
        "    \n",
        "    self._train_labeled_dataset = SimpleTextDataset(self.x_labeled,\n",
        "                                                    self.y_labeled,\n",
        "                                                    transform=preprocessor)\n",
        "    if self.hparams['training_method'] == 'FixMatch':\n",
        "      x_unlabeled_paraphrases = train_df['paraphrases'].values\n",
        "      self._train_unlabeled_dataset = \\\n",
        "         FixMatchAugmentedTextDataset(self.x_unlabeled,\n",
        "                                      x_unlabeled_paraphrases[unlabeled_idx],\n",
        "                                      self.y_unlabeled,\n",
        "                                      model_preprocessing=preprocessor,\n",
        "                                      show=self.hparams['show_augmentation'],\n",
        "                                      fix_match_augmentation=TransformFix(\n",
        "                                        n_weak=self.hparams['n_weak'],\n",
        "                                        # n_strong=self.hparams['n_strong'],\n",
        "                                        show=self.hparams['show_augmentation']),\n",
        "                                     )\n",
        "    else:\n",
        "      self._train_unlabeled_dataset = SimpleTextDataset(self.x_unlabeled,\n",
        "                                                        self.y_unlabeled,\n",
        "                                                        transform=preprocessor)\n",
        "    \n",
        "    self._val_dataset = SimpleTextDataset(self.x_val,\n",
        "                                          self.y_val,\n",
        "                                          transform=preprocessor)\n",
        "    \n",
        "    self._test_dataset = SimpleTextDataset(self.x_test,\n",
        "                                           self.y_test,\n",
        "                                           transform=preprocessor)\n",
        "    \n",
        "    self.total_iterations = len(self._train_unlabeled_dataset) // self.hparams['unl_batch_size'] \n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "\n",
        "    encoded_label = torch.tensor(self.le.transform(self.y_labeled))\n",
        "    self._train_labeled_dataloader = torch.utils.data.DataLoader(\n",
        "                            self._train_labeled_dataset,\n",
        "                            batch_size=self.hparams['l_batch_size'],\n",
        "                            # shuffle=True,\n",
        "                            num_workers=0,\n",
        "                            sampler=BalancedBatchSampler(self._train_labeled_dataset,\n",
        "                                                         encoded_label),\n",
        "                            )\n",
        "      \n",
        "    self.train_labeled_dataloader_iterator = iter(self._train_labeled_dataloader)\n",
        "    return DataLoader(\n",
        "                      self._train_unlabeled_dataset,\n",
        "                      batch_size=self.hparams['unl_batch_size'],\n",
        "                      num_workers=8,\n",
        "                      shuffle=True # without shuffle it want work cause it need to create map index before __get_item__ function\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "                      self._val_dataset,\n",
        "                      batch_size=self.hparams['val_batch_size'],\n",
        "                      num_workers=8\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(\n",
        "                      self._test_dataset,\n",
        "                      batch_size=self.hparams['test_batch_size'],\n",
        "                      num_workers=8\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "\n",
        "    if self.hparams['model_arch'] == 'Transformer':\n",
        "      param_optimizer = list(self.model.named_parameters())\n",
        "      no_decay = [\"bias\", 'LayerNorm.weight']\n",
        "      optimizer_grouped_parameters = [\n",
        "              {\n",
        "                  \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                  \"weight_decay_rate\": 0.01\n",
        "                  },\n",
        "              {\n",
        "                  \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                  \"weight_decay_rate\": 0.0\n",
        "                  },\n",
        "              ]\n",
        "      print('total_iterations: ', self.total_iterations)\n",
        "      optimizer = AdamW(\n",
        "                        optimizer_grouped_parameters,\n",
        "                        lr=self.hparams['lr'],\n",
        "                      )\n",
        "      scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                  self.hparams['warmup_steps'],\n",
        "                                                  self.total_iterations,\n",
        "                                                  -1)\n",
        "\n",
        "    elif self.hparams['model_arch'] == 'Convolution':\n",
        "      optimizer_dict = {'Adam': torch.optim.Adam(self.parameters(),\n",
        "                                                lr=self.hparams['lr']),\n",
        "                      }\n",
        "  \n",
        "      optimizer = optimizer_dict[self.hparams['optimizer_type']]\n",
        "                    \n",
        "      scheduler_dict = {'ExponentialLR': torch.optim.lr_scheduler.ExponentialLR(\n",
        "                                                          optimizer=optimizer,\n",
        "                                                          gamma=0.9),\n",
        "                        'StepLR': torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                              self.hparams['decay_step_size'],\n",
        "                                              self.hparams['decay_gamma']),\n",
        "                        'None': None,\n",
        "                        \n",
        "                      }\n",
        "  \n",
        "      scheduler = scheduler_dict[self.hparams['sheduler_type']]\n",
        "\n",
        "    return [optimizer], [scheduler]\n",
        "\n",
        "  def supervised(self, texts, labels, logs):\n",
        "\n",
        "    logits = self.forward(texts)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "    return logits, loss, logs\n",
        "\n",
        "  def vat(self, l_texts, labels, unl_texts, logs):\n",
        "\n",
        "    vat_loss = VATLoss(xi=self.hparams['xi'],\n",
        "                        eps=self.hparams['eps'],\n",
        "                        ip=self.hparams['ip'])\n",
        "    \n",
        "    logits = self.forward(l_texts)\n",
        "    supervised_loss = self.loss_fct(logits, labels)\n",
        "    logs.update({'supervised_loss': supervised_loss})\n",
        "\n",
        "    embeddings = self.forward(unl_texts, embeddings_only=True)\n",
        "    lds = vat_loss(self, embeddings)\n",
        "    logs.update({'lds_loss': lds})\n",
        "\n",
        "    loss = supervised_loss + self.hparams['alpha'] * lds  \n",
        "\n",
        "    return logits, loss, logs\n",
        "\n",
        "  def fixmatch(self, l_embeddings, labels, unl_embeddings, logs):\n",
        "\n",
        "    unl_w_dict, unl_s_dict = unl_embeddings\n",
        "    unl_w, unl_s = unl_w_dict['embedding'], unl_s_dict['embedding']\n",
        "\n",
        "    if self.hparams['model_arch'] == 'transformer':\n",
        "\n",
        "      stacked_inputs_ids = torch.cat((l_embeddings[\"input_ids\"],\n",
        "                                      unl_w['input_ids'],\n",
        "                                      unl_s['input_ids']))\n",
        "      \n",
        "      stacked_attention_mask = torch.cat((l_embeddings[\"attention_mask\"],\n",
        "                                          unl_w['attention_mask'],\n",
        "                                          unl_s['attention_mask']))\n",
        "      \n",
        "      stacked_token_type_ids = torch.cat((l_embeddings[\"token_type_ids\"],\n",
        "                                          unl_w['token_type_ids'],\n",
        "                                          unl_s['token_type_ids']))\n",
        "    \n",
        "      x = {\"inputs_ids\": stacked_inputs_ids,\n",
        "          \"attention_mask\": stacked_attention_mask,\n",
        "          \"token_type_ids\": stacked_token_type_ids}\n",
        "    else:\n",
        "      x = torch.cat((l_embeddings,\n",
        "                     unl_w,\n",
        "                     unl_s))\n",
        "\n",
        "    logits = self.forward(x)\n",
        "    batch_size = self.hparams['l_batch_size'] \n",
        "    logits_x = logits[:batch_size]\n",
        "    logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
        "    del logits\n",
        "     \n",
        "\n",
        "    Lx = F.cross_entropy(logits_x, labels, reduction='mean')\n",
        "\n",
        "    pseudo_label = torch.softmax(logits_u_w.detach_(), dim=-1)\n",
        "    max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
        "    print('unl_max_probs', max_probs)\n",
        "    print('unl_max_max_probs', torch.max(max_probs))\n",
        "\n",
        "    logs.update({'unl_max_confident': torch.max(max_probs),\n",
        "                 'unl_min_confident': torch.min(max_probs),\n",
        "                 'unl_mean_confident': torch.mean(max_probs),\n",
        "                 'unl_std_confident': torch.std(max_probs),\n",
        "                 })\n",
        "    mask = max_probs.ge(self.hparams['threshold']).float()\n",
        "    Lu = (F.cross_entropy(logits_u_s, targets_u,\n",
        "                          reduction='none') * mask).mean()\n",
        "\n",
        "    loss = Lx + self.hparams['lambda_u'] * Lu\n",
        "\n",
        "    logs.update({\"Lu\": Lu})\n",
        "    logs.update({\"Lx\": Lx})\n",
        "\n",
        "    return logits_x, loss, logs\n",
        "\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "\n",
        "    try:\n",
        "      labeled = next(self.train_labeled_dataloader_iterator)\n",
        "\n",
        "    except StopIteration:\n",
        "      self.train_labeled_dataloader_iterator = iter(self._train_labeled_dataloader)\n",
        "      labeled = next(self.train_labeled_dataloader_iterator)\n",
        "\n",
        "    l_texts, labels = labeled['embedding'], labeled['label']\n",
        "\n",
        "    if self.hparams['model_arch'] == 'transformer':\n",
        "      gpu_l_texts = toolz.dicttoolz.valmap(torch.Tensor.cuda, l_texts)\n",
        "    else:\n",
        "      gpu_l_texts = l_texts.cuda() \n",
        "\n",
        "    gpu_labels = labels.cuda()  \n",
        "    logs = dict()\n",
        "    \n",
        "    if self.hparams['training_method'] == 'VAT':\n",
        "      unl_embeddings = batch['embedding'] # dont need label\n",
        "      logits, loss, logs = self.vat(gpu_l_texts, gpu_labels, unl_embeddings, logs)\n",
        "    elif self.hparams['training_method'] == \"FixMatch\":\n",
        "      logits, loss, logs = self.fixmatch(gpu_l_texts, gpu_labels, batch, logs)\n",
        "    elif self.hparams['training_method'] == \"Supervised\":\n",
        "      logits, loss, logs = self.supervised(gpu_l_texts, gpu_labels, logs)\n",
        "    else:\n",
        "      raise ValueError('Wrong training method type: {} \\n Possible methods : VAT, FixMatch, Supervised'.format(self.hparams['model_arch']))\n",
        "\n",
        "    max_probs, labels_hat = torch.max(logits, dim=-1)\n",
        "\n",
        "    print('l_max_probs: ', max_probs)\n",
        "    print('l_max_max_probs: ', torch.max(max_probs))\n",
        "    print('labels_hat: ', labels_hat)\n",
        "\n",
        "    logs.update({'unl_max_confident': torch.max(max_probs),\n",
        "                 'unl_min_confident': torch.min(max_probs),\n",
        "                 'unl_mean_confident': torch.mean(max_probs),\n",
        "                 'unl_std_confident': torch.std(max_probs),\n",
        "                 })\n",
        "\n",
        "    # labels_hat = logits.max(dim=1)[1]\n",
        "\n",
        "    labels = gpu_labels.cpu()\n",
        "    labels_hat = labels_hat.cpu()\n",
        "    \n",
        "    accuracy_error = 1 - accuracy_score(labels, labels_hat)\n",
        "    f1_error = 1 - f1_score(labels, labels_hat, average='micro')\n",
        "    recall_error = 1 - recall_score(labels, labels_hat, average='micro')\n",
        "    precision_error = 1 - precision_score(labels, labels_hat, average='micro')\n",
        "\n",
        "    logs.update({'train_loss': loss,\n",
        "            'train_accuracy_error': accuracy_error,\n",
        "            'train_f1_error': f1_error,\n",
        "            'train_recall_error': recall_error,\n",
        "            'train_precision_error': precision_error,\n",
        "           })  \n",
        "\n",
        "    return {'loss': loss,\n",
        "            'log': logs}\n",
        "\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "\n",
        "    embeddings = batch['embedding']\n",
        "    labels = batch['label']\n",
        "  \n",
        "    logits = self.forward(embeddings)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "    labels_hat = torch.argmax(logits, dim=1)\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    labels_hat = labels_hat.cpu()\n",
        "\n",
        "    accuracy_error = 1 - accuracy_score(labels, labels_hat)\n",
        "    f1_error = 1 - f1_score(labels, labels_hat, average='micro')\n",
        "    recall_error = 1 - recall_score(labels, labels_hat, average='micro')\n",
        "    precision_error = 1 - precision_score(labels, labels_hat, average='micro')\n",
        "  \n",
        "    output = {\n",
        "            \"val_loss\": loss,\n",
        "            'accuracy_error': accuracy_error,\n",
        "            'f1_error': f1_error,\n",
        "            'recall_error': recall_error,\n",
        "            'precision_error': precision_error,\n",
        "            }\n",
        "  \n",
        "    return output\n",
        "\n",
        "\n",
        "  def validation_epoch_end(self, outputs):\n",
        "\n",
        "    val_acc = compute_global_metric(outputs, 'accuracy_error') \n",
        "    val_f1 = compute_global_metric(outputs, 'f1_error')\n",
        "    val_recall = compute_global_metric(outputs, 'recall_error')\n",
        "    val_precision = compute_global_metric(outputs, 'precision_error')\n",
        "    val_loss = compute_global_metric(outputs, \"val_loss\")\n",
        "\n",
        "    tqdm_dict = {\n",
        "                 \"val_loss\": val_loss,\n",
        "                 \"val_acc_error\": val_acc,\n",
        "                 \"val_f1_error\": val_f1,\n",
        "                 \"val_recall_error\": val_recall,\n",
        "                 \"val_precision_error\": val_precision,\n",
        "                }\n",
        "    return {\n",
        "            \"progress_bar\": tqdm_dict,\n",
        "            \"log\": tqdm_dict,\n",
        "            \"val_loss\": val_loss,\n",
        "            'val_acc_error': val_acc,\n",
        "            'val_f1_error': val_f1\n",
        "           }\n",
        "\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "\n",
        "    embeddings = batch[\"embedding\"]\n",
        "    labels = batch[\"label\"]\n",
        "  \n",
        "    logits = self.forward(embeddings)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "    labels_hat = torch.argmax(logits, dim=1)\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    labels_hat = labels_hat.cpu()\n",
        "\n",
        "    accuracy_error = 1 - accuracy_score(labels, labels_hat)\n",
        "    f1_error = 1 - f1_score(labels, labels_hat, average='micro')\n",
        "    recall_error = 1 - recall_score(labels, labels_hat, average='micro')\n",
        "    precision_error = 1 - precision_score(labels, labels_hat, average='micro')\n",
        "  \n",
        "    return {\n",
        "            \"test_loss\": loss,\n",
        "            'accuracy_error': accuracy_error,\n",
        "            'f1_error': f1_error,\n",
        "            'recall_error': recall_error,\n",
        "            'precision_error': precision_error,\n",
        "           }\n",
        "\n",
        "\n",
        "  def test_epoch_end(self, outputs):\n",
        "\n",
        "    test_acc = compute_global_metric(outputs, 'accuracy_error') \n",
        "    test_f1 = compute_global_metric(outputs, 'f1_error')\n",
        "    test_recall = compute_global_metric(outputs, 'recall_error')\n",
        "    test_precision = compute_global_metric(outputs, 'precision_error')\n",
        "    test_loss = compute_global_metric(outputs, \"test_loss\")\n",
        "\n",
        "    tqdm_dict = {\n",
        "                 \"test_loss\": test_loss,\n",
        "                 \"test_acc_error\": test_acc,\n",
        "                 \"test_f1_error\": test_f1,\n",
        "                 \"test_recall_error\": test_recall,\n",
        "                 \"test_precision_error\": test_precision,\n",
        "                }\n",
        "    return {\n",
        "            \"progress_bar\": tqdm_dict,\n",
        "            \"log\": tqdm_dict,\n",
        "            \"test_loss\": test_loss,\n",
        "            'test_acc_error': test_acc,\n",
        "            'test_f1_error': test_f1\n",
        "           }"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aYX9ZJVIoEs",
        "colab_type": "text"
      },
      "source": [
        "### Configure experiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC9zE6cKkbPi",
        "colab_type": "text"
      },
      "source": [
        "##### Choose dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEwfu7pNkDJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = 'IMDB'\n",
        "\n",
        "if dataset == 'IMDB':\n",
        "  dataset_path = str(Path()/\n",
        "                        'gdrive'/\n",
        "                        'My Drive'/\n",
        "                        'praca_magisterska'/\n",
        "                        'pytorch_lightning'/\n",
        "                        'datasets'/\n",
        "                        'imdb_with_bt.csv')\n",
        "  \n",
        "  hparams = {\"dataset\": 'IMDB',\n",
        "            \"num_classes\": 2,\n",
        "            \"dataset_path\": dataset_path,\n",
        "            }\n",
        "\n",
        "\n",
        "elif dataset == 'MR':\n",
        "  dataset_path = str(Path()/\n",
        "                        'gdrive'/\n",
        "                        'My Drive'/\n",
        "                        'praca_magisterska'/\n",
        "                        'pytorch_lightning'/\n",
        "                        'datasets'/\n",
        "                        'mr_with_bt.csv')\n",
        "  \n",
        "  hparams = {\"dataset\": 'MR',\n",
        "            \"num_classes\": 2,\n",
        "            \"dataset_path\": dataset_path,\n",
        "            }\n",
        "\n",
        "elif dataset == 'NLUHD':\n",
        "  dataset_path = str(Path()/\n",
        "                    'gdrive'/\n",
        "                    'My Drive'/\n",
        "                    'praca_magisterska'/\n",
        "                    'pytorch_lightning'/\n",
        "                    'datasets'/\n",
        "                    'NLU-Data-Home-Domain-preprocessed-without-ner.csv')\n",
        "  \n",
        "  hparams = {\"dataset\": 'NLUHD',\n",
        "            \"num_classes\": 68, # ???\n",
        "            \"dataset_path\": dataset_path}\n",
        "            \n",
        "else:\n",
        "  raise ValueError('Wrong dataset name : {} \\\n",
        "   \\n Possible datasets: IMDB, MR, NLUHD'.format(hparams['dataset']))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uf3R7m6kd6z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##### Choose traning method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxhbdqoNkeGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_method = 'VAT' \n",
        "\n",
        "if training_method == 'VAT':\n",
        "  hparams.update({\"training_method\": 'VAT',\n",
        "                  'xi':1e-05,\n",
        "                  'eps':4.5, \n",
        "                  'ip':1, \n",
        "                  'alpha':1} \n",
        "                )\n",
        "elif training_method == 'FixMatch':\n",
        "  hparams.update({\"training_method\": 'FixMatch',\n",
        "                  'mu': 4, \n",
        "                  'threshold': 0.67,\n",
        "                  'lambda_u': 1,\n",
        "                  'n_weak': 0,\n",
        "                  'show_augmentation': False,\n",
        "                })\n",
        "\n",
        "elif training_method == 'Supervised':\n",
        "  hparams.update({\"training_method\": 'Supervised'})\n",
        "\n",
        "else:\n",
        "  raise ValueError('Wrong training method type: {} \\n\" \\\n",
        "       Possible methods : VAT, FixMatch, Supervised'.format(self.hparams['model_arch']))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FownSMlUmIzo",
        "colab_type": "text"
      },
      "source": [
        "#### Choose model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH4lZGqlmJAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_arch = 'Convolution'\n",
        "\n",
        "if model_arch == \"Transformer\":\n",
        "  hparams.update({\n",
        "      'model_arch': \"Transformer\",\n",
        "      'model_type': 'bert-base-uncased',\n",
        "      'max_sentence_len': 156,\n",
        "\n",
        "      'xi': 0.00001,\n",
        "      'lr': 5e-5,\n",
        "      'weight_decay': 0.01,\n",
        "      'adam_eps': 1e-06,\n",
        "      'warmup_steps': 150,\n",
        "      })\n",
        "\n",
        "elif model_arch == 'Convolution':\n",
        "  hparams.update({\n",
        "           'model_arch': \"Convolution\",\n",
        "           'max_sentence_len': 400,\n",
        "           'embeder_type': \"fastText\",\n",
        "           'embed_dim': 300,\n",
        "           'Ci': 1,\n",
        "           'kernel_num': 100,\n",
        "           'kernel_sizes': '3,4,5',\n",
        "           'dropout':0.5, \n",
        "\n",
        "           'optimizer_type': 'Adam',\n",
        "           'sheduler_type': 'StepLR',\n",
        "           'lr': 0.001,\n",
        "           'decay_step_size': 10000,\n",
        "           'decay_gamma':0.5,\n",
        "\n",
        "  })\n",
        "else:\n",
        "  raise ValueError('Wrong model architecture type: {} \\\n",
        "   \\n Possible datasets: Transformer, Convolution'.format(self.hparams['model_arch']))\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st6sL-cOwym2",
        "colab_type": "text"
      },
      "source": [
        "#### Choose training params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8YWsCR-jGM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " hparams.update({\n",
        "           'train_test_split': 0.9,\n",
        "           'seed': 42,\n",
        "           'l_batch_size': 16,\n",
        "           'unl_batch_size': 32,\n",
        "           'val_batch_size': 16,\n",
        "           'test_batch_size': 16,\n",
        "           'n_labeled': 160, # number of labeled samples  # must be a multiplication of l_batch_size\n",
        "           'valid_size_per_class': 1000, # 68 class => n_val_samples = 68 * 10 \n",
        "\n",
        "           'loss_function':'CrossEntropyLoss',\n",
        "\n",
        "           'test_run': False,\n",
        "           'max_epochs': 5,\n",
        "           'min_epochs': 1,\n",
        "           'val_check_interval': 0.2, \n",
        "           'patience': 15, # early stopping callback parameter\n",
        "     \n",
        " }) \n",
        "\n",
        " \n",
        "tags = []\n",
        "tags.append(hparams['dataset'])\n",
        "tags.append(hparams['model_arch'])\n",
        "tags.append(hparams['training_method'])\n",
        "tags.append(hparams['n_labeled'])\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4n0cvZNw5Ec",
        "colab_type": "text"
      },
      "source": [
        "### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "velwnCrgNCeb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e6d5042-e9eb-4273-ee26-7a9e8cafd9c3"
      },
      "source": [
        "set_seed(hparams['seed'])\n",
        "experiment_name = training_method + \"-\" + model_arch + \"-\" + dataset \n",
        "neptune_logger = NeptuneLogger(\n",
        "                               project_name=\"m1f1/lightning-exps-text\",\n",
        "                               close_after_fit=False,\n",
        "                               experiment_name=experiment_name,  # Optional,\n",
        "                               params=hparams, # Optional,\n",
        "                               tags=tags # Optional,\n",
        "                              )\n",
        "# callbacks\n",
        "early_stop_callback = EarlyStopping(\n",
        "                        monitor=\"val_loss\",\n",
        "                        min_delta=0.0,\n",
        "                        patience=hparams['patience'],\n",
        "                        verbose=True,\n",
        "                        mode='min'\n",
        "                      )\n",
        "# Path(\"./checkpoints\").mkdir(parents=True, exist_ok=True)\n",
        "# model_checkpoint = pl.callbacks.ModelCheckpoint(filepath='./checkpoints') # check if it overwrite last checkpoint\n",
        "\n",
        "# training and evaluating model\n",
        "trainer = pl.Trainer(\n",
        "                gpus=1,\n",
        "                logger=neptune_logger,\n",
        "                # checkpoint_callback=model_checkpoint,\n",
        "                # early_stop_callback=early_stop_callback,\n",
        "                val_check_interval=hparams['val_check_interval'],\n",
        "                # distributed_backend=hparams['distributed_backend'],\n",
        "                # default_root_dir=\"./test_run_logs\",\n",
        "                fast_dev_run=hparams['test_run'],\n",
        "              #  train_percent_check=0.001,\n",
        "              #  val_percent_check=0.001,\n",
        "                min_epochs=hparams['min_epochs'],\n",
        "                max_epochs=hparams['max_epochs'],\n",
        "          )\n",
        "\n",
        "model = LitComposableFramework(hparams)\n",
        "[print(f'{k}: {v}') for k, v in hparams.items()]\n",
        "    \n",
        "trainer.fit(model)\n",
        "trainer.test(model)\n",
        "\n",
        "# list_of_files = glob.glob('./checkpoints/*') # * means all if need specific format then *.csv\n",
        "# latest_file = max(list_of_files, key=os.path.getctime)\n",
        "# print(latest_file)\n",
        "# model = LitBERT.load_from_checkpoint(latest_file))\n",
        "\n",
        "\n",
        "# neptune_logger.experiment.log_artifact('./checkpoints')\n",
        "neptune_logger.experiment.log_artifact(os.environ['REQUIREMENTS_PATH'])\n",
        "neptune_logger.experiment.stop()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://ui.neptune.ai/m1f1/lightning-exps-text/e/LIG-400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "NeptuneLogger will work in online mode\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset: IMDB\n",
            "num_classes: 2\n",
            "dataset_path: gdrive/My Drive/praca_magisterska/pytorch_lightning/datasets/imdb_with_bt.csv\n",
            "training_method: VAT\n",
            "xi: 1e-05\n",
            "eps: 4.5\n",
            "ip: 1\n",
            "alpha: 1\n",
            "model_arch: Convolution\n",
            "max_sentence_len: 400\n",
            "embeder_type: fastText\n",
            "embed_dim: 300\n",
            "Ci: 1\n",
            "kernel_num: 100\n",
            "kernel_sizes: 3,4,5\n",
            "dropout: 0.5\n",
            "optimizer_type: Adam\n",
            "sheduler_type: StepLR\n",
            "lr: 0.001\n",
            "decay_step_size: 10000\n",
            "decay_gamma: 0.5\n",
            "train_test_split: 0.9\n",
            "seed: 42\n",
            "l_batch_size: 16\n",
            "unl_batch_size: 32\n",
            "val_batch_size: 16\n",
            "test_batch_size: 16\n",
            "n_labeled: 160\n",
            "valid_size_per_class: 1000\n",
            "loss_function: CrossEntropyLoss\n",
            "test_run: False\n",
            "max_epochs: 5\n",
            "min_epochs: 1\n",
            "val_check_interval: 0.2\n",
            "patience: 15\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 6 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0           50000 non-null  object\n",
            " 1   text    50000 non-null  object\n",
            " 2   intent  50000 non-null  object\n",
            " 3   bt      50000 non-null  object\n",
            " 4   len     50000 non-null  object\n",
            " 5   diff    50000 non-null  object\n",
            "dtypes: object(6)\n",
            "memory usage: 2.3+ MB\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-be13329f0a3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{k}: {v}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0;31m# or in the case where each node needs to do its own manipulation in which case just local_rank=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_prepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_data_prepared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-f59529e4f8cb>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m       \u001b[0munique_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'unique_labels: {unique_labels}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'number_of_categories : {len(unique_labels)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'label'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8Loooc9iAcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = pd.read_csv(str(Path(os.environ['DATASETS_PATH'])/'imdb_with_bt.csv'))\n",
        "# df = df.rename(columns={'intent': 'label', 'bt': 'paraphrases'})\n",
        "# df.to_csv(str(Path(os.environ['DATASETS_PATH'])/'imdb_with_bt.csv'))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfaD-N-RFmpH",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhTVU9r32eyI",
        "colab_type": "text"
      },
      "source": [
        "#### System spec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucbrH_0LsQ8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LitYKConv_HPO(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, hparams, trial):\n",
        "\n",
        "    super().__init__()\n",
        "    self.hparams = hparams\n",
        "\n",
        "    if self.hparams['with_VAT']:\n",
        "      xi_interval = list(map(float, hparams['xi'].split(',')))\n",
        "      eps_interval = list(map(float, hparams['eps'].split(',')))\n",
        "      ip_interval = list(map(int, hparams['ip'].split(',')))\n",
        "      alpha_interval = list(map(float, hparams['alpha'].split(',')))\n",
        "\n",
        "      self.xi = trial.suggest_uniform('xi', *xi_interval)\n",
        "      print('xi: ', self.xi)\n",
        "      self.eps = trial.suggest_uniform('eps', *eps_interval)\n",
        "      print('eps: ', self.eps)\n",
        "      self.ip = trial.suggest_int('ip', *ip_interval)\n",
        "      print('ip: ', self.ip)\n",
        "      self.alpha = trial.suggest_uniform('alpha', *alpha_interval)\n",
        "      print('alpha: ', self.alpha)\n",
        "\n",
        "    lr_interval = list(map(float, hparams['lr'].split(',')))\n",
        "    print(lr_interval)\n",
        "    kernel_num_interval = list(map(int, hparams['kernel_num'].split(','))) \n",
        "    print(kernel_num_interval)\n",
        "    dropout_interval = list(map(float, hparams['dropout'].split(','))) \n",
        "    print(dropout_interval)\n",
        "\n",
        "    self.lr = trial.suggest_loguniform('learning_rate', *lr_interval)\n",
        "    print('lr: ', self.lr)\n",
        "    self.Co = trial.suggest_int('kernel_num', *kernel_num_interval) #hparams['kernel_num']\n",
        "    print('kernel_num: ', self.Co)\n",
        "    dropout = trial.suggest_uniform('dropout', *dropout_interval)\n",
        "    print('dropout: ', dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    self.embeder_dict = {\n",
        "                         'fastText': (create_ft_embeder, gensim_tokenizer),\n",
        "                         'spaCy':(create_spacy_nlp_embeder, lambda x: x)\n",
        "                        }\n",
        "    embeder, self.tokenizer_fun = self.embeder_dict[hparams['embeder_type']]\n",
        "    self.embeder = nlp #embeder()\n",
        "    self.D = hparams['embed_dim']\n",
        "    self.Ci = hparams['Ci'] \n",
        "    \n",
        "    self.loss_fct = getattr(nn, hparams['loss_function'])()\n",
        "    self.num_classes = hparams['num_classes']\n",
        "\n",
        "    self.Ks = list(map(int, hparams['kernel_sizes'].split(','))) # (3,4,5)\n",
        "    self.convs1 = nn.ModuleList([nn.Conv2d(self.Ci, self.Co, (K, self.D)) for K in self.Ks])\n",
        "    self.fc1 = nn.Linear(len(self.Ks) * self.Co, self.num_classes) \n",
        "\n",
        "    self.total_iterations = 0 \n",
        "\n",
        "  def forward(self, x):\n",
        "      # print('org: ', x.size())\n",
        "      x = x.unsqueeze(self.Ci)  # (N, Ci, W, D)\n",
        "      # from pdb import set_trace as st\n",
        "      # st() \n",
        "      # print(f'unsqueeze {self.Ci}: {x.size()}')\n",
        "      x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
        "      # print(f'conv, relu, squeeze : {x.size()}')\n",
        "      x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "      # print(f'max_pool1d, squeeze : {x.size()}')\n",
        "      x = torch.cat(x, 1)\n",
        "      # print(f' cat: {x.size()}')\n",
        "      x = self.dropout(x)  # (N, len(Ks)*Co)\n",
        "      logit = self.fc1(x)  # (N, C)\n",
        "      # print(f' logit: {logit.size()}')\n",
        "      return logit\n",
        "\n",
        "\n",
        "  def prepare_data(self):\n",
        "\n",
        "    if self.hparams['dataset'] == 'NLUHD':\n",
        "\n",
        "      not_none = lambda x: x[\"text\"] is not None \n",
        "      ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "      unique_labels = list(pd.DataFrame(ds).intent.unique())\n",
        "      le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      print(f\"Unique labels: {unique_labels}\")\n",
        "      print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                            int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                            seed=self.hparams['seed'])\n",
        "      preprocessor = partial(\n",
        "                            preprocess_NLUHD,\n",
        "                            self.hparams['model_type'],\n",
        "                            self.hparams['max_sentence_len'],\n",
        "                            self.tokenizer_dict[self.hparams['model_type']],\n",
        "                            le,\n",
        "                            )\n",
        "      \n",
        "    elif self.hparams['dataset'] == 'MR':\n",
        "\n",
        "      preprocessor = partial(\n",
        "                            preprocess_MR,\n",
        "                            self.hparams,\n",
        "                            self.tokenizer_fun, \n",
        "                            self.embeder,\n",
        "                            le,\n",
        "                            )\n",
        "\n",
        "    elif self.hparams['dataset'] == 'IMDB':\n",
        "\n",
        "      ds = lfds.Imdb('train') + lfds.Imdb('test')\n",
        "      ds = ds.map(lambda x: {'text': x[0], 'label': x[1]})\n",
        "      df = pd.DataFrame(ds)\n",
        "      # self.embeder.build_vocab(new_sentences, update=True)\n",
        "      # self.embeder.train(new_sentences, total_examples=len(new_sentences), epochs=)\n",
        "      print(df.info(memory_usage=True))\n",
        "      unique_labels = list(df.label.unique())\n",
        "      print(f'unique_labels: {unique_labels}')\n",
        "      print(f'number_of_categories : {len(unique_labels)}')\n",
        "      le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                             int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                             seed=self.hparams['seed'])\n",
        "      preprocessor = partial(\n",
        "                             preprocess_IMDB,\n",
        "                             self.hparams,\n",
        "                             self.tokenizer_fun, \n",
        "                             self.embeder,\n",
        "                             le,\n",
        "                            )\n",
        "    else:\n",
        "      raise ValueError('Wrong dataset name : {}'.format(self.hparams['dataset']))\n",
        "\n",
        "    \n",
        "    train_df, test_df = pd.DataFrame(train), pd.DataFrame(test)\n",
        "    x_train, y_train = train_df['text'].values, train_df['label'].values\n",
        "    x_test, y_test = test_df['text'].values, test_df['label'].values\n",
        "\n",
        "    # split's parameters  \n",
        "    num_classes = len(unique_labels)\n",
        "    label_per_class = self.hparams['n_labeled'] // num_classes\n",
        "    valid_size = self.hparams['valid_size_per_class']\n",
        "\n",
        "    labeled_idx = []\n",
        "    unlabeled_idx = []\n",
        "    val_idx = []\n",
        "    \n",
        "    for label in unique_labels:\n",
        "        idx = np.where(y_train == label)[0]\n",
        "        np.random.shuffle(idx)\n",
        "        labeled_idx.extend(idx[:label_per_class])\n",
        "        val_idx.extend(idx[label_per_class: label_per_class + valid_size])\n",
        "        unlabeled_idx.extend(idx[label_per_class + valid_size:])\n",
        "\n",
        "    x_labeled, y_labeled  = x_train[labeled_idx], y_train[labeled_idx]\n",
        "    x_unlabeled, y_unlabeled = x_train[unlabeled_idx], y_train[unlabeled_idx]\n",
        "    x_val, y_val = x_train[val_idx], y_train[val_idx]\n",
        "    \n",
        "    train_labeled_dataset = SimpleTextDataset(x_labeled,\n",
        "                                              y_labeled,\n",
        "                                              transform=preprocessor)\n",
        "    \n",
        "    train_unlabeled_dataset = SimpleTextDataset(x_unlabeled,\n",
        "                                                y_unlabeled,\n",
        "                                                transform=preprocessor)\n",
        "    \n",
        "    self._train_dataset = TwoInOneDataset([train_labeled_dataset,\n",
        "                                            train_unlabeled_dataset])\n",
        "    \n",
        "    self._val_dataset = SimpleTextDataset(x_val,\n",
        "                                          y_val,\n",
        "                                          transform=preprocessor)\n",
        "    \n",
        "    self._test_dataset = SimpleTextDataset(x_test,\n",
        "                                           y_test,\n",
        "                                           transform=preprocessor)\n",
        "    \n",
        "    self.total_iterations = len(train_unlabeled_dataset) // self.hparams['batch_size'] \n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(\n",
        "                      self._train_dataset,\n",
        "                      batch_size=self.hparams['batch_size'],\n",
        "                      num_workers=8,\n",
        "                      shuffle=True # without shuffle it want work cause it need to create map index before __get_item__ function\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "                      self._val_dataset,\n",
        "                      batch_size=self.hparams['batch_size'],\n",
        "                      num_workers=8\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(\n",
        "                      self._test_dataset,\n",
        "                      batch_size=self.hparams['batch_size'],\n",
        "                      num_workers=8\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "\n",
        "    optimizers = [\n",
        "                  torch.optim.Adam(self.parameters(), lr=self.lr),\n",
        "                 ]\n",
        "    schedulers = [\n",
        "                  {\n",
        "                    'scheduler': ReduceLROnPlateau(optimizers[0],'min', verbose=True), \n",
        "                    'monitor': 'val_loss', # Default: val_loss\n",
        "                    'interval': 'epoch',\n",
        "                    'frequency': 1\n",
        "                  },\n",
        "                 ]\n",
        "\n",
        "    return optimizers, schedulers\n",
        "\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "\n",
        "    l_batch = batch[0]\n",
        "    l_texts = l_batch['embedding']\n",
        "    labels = l_batch['label']\n",
        "    unl_texts = batch[1]['embedding']\n",
        "\n",
        "    if self.hparams['with_VAT']:\n",
        "      vat_loss = VATLoss(xi=self.xi,\n",
        "                         eps=self.eps,\n",
        "                         ip=self.ip)\n",
        "      lds = vat_loss(self, unl_texts)\n",
        "\n",
        "    logits = self.forward(l_texts)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "\n",
        "    if self.hparams['with_VAT']:\n",
        "      loss += self.alpha * lds \n",
        "\n",
        "    labels_hat = logits.max(dim=1)[1]\n",
        "\n",
        "    labels = labels.detach().cpu()\n",
        "    labels_hat = labels_hat.detach().cpu()\n",
        "    \n",
        "    accuracy_error = torch.tensor(1 - accuracy_score(labels, labels_hat))\n",
        "    f1_error = torch.tensor(1 - f1_score(labels, labels_hat, average='micro'))\n",
        "    recall_error = torch.tensor(1 - recall_score(labels, labels_hat, average='micro'))\n",
        "    precision_error = torch.tensor(1 - precision_score(labels, labels_hat, average='micro'))\n",
        "\n",
        "    logs = {'train_loss': loss,\n",
        "            'train_accuracy_error': accuracy_error,\n",
        "            'train_f1_error': f1_error,\n",
        "            'train_recall_error': recall_error,\n",
        "            'train_precision_error': precision_error,\n",
        "           }  \n",
        "\n",
        "    if self.hparams['with_VAT']:\n",
        "      logs.update({'lds_loss': lds.item()})\n",
        "\n",
        "\n",
        "    return {'loss': loss,\n",
        "            'log': logs}\n",
        "\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    texts = batch['embedding']\n",
        "    labels = batch['label']\n",
        "  \n",
        "    logits = self.forward(texts)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "    labels_hat = torch.argmax(logits, dim=1)\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    labels_hat = labels_hat.cpu()\n",
        "\n",
        "    accuracy_error = torch.tensor(1 - accuracy_score(labels, labels_hat))\n",
        "    f1_error = torch.tensor(1 - f1_score(labels, labels_hat, average='micro'))\n",
        "    recall_error = torch.tensor(1 - recall_score(labels, labels_hat, average='micro'))\n",
        "    precision_error = torch.tensor(1 - precision_score(labels, labels_hat, average='micro'))\n",
        "  \n",
        "    output = {\n",
        "            \"val_loss\": loss,\n",
        "            'accuracy_error': accuracy_error,\n",
        "            'f1_error': f1_error,\n",
        "            'recall_error': recall_error,\n",
        "            'precision_error': precision_error,\n",
        "            }\n",
        "  \n",
        "    return output\n",
        "\n",
        "\n",
        "  def validation_epoch_end(self, outputs):\n",
        "    # CHANGE FOR TENSORS!!!!\n",
        "    val_acc = compute_global_metric(outputs, 'accuracy_error')\n",
        "    val_f1 = compute_global_metric(outputs, 'f1_error')\n",
        "    val_recall = compute_global_metric(outputs, 'recall_error')\n",
        "    val_precision = compute_global_metric(outputs, 'precision_error')\n",
        "    val_loss = compute_global_metric(outputs, \"val_loss\")\n",
        "\n",
        "    tqdm_dict = {\n",
        "                 \"val_loss\": val_loss,\n",
        "                 \"val_acc\": val_acc,\n",
        "                 \"val_f1\": val_f1,\n",
        "                 \"val_recall\": val_recall,\n",
        "                 \"val_precision\": val_precision,\n",
        "                }\n",
        "    return {\n",
        "            \"progress_bar\": tqdm_dict,\n",
        "            \"log\": tqdm_dict,\n",
        "            \"val_loss\": val_loss,\n",
        "            'val_acc': val_acc,\n",
        "            'val_f1': val_f1\n",
        "           }\n",
        "\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "\n",
        "    texts = batch[\"embedding\"]\n",
        "    labels = batch[\"label\"]\n",
        "  \n",
        "    logits = self.forward(texts)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "    labels_hat = torch.argmax(logits, dim=1)\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    labels_hat = labels_hat.cpu()\n",
        "\n",
        "\n",
        "    accuracy_error = torch.tensor(1 - accuracy_score(labels, labels_hat))\n",
        "    f1_error = torch.tensor(1 - f1_score(labels, labels_hat, average='micro'))\n",
        "    recall_error = torch.tensor(1 - recall_score(labels, labels_hat, average='micro'))\n",
        "    precision_error = torch.tensor(1 - precision_score(labels, labels_hat, average='micro'))\n",
        "  \n",
        "    return {\n",
        "            \"test_loss\": loss,\n",
        "            'accuracy_error': accuracy_error,\n",
        "            'f1_error': f1_error,\n",
        "            'recall_error': recall_error,\n",
        "            'precision_error': precision_error,\n",
        "           }\n",
        "\n",
        "\n",
        "  def test_epoch_end(self, outputs):\n",
        "\n",
        "    test_acc = compute_global_metric(outputs, 'accuracy_error') \n",
        "    test_f1 = compute_global_metric(outputs, 'f1_error')\n",
        "    test_recall = compute_global_metric(outputs, 'recall_error')\n",
        "    test_precision = compute_global_metric(outputs, 'precision_error')\n",
        "    test_loss = compute_global_metric(outputs, \"test_loss\")\n",
        "\n",
        "    tqdm_dict = {\n",
        "                 \"test_loss\": test_loss,\n",
        "                 \"test_acc\": test_acc,\n",
        "                 \"test_f1\": test_f1,\n",
        "                 \"test_recall\": test_recall,\n",
        "                 \"test_precision\": test_precision,\n",
        "                }\n",
        "    return {\n",
        "            \"progress_bar\": tqdm_dict,\n",
        "            \"log\": tqdm_dict,\n",
        "            \"test_loss\": test_loss,\n",
        "            'test_acc': test_acc,\n",
        "            'test_f1': test_f1\n",
        "           }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8blDj0ji58J4",
        "colab_type": "text"
      },
      "source": [
        "#### Define objective func\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPAMdmP6F-DM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def objective(trial):\n",
        "  # nluhd_dataset_path = str(Path()/\n",
        "  #                         'gdrive'/\n",
        "  #                         'My Drive'/\n",
        "  #                         'praca_magisterska'/\n",
        "  #                         'pytorch_lightning'/\n",
        "  #                         'datasets'/\n",
        "  #                         'NLU-Data-Home-Domain-preprocessed-without-ner.csv')\n",
        "  hparams = {\n",
        "            # model architecture\n",
        "            'model_type': 'YoonKimConvNN',\n",
        "            'dropout':'0.2,0.7', \n",
        "            'kernel_sizes': '3,4,5', # (3,4,5)\n",
        "            'kernel_num': '60,120', # interval\n",
        "            'Ci': 1,\n",
        "            'loss_function':'CrossEntropyLoss',\n",
        "            # pl trainer params\n",
        "            'seed': 42,\n",
        "            'monitor_value': 'val_acc',\n",
        "            'percent_valid_examples': 0.5, \n",
        "            'test_run': False,\n",
        "            'with_VAT': True,\n",
        "            'max_epochs': 2,\n",
        "            'min_epochs': 1,\n",
        "            'val_check_interval': 0.5, \n",
        "            'patience': 3, # early stopping callback parameter\n",
        "            'distributed_backend': 'dp',\n",
        "            # embeddings params\n",
        "            'embeder_type': \"fastText\",\n",
        "            'embed_dim': 300,\n",
        "            'max_sentence_len': 400,\n",
        "            # dataset params\n",
        "            'train_test_split': 0.8,\n",
        "            'batch_size': 32,\n",
        "            'n_labeled': 1000, # number of labeled samples \n",
        "            'valid_size_per_class': 1000, # 68 class => n_val_samples = 68 * 10 \n",
        "            # optimizer params\n",
        "            'lr': '0.00001, 10',\n",
        "            # VAT params\n",
        "            'xi':'6,12',\n",
        "            'eps':'1,3',\n",
        "            'ip':'1,3',\n",
        "            'alpha':'1,3',\n",
        "            }\n",
        "  \n",
        "  set_seed(hparams['seed'])\n",
        "  \n",
        "  hparams.update({'dataset':'IMDB',\n",
        "                  'num_classes': 2})\n",
        "  \n",
        "  \n",
        "  # training and evaluating model\n",
        "  \n",
        "          \n",
        "  metrics_callback = MetricsCallback()\n",
        "  \n",
        "  checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "          os.path.join(os.environ['RESULT_PATH'],\n",
        "                       \"trial_{}\".format(trial.number),\n",
        "                       \"{epoch}\"), monitor=hparams['monitor_value']\n",
        "      )\n",
        "  \n",
        "  trainer = pl.Trainer(\n",
        "                  gpus=1 if torch.cuda.is_available() else None,\n",
        "                  logger=False,\n",
        "                  # val_percent_check=hparams['percent_valid_examples'],\n",
        "                  checkpoint_callback=checkpoint_callback,\n",
        "                  max_epochs=hparams['max_epochs'],\n",
        "                  fast_dev_run=hparams['test_run'],\n",
        "                  callbacks=[metrics_callback],\n",
        "                  early_stop_callback=PyTorchLightningPruningCallback(trial, monitor=hparams['monitor_value'])\n",
        "            )\n",
        "  \n",
        "  model = LitYKConv_HPO(hparams, trial=trial)\n",
        "  \n",
        "  trainer.fit(model)\n",
        "  return metrics_callback.metrics[-1][\"val_acc\"]\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjAj0j4L5vTG",
        "colab_type": "text"
      },
      "source": [
        "#### Run trails"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuNsLhst6BPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pruning = True\n",
        "\n",
        "pruner = optuna.pruners.MedianPruner() if pruning else optuna.pruners.NopPruner()\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
        "study.optimize(objective, n_trials=20, timeout=None)\n",
        "\n",
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "# shutil.rmtree(os.environ['RESULT_PATH'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8LrwpD19Y3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optuna.visualization.plot_intermediate_values(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y5eSN9y9Xqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optuna.visualization.plot_param_importances(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRChtuTLMfxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}