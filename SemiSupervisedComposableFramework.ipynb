{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemiSupervisedComposableFramework.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_DWq_Q17ENpc",
        "_9cu0GG3qjHz",
        "aHn7X8Oc2IJN",
        "AhTVU9r32eyI",
        "8blDj0ji58J4",
        "AjAj0j4L5vTG"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "170da35b4ba0440b85fb8ec2e8e349e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_433ad3af0ab941ee8ee02368de5a7552",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f8e91bd0bc424aa398045c0d520acf2d",
              "IPY_MODEL_fe8b40a7e9634535ac12608b2b996343"
            ]
          }
        },
        "433ad3af0ab941ee8ee02368de5a7552": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "f8e91bd0bc424aa398045c0d520acf2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ccfefc0b38a244f4bc3e576c26ed8bca",
            "_dom_classes": [],
            "description": "Validation sanity check: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4714c7a6d319430fb4745b0c5a41499b"
          }
        },
        "fe8b40a7e9634535ac12608b2b996343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7cefa9d3958e4d528b99ec9e1dc699f2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:04&lt;00:00,  2.69s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fbeb2198b30c40f6a5aa9d6b6022b7f7"
          }
        },
        "ccfefc0b38a244f4bc3e576c26ed8bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4714c7a6d319430fb4745b0c5a41499b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7cefa9d3958e4d528b99ec9e1dc699f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fbeb2198b30c40f6a5aa9d6b6022b7f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d8352f8eb7414187a7d8db2a71d518a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2bc61761751b45c1a6a1bc89d742d2a6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2f161cd484b94a28942ebc744d26080c",
              "IPY_MODEL_7dfac5c9374c4d79ae82c47142799a34"
            ]
          }
        },
        "2bc61761751b45c1a6a1bc89d742d2a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "2f161cd484b94a28942ebc744d26080c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d8b94fa865ca4b6db07b10061756e20a",
            "_dom_classes": [],
            "description": "Epoch 1:   2%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1964,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 31,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_644c2909cc1b42528d226db1e3f37e50"
          }
        },
        "7dfac5c9374c4d79ae82c47142799a34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e1cd1c693b8e43adace46c11b38e6a11",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 31/1964 [00:26&lt;27:20,  1.18it/s, loss=0.687, v_num=LIG-435]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e038e0ab872b47c4b8664d640a6a7aaa"
          }
        },
        "d8b94fa865ca4b6db07b10061756e20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "644c2909cc1b42528d226db1e3f37e50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e1cd1c693b8e43adace46c11b38e6a11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e038e0ab872b47c4b8664d640a6a7aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M1F1/MasterThesis/blob/master/SemiSupervisedComposableFramework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmIolHgLI6Yn",
        "colab_type": "text"
      },
      "source": [
        "### Setup env "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE7eYLcuXnX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "2371be18-f68c-4dcb-898d-c6aa342cffa4"
      },
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "import pkg_resources\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "if importlib.util.find_spec('neptune') is None:\n",
        "  !pip install neptune-client\n",
        "\n",
        "if importlib.util.find_spec('pytorch_lightning') is None:\n",
        "  !pip install pytorch-lightning\n",
        "\n",
        "if importlib.util.find_spec('logzero') is None:\n",
        "  !pip install logzero \n",
        "\n",
        "if importlib.util.find_spec('tensorboardX') is None:\n",
        "  !pip install tensorboardX \n",
        "\n",
        "if importlib.util.find_spec('lineflow') is None:\n",
        "  !pip install lineflow\n",
        "\n",
        "if importlib.util.find_spec('optuna') is None:\n",
        "  !pip install optuna\n",
        "\n",
        "#if importlib.util.find_spec('gdown') is None:\n",
        "!pip install gdown==3.11.0\n",
        "  \n",
        "if importlib.util.find_spec('transformers') is None:\n",
        "  !pip install transformers \n",
        "  \n",
        "if importlib.util.find_spec('nlpaug') is None:\n",
        "  !pip install nlpaug \n",
        "\n",
        "import gdown\n",
        "import contextlib\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from typing import Dict\n",
        "import re\n",
        "import time\n",
        "\n",
        "import lineflow as lf\n",
        "import lineflow.datasets as lfds\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset, sampler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "if pkg_resources.parse_version(pl.__version__) < pkg_resources.parse_version(\"0.7.1\"):\n",
        "  raise RuntimeError(\"PyTorch Lightning>=0.7.1 is required for this code.\")\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.logging.neptune import NeptuneLogger \n",
        "from pytorch_lightning import Callback\n",
        "\n",
        "from gensim.utils import tokenize as gensim_tokenizer\n",
        "import gensim\n",
        "from gensim.models.fasttext import FastText as FT_gensim\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, RobertaTokenizer, RobertaModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup \n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "import toolz\n",
        "from toolz import partial, compose, juxt\n",
        "from nltk.corpus import stopwords\n",
        "import optuna\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.model.word_stats as nmw\n",
        "import nlpaug.flow as nafc\n",
        "from nlpaug.util import Action\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "!pip freeze > requirements.txt\n",
        "\n",
        "os.environ['PROJECT_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning')\n",
        "os.environ['DATASETS_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'datasets')\n",
        "os.environ['REQUIREMENTS_PATH'] = str(Path()/'requirements.txt')\n",
        "# Watch out for this path\n",
        "os.environ['RESULT_PATH'] = str(Path()/'result')\n",
        "os.environ['SPELLING_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'nlpaug'/'spelling_en.txt') \n",
        "os.environ['NLPAUG_PATH'] = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'nlpaug')\n",
        "artefacts_temp_dir = os.path.join(os.environ['PROJECT_PATH'], 'parametrized_nbs')\n",
        "\n",
        "neptune_api_token_key_file = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'neptune_api_token.txt')\n",
        "with open (neptune_api_token_key_file, 'r') as f:\n",
        "  os.environ['NEPTUNE_API_TOKEN'] = f.readlines()[0]\n",
        "\n",
        "if not os.path.exists(artefacts_temp_dir):\n",
        "  os.makedirs(artefacts_temp_dir)\n",
        "\n",
        "if not os.path.exists(os.environ['RESULT_PATH']):\n",
        "  os.makedirs(os.environ['RESULT_PATH'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: gdown==3.11.0 in /usr/local/lib/python3.6/dist-packages (3.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (1.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown==3.11.0) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown==3.11.0) (1.7.1)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu76gSLTpagD",
        "colab_type": "text"
      },
      "source": [
        "### Dataset statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3KMkjEDKPoE",
        "colab_type": "text"
      },
      "source": [
        "#### IMDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isBfou21d5f7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train = lfds.Imdb('train')\n",
        "# test = lfds.Imdb('test')\n",
        "# dataset = train + test\n",
        "# ds = dataset.map(lambda x: {'text': x[0], 'label':x[1], 'tokens_len': len(list(gensim_tokenizer(x[0])))})\n",
        "# df = pd.DataFrame(ds)\n",
        "# df['tokens_len'].describe()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IPyAeQnwtjk",
        "colab_type": "text"
      },
      "source": [
        "### Embeders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj41DiVYK6ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_ft_embeder():\n",
        "  my_file = Path(\"./cc.en.300.bin\")\n",
        "  # my_file = Path(\"./wiki-news-300d-1M.vec.zip\")\n",
        "  if not my_file.is_file():\n",
        "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "    # !https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "    !gunzip cc.en.300.bin.gz\n",
        "  print('Loading fastText model into memory, it can take a while...')\n",
        "  start = time.time()\n",
        "  # ft = gensim.models.fasttext.load_text_format()  # Original fasttext embeddings from https://fasttext.cc/\n",
        "  ft = gensim.models.FastText.load_fasttext_format(\"./cc.en.300.bin\")\n",
        "  # ft = FT_gensim.load(\"./cc.en.300.bin\")\n",
        "  # ft = fasttext.load_model(\"./cc.en.300.bin\")\n",
        "  end = time.time()\n",
        "  duration =  end - start\n",
        "  print(f'Loading took: {duration} s')\n",
        "  return ft\n",
        "\n",
        "def create_spacy_nlp_embeder():\n",
        "  my_file = Path(\"./crawl-300d-2M.vec.zip\")\n",
        "  if not my_file.is_file():\n",
        "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip \n",
        "  my_file = Path(\"./en_vectors_wiki_lg\")\n",
        "  if not my_file.is_file():\n",
        "    !python -m spacy init-model en /en_vectors_wiki_lg --vectors-loc crawl-300d-2M.vec.zip\n",
        "  print('Loading fastText vectors into spaCy model into memory, it can take a while...')\n",
        "  start = time.time()\n",
        "  nlp = spacy.load(\"/en_vectors_wiki_lg\")\n",
        "  end = time.time()\n",
        "  duration =  end - start\n",
        "  print(f'Loading took: {duration} s')\n",
        "  return nlp \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbMKpkCwWlXj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "3eb3a759-97ac-4afa-82a8-15524ea85265"
      },
      "source": [
        "nlp = create_spacy_nlp_embeder()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "✔ Successfully created model\n",
            "⠙ Reading vectors from crawl-300d-2M.vec.ziptcmalloc: large alloc 2400002048 bytes == 0x33ca000 @  0x7fdacaf1e001 0x7fdac8a82765 0x7fdac8ae6bb0 0x7fdac8ae8a4f 0x7fdac8b7f048 0x50a635 0x50cd96 0x509758 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x588d41 0x59fc4e 0x50d356 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x516345 0x50a2bf 0x50bfb4 0x507d64\n",
            "1999995it [03:45, 8881.78it/s]\n",
            "✔ Loaded vectors from crawl-300d-2M.vec.zip\n",
            "✔ Sucessfully compiled vocab\n",
            "2000191 entries, 1999995 vectors\n",
            "Loading fastText vectors into spaCy model into memory, it can take a while...\n",
            "Loading took: 26.922248363494873 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wggG_1T2gT_y",
        "colab_type": "text"
      },
      "source": [
        "### VAT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEXOJ_dmgWOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@contextlib.contextmanager\n",
        "def _disable_tracking_bn_stats(model):\n",
        "\n",
        "    def switch_attr(m):\n",
        "        if hasattr(m, 'track_running_stats'):\n",
        "            m.track_running_stats ^= True\n",
        "    model.apply(switch_attr)\n",
        "    yield\n",
        "    model.apply(switch_attr)\n",
        "\n",
        "class VATLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, xi=10.0, eps=1.0, ip=1):\n",
        "        \"\"\"VAT loss\n",
        "        :param xi: hyperparameter of VAT (default: 10.0)\n",
        "        :param eps: hyperparameter of VAT (default: 1.0)\n",
        "        :param ip: iteration times of computing adv noise (default: 1)\n",
        "        \"\"\"\n",
        "        super(VATLoss, self).__init__()\n",
        "\n",
        "        self.xi = xi\n",
        "        self.eps = eps\n",
        "        self.ip = ip\n",
        "\n",
        "    def forward(self, model, x):\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            pred = F.softmax(logits, dim=1)\n",
        "\n",
        "        # prepare random unit tensor\n",
        "\n",
        "        d = torch.rand(x.shape).sub(0.5).to(x.device)\n",
        "        def _l2_normalize(d):\n",
        "          d_reshaped = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))\n",
        "          d /= torch.norm(d_reshaped, dim=1, keepdim=True) + 1e-8\n",
        "          return d\n",
        "\n",
        "        d = _l2_normalize(d)\n",
        "        # self.forward as model it may crush?\n",
        "        with _disable_tracking_bn_stats(model):\n",
        "            # calc adversarial direction\n",
        "          for _ in range(self.ip):\n",
        "              d.requires_grad_()\n",
        "              pred_hat = model(x + self.xi * d)\n",
        "              logp_hat = F.log_softmax(pred_hat, dim=1)\n",
        "              adv_distance = F.kl_div(logp_hat, pred, reduction='batchmean')\n",
        "              adv_distance.backward(retain_graph=True)\n",
        "              d = _l2_normalize(d.grad)\n",
        "              model.zero_grad()\n",
        "\n",
        "        # calc LDS\n",
        "        r_adv = d * self.eps\n",
        "        pred_hat = model(x + r_adv)\n",
        "        logp_hat = F.log_softmax(pred_hat, dim=1)\n",
        "        lds = F.kl_div(logp_hat, pred, reduction='batchmean')\n",
        "\n",
        "        return lds "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfNoF2dG1yGl",
        "colab_type": "text"
      },
      "source": [
        "### FixMatch Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nIV2Bly11Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------------------    \n",
        "# augmentations on char level \n",
        "# ---------------------------------------------    \n",
        "def print_augmentations(func, text):\n",
        "  def wrapper():\n",
        "    print(\"Augmentation function: \", func.__name__)\n",
        "    print(\"Original: \")\n",
        "    print(text)\n",
        "    augmented_text = func(text)\n",
        "    print(\"Augmention result: \")\n",
        "    print(augmented_text)\n",
        "    return augmented_text\n",
        "\n",
        "  return wrapper() \n",
        "\n",
        "def substitute_character_by_keyboard_distance(text):\n",
        "  aug = nac.KeyboardAug()\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "\n",
        "def insert_character_randomly(text):\n",
        "  aug = nac.RandomCharAug(action=\"insert\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def substitute_character_randomly(text):\n",
        "  aug = nac.RandomCharAug(action=\"substitute\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def delete_char_randomly(text):\n",
        "  aug = nac.RandomCharAug(action=\"delete\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def swap_character_randomly(text):\n",
        "  aug = nac.RandomCharAug(action=\"swap\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "\n",
        "# ---------------------------------------------    \n",
        "# augmentations on word level \n",
        "# ---------------------------------------------    \n",
        "# models - spelling_en.txt\n",
        "# model_dir with fasttext or word2vec or glove \n",
        "# model dir with tf-idf\n",
        "\n",
        "# its consume to much RAM\n",
        "# def insert_word_randomly_by_word_embeddings_similarity(text):\n",
        "#   # model_type: word2vec, glove or fasttext\n",
        "#   aug = naw.WordEmbsAug(\n",
        "#       model_type='word2vec', model_path=os.environ('WORD2VEC_MODEL_PATH'),\n",
        "#       action=\"insert\")\n",
        "#   augmented_text = aug.augment(text)\n",
        "#   print(\"Original:\")\n",
        "#   print(text)\n",
        "#   print(\"Augmented Text:\")\n",
        "#   print(augmented_text)\n",
        "#   return augmented_text\n",
        "\n",
        "\n",
        "def insert_word_by_tf_idf_similarity(text):\n",
        "  aug = naw.TfIdfAug(\n",
        "      model_path=os.environ['NLPAUG_PATH'],\n",
        "      action=\"insert\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "\n",
        "def split_word_to_two_tokens_randomly(text):\n",
        "  aug = naw.SplitAug()\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def swap_word_randomly(text):\n",
        "  aug = naw.RandomWordAug(action=\"swap\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def substitute_word_by_antonym(text):\n",
        "  aug = naw.AntonymAug()\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def substitute_word_by_spelling_mistake_words_dictionary(text):\n",
        "  aug = naw.SpellingAug(os.environ['SPELLING_PATH'])\n",
        "  augmented_text = aug.augment(text, n=1)\n",
        "  return augmented_text \n",
        "\n",
        "\n",
        "def insert_word_by_contextual_word_embeddings(text):\n",
        "  aug = naw.ContextualWordEmbsAug(\n",
        "      model_path='bert-base-uncased', action=\"insert\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "\n",
        "\n",
        "def subtitute_word_by_contextual_word_embeddings(text):\n",
        "  aug = naw.ContextualWordEmbsAug(\n",
        "           model_path='bert-base-uncased', action=\"substitute\")\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "\n",
        "\n",
        "def substitute_word_by_WordNets_synonym(text):\n",
        "  aug = naw.SynonymAug(aug_src='wordnet')\n",
        "  augmented_text = aug.augment(text)\n",
        "  return augmented_text\n",
        "  \n",
        "def fixmatch_weak_augment_pool():\n",
        "    augs = [\n",
        "            substitute_character_by_keyboard_distance,\n",
        "            insert_character_randomly,\n",
        "            substitute_character_randomly,\n",
        "            delete_char_randomly,\n",
        "            swap_character_randomly,\n",
        "            # insert_word_randomly_by_word_embeddings_similarity,\n",
        "            insert_word_by_tf_idf_similarity,\n",
        "            split_word_to_two_tokens_randomly,\n",
        "            swap_word_randomly,\n",
        "            substitute_word_by_antonym,\n",
        "            substitute_word_by_spelling_mistake_words_dictionary,\n",
        "            insert_word_by_contextual_word_embeddings,\n",
        "            subtitute_word_by_contextual_word_embeddings,\n",
        "            substitute_word_by_WordNets_synonym,\n",
        "           ]\n",
        "\n",
        "    return augs\n",
        "\n",
        "# def fixmatch_strong_augment_pool():\n",
        "#     augs = [\n",
        "#             insert_word_by_contextual_word_embeddings,\n",
        "#             subtitute_word_by_contextual_word_embeddings,\n",
        "#             substitute_word_by_WordNets_synonym,\n",
        "#            ]\n",
        "\n",
        "#     return augs\n",
        "\n",
        "\n",
        "class WeakRandAugment(object):\n",
        "  def __init__(self, n, show=False):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    n (int): number of operations\n",
        "\n",
        "    \"\"\"\n",
        "    assert n >= 0\n",
        "    self.n = n\n",
        "    self.augment_pool = fixmatch_weak_augment_pool()\n",
        "    self.show=show\n",
        "\n",
        "  def __call__(self, text):\n",
        "    if self.n <= 0:\n",
        "      return text\n",
        "    ops = random.choices(self.augment_pool, k=self.n)\n",
        "    for op in ops:\n",
        "        if random.random() < 1.:\n",
        "          if self.show:\n",
        "            text = print_augmentations(op, text)\n",
        "          else:\n",
        "            text = op(text)\n",
        "    return text \n",
        "\n",
        "# not necessery\n",
        "# class StrongRandAugment(object):\n",
        "#   def __init__(self, n, show=False):\n",
        "#     assert n >= 1\n",
        "#     self.n = n\n",
        "#     self.augment_pool = fixmatch_strong_augment_pool()\n",
        "#     self.show= show\n",
        "\n",
        "#   def __call__(self, text):\n",
        "#     ops = random.choices(self.augment_pool, k=self.n)\n",
        "#     for op in ops:\n",
        "#       if random.random() < 1.:\n",
        "#         if self.show:\n",
        "#           text = print_augmentations(op, text)\n",
        "#         else:\n",
        "#           text = op(text)\n",
        "#     return text \n",
        "\n",
        "\n",
        "class TransformFix(object):\n",
        "  def __init__(self, n_weak=3, show=False):\n",
        "  # def __init__(self, n_weak=3, n_strong=2, show=False):\n",
        "    self.weak = WeakRandAugment(n=n_weak, show=show) \n",
        "    # self.strong = StrongRandAugment(n=n_strong, show=show)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    weak = self.weak(x)\n",
        "    # strong = self.strong(x)\n",
        "    return weak #, strong\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKpMf7Gz3CpR",
        "colab_type": "text"
      },
      "source": [
        "### Datasets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkdrQH7a3FKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoInOneDataset(Dataset):\n",
        "    def __init__(self, datasets):\n",
        "        self.datasets = datasets\n",
        "\n",
        "        self.map_indexes = [[] for _ in self.datasets]\n",
        "\n",
        "        self.min_length = min(len(d) for d in self.datasets)\n",
        "        self.max_length = max(len(d) for d in self.datasets)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return tuple(d[m[i]] for d, m in zip(self.datasets, self.map_indexes))\n",
        "\n",
        "    def construct_map_index(self):\n",
        "        def update_indices(original_indexes, target_len, max_len):\n",
        "            # map max_len to target_len (large to small)\n",
        "\n",
        "            # return: a list, which maps the range(max_len) to the valid index in the dataset\n",
        "            \n",
        "            original_indexes = original_indexes[max_len:] # remove used indices\n",
        "            fill_num = max_len - len(original_indexes)\n",
        "            batch = fill_num // target_len\n",
        "\n",
        "            if fill_num % target_len != 0:\n",
        "                # to let the fill_num + len(original_indexes) greater than max_len\n",
        "                batch += 1\n",
        "\n",
        "            additional_indexes = list(range(target_len)) * batch\n",
        "            random.shuffle(additional_indexes)\n",
        "\n",
        "            original_indexes += additional_indexes\n",
        "\n",
        "            assert len(original_indexes) >= max_len, \"the length of matcing indexes is too small\"\n",
        "\n",
        "            return original_indexes\n",
        "\n",
        "        self.map_indexes = [update_indices(m, len(d), self.max_length) \n",
        "            for m, d in zip(self.map_indexes, self.datasets)]\n",
        "\n",
        "    def __len__(self):\n",
        "        # will be called every epoch\n",
        "        self.construct_map_index()\n",
        "        return self.max_length\n",
        "        \n",
        "class SimpleTextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, x, y, transform=None):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # special dict convention for f: process_NLUHD \n",
        "        data_dict = { 'text': self.x[index], 'label': self.y[index]}\n",
        "        if self.transform is not None:\n",
        "          return self.transform(data_dict)\n",
        "        return tuple(data_dict.values()) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "class FixMatchAugmentedTextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, x, x_paraphrases, y,\n",
        "                 model_preprocessing = None,\n",
        "                 fix_match_augmentation = None, show=False):\n",
        "        self.x = x\n",
        "        self.x_paraphrases = x_paraphrases\n",
        "        self.y = y\n",
        "        self.model_preprocessing = model_preprocessing\n",
        "        self.fix_match_augmentation = fix_match_augmentation\n",
        "        self.show = show\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # special dict convention for f: process_NLUHD \n",
        "        if self.fix_match_augmentation is not None:\n",
        "          weak_augmented, strong_augmented = \\\n",
        "           self.fix_match_augmentation(self.x[index]), self.x_paraphrases[index] \n",
        "          if self.show:\n",
        "            def back_translation(text):\n",
        "              return strong_augmented \n",
        "            print_augmentations(back_translation, self.x_paraphrases[index])\n",
        "        \n",
        "\n",
        "        weak_aug_data_dict = { 'text': weak_augmented , 'label': self.y[index]}\n",
        "        strong_aug_data_dict = { 'text': strong_augmented , 'label': self.y[index]}\n",
        "\n",
        "        if self.model_preprocessing is not None:\n",
        "          return self.model_preprocessing(weak_aug_data_dict), self.model_preprocessing(strong_aug_data_dict)\n",
        "\n",
        "        return tuple(weak_aug_data_dict.values()), tuple(strong_aug_data_dict.values()) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA1ZSYrtFVK6",
        "colab_type": "text"
      },
      "source": [
        "### Datasets related processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsGvdNvB90p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_NLUHD(comment, nlp,  ner_abstract_tag: bool=True):\n",
        "  expression = r\"\\[.*?\\]\"\n",
        "  matches = []\n",
        "  for match in re.finditer(expression, comment.text):\n",
        "      start, end = match.span()\n",
        "      span = comment.char_span(start, end)\n",
        "      # This is a Span object or None if match doesn't map to valid token sequence\n",
        "      if span is not None:\n",
        "          # print(\"Found match:\", span.text)\n",
        "          if ner_abstract_tag:\n",
        "            expression_scd = r\"\\[.*?\\:\"\n",
        "          else:\n",
        "            expression_scd = r\"\\:.*?\\]\"\n",
        "\n",
        "          temp_doc = nlp(span.text)\n",
        "          scd_match = next(re.finditer(expression_scd, temp_doc.text))\n",
        "          start1, end1 = scd_match.span()\n",
        "          # print(start1, end1)\n",
        "          s1 = int(start1) + 1\n",
        "          e1 = int(end1) - 1\n",
        "          # print(type(e1))\n",
        "          replace_str = temp_doc.text[s1:e1].strip()\n",
        "          # scd_doc = temp_doc.char_span(start1 + 1, end1 - 2) \n",
        "          matches += [((start, end), replace_str)]\n",
        "\n",
        "  start_line = 0\n",
        "  new_comment = \"\"\n",
        "  for match in matches:\n",
        "    s = match[0][0]\n",
        "    e = match[0][1]\n",
        "    replace_word = match[1]\n",
        "    new_comment += comment.text[start_line:s] + replace_word \n",
        "    start_line = e\n",
        "  new_comment += comment.text[start_line:]\n",
        "  ret_val = nlp(new_comment)\n",
        "  return ret_val\n",
        "\n",
        "def preprocess_NLUHD(lowercase,\n",
        "                     remove_stopwords,\n",
        "                     with_ner_tags,\n",
        "                     nlp,\n",
        "                     label_encoder,\n",
        "                     sample):\n",
        "  \n",
        "  stops = stopwords.words(\"english\")\n",
        "  comment = sample['text']\n",
        "  if lowercase:\n",
        "      comment = comment.lower()\n",
        "  comment = nlp(comment)\n",
        "  if with_ner_tags is True:\n",
        "    comment = prepare_NLUHD(comment, ner_abstract_tag=True, nlp=nlp)\n",
        "  else:\n",
        "    comment = prepare_NLUHD(comment, ner_abstract_tag=False, nlp=nlp)\n",
        "  lemmatized = list()\n",
        "  if remove_stopwords:\n",
        "    for word in comment:\n",
        "        lemma = word.lemma_.strip()\n",
        "        if lemma:\n",
        "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
        "                lemmatized.append(lemma)\n",
        "    processed_text = \" \".join(lemmatized) \n",
        "  processed_text = comment.text\n",
        "  encoded_label = torch.tensor(int(label_encoder.transform([sample[\"label\"]])))\n",
        "  return {\"text\": processed_text,\n",
        "          \"label\": encoded_label}\n",
        "\n",
        "def preprocess_IMDB(label_encoder, sample: Dict):\n",
        "  pattern1 = re.compile(r'<.*?>')\n",
        "  # pattern2 = re.compile('[\\W_]+ ')\n",
        "  # text = pattern2.sub(' ', text)\n",
        "  text = pattern1.sub('', sample['text']).lower()\n",
        "  encoded_label = torch.tensor(int(label_encoder.transform([sample[\"label\"]])))\n",
        "  return {\"text\": text,\n",
        "          \"label\": encoded_label}\n",
        "          \n",
        "def preprocess_MR(label_encoder, sample: Dict):\n",
        "  pattern1 = re.compile(r'<.*?>')\n",
        "  # pattern2 = re.compile('[\\W_]+ ')\n",
        "  # text = pattern2.sub(' ', text)\n",
        "  text = pattern1.sub('', sample['text']).lower()\n",
        "  encoded_label = torch.tensor(int(label_encoder.transform([sample[\"label\"]])))\n",
        "  return {\"text\": text,\n",
        "          \"label\": encoded_label}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGCcDLd_-udN",
        "colab_type": "text"
      },
      "source": [
        "### Model related processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvkT6fi64bEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transformer_preprocessing(model_type: str,\n",
        "                              MAX_LEN: int,\n",
        "                              tokenizer: BertTokenizer,\n",
        "                              sample:Dict,) -> Dict:\n",
        "  \n",
        "    inputs = tokenizer.encode_plus(\n",
        "            sample[\"text\"],\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            )\n",
        "    # Output of `tokenizer.encode_plus` is a dictionary.\n",
        "    if model_type == 'roberta-base':\n",
        "      token_type_ids = [] \n",
        "    else:\n",
        "      input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
        "    # For BERT, we need `attention_mask` along with `input_ids` as input.\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "    # We are going to pad sequences.\n",
        "    padding_length = MAX_LEN - len(input_ids)\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "    input_ids = input_ids + ([pad_id] * padding_length)\n",
        "    attention_mask = attention_mask + ([0] * padding_length)\n",
        "    token_type_ids = token_type_ids + ([pad_id] * padding_length)\n",
        "\n",
        "    assert len(input_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(input_ids), MAX_LEN)\n",
        "    assert len(attention_mask) == MAX_LEN, \"Error with input length {} vs {}\".format(len(attention_mask), MAX_LEN)\n",
        "    assert len(token_type_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(token_type_ids), MAX_LEN)\n",
        "\n",
        "    # Just a python list to `torch.tensor`\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "    token_type_ids = torch.tensor(token_type_ids)\n",
        "\n",
        "    # What we return will one instance in batch which `LightningModule.train_step` receives.\n",
        "    return {\n",
        "            \"label\": sample['label'],\n",
        "            \"embedding\": {\n",
        "                          \"input_ids\": input_ids,\n",
        "                          \"attention_mask\": attention_mask,\n",
        "                          \"token_type_ids\": token_type_ids\n",
        "                         }\n",
        "            }\n",
        "\n",
        "def generate_embeddings(\n",
        "                         hparams,\n",
        "                         tokenizer,\n",
        "                         embeder,\n",
        "                         sample):\n",
        "\n",
        "  embedding = torch.Tensor([token.vector for token in embeder(sample[\"text\"])])#torch.tensor(embeder.wv[tokens])\n",
        "\n",
        "  if embedding.size()[0] >= hparams['max_sentence_len']:\n",
        "    embedding = torch.narrow(embedding, 0, 0, hparams['max_sentence_len'])\n",
        "  else:\n",
        "    padding_length = hparams['max_sentence_len'] - len(embedding)\n",
        "    padding_vectors = torch.zeros((padding_length, hparams['embed_dim']))\n",
        "    embedding = torch.cat((embedding, padding_vectors)) \n",
        "\n",
        "  return {'label': sample['label'],\n",
        "          'embedding': embedding}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_ppWCPTVnLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# datasets_path = str(Path()/'gdrive'/'My Drive'/'praca_magisterska'/'pytorch_lightning'/'datasets')\n",
        "# NLU_HD_path = os.path.join(datasets_path,'NLU-Data-Home-Domain-Annotated-All.csv')\n",
        "# print(NLU_HD_path)\n",
        "# # df = pd.read_csv(str(NLU_HD_path), delimiter=';')[['intent', 'answer_annotation']]\n",
        "# df = pd.read_csv(str(NLU_HD_path), delimiter=';')[['intent', 'answer_annotation', 'scenario']]\n",
        "# df['intent'] = df[['scenario', 'intent']].agg('-'.join, axis=1) \n",
        "# del df['scenario']\n",
        "# df = df[df['answer_annotation'].notna()]\n",
        "# df = df.rename(columns={\"answer_annotation\": \"text\"})\n",
        "# nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "# df['text'] = df['text'].apply(normalize,\n",
        "#                               lowercase=True,\n",
        "#                               remove_stopwords=False,\n",
        "#                               with_ner_tags=False,\n",
        "#                               nlp=nlp) \n",
        "# # df.to_csv(os.path.join(datasets_path,'NLU-Data-Home-Domain-preprocessed-without-ner_no-scenario.csv'))\n",
        "# df.to_csv(os.path.join(datasets_path,'NLU-Data-Home-Domain-preprocessed-without-ner.csv'))\n",
        "# df['intent'].value_counts().plot(kind=\"bar\", figsize= (21,20))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPAoAdd3Fm2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pd.set_option('display.max_rows', None)\n",
        "# df['intent'].value_counts()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr0K3ed_C6hV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df.head(10)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk1pzfVafNFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(df['intent'].unique())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7_n1xgQqHAq",
        "colab_type": "text"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH4XjCAXlRDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_global_metric(outputs, metric):\n",
        "    return sum([out[metric] for out in outputs]) / len(outputs)\n",
        "\n",
        "def create_metrics_dict(phase_type: str, loss, labels, labels_hat, max_probs) -> dict:\n",
        "\n",
        "    output_dict = {f'{phase_type}_loss': loss}\n",
        "    \n",
        "    metrics_keys = [\n",
        "                    f'{phase_type}_accuracy_error',\n",
        "                    f'{phase_type}_f1_error',\n",
        "                    f'{phase_type}_recall_error',\n",
        "                    f'{phase_type}_precision_error'\n",
        "                   ]\n",
        "\n",
        "    error_map = lambda x: 1-x\n",
        "\n",
        "    error_metrics_values = juxt(\n",
        "                                compose(error_map, partial(accuracy_score)),\n",
        "                                compose(error_map, partial(f1_score, **{'average':'micro'})),\n",
        "                                compose(error_map, partial(recall_score, **{'average':'micro'})),\n",
        "                                compose(error_map, partial(precision_score, **{'average':'micro'}))\n",
        "                               )(labels, labels_hat)\n",
        "\n",
        "\n",
        "    output_dict.update(dict(zip(metrics_keys, error_metrics_values)))\n",
        "\n",
        "    confident_keys = [\n",
        "                      f'{phase_type}_max_confident',\n",
        "                      f'{phase_type}_min_confident',\n",
        "                      f'{phase_type}_mean_confident',\n",
        "                      f'{phase_type}_std_confident'\n",
        "                     ]\n",
        "    confident_values = toolz.juxt(\n",
        "                                  np.max,\n",
        "                                  np.min, \n",
        "                                  np.mean,\n",
        "                                  np.std,\n",
        "                                 )(max_probs)\n",
        "\n",
        "    output_dict.update(dict(zip(confident_keys, confident_values)))\n",
        "\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def tfidf_tokenizer(text, token_pattern=r\"(?u)\\b\\w\\w+\\b\"):\n",
        "    token_pattern = re.compile(token_pattern)\n",
        "    return token_pattern.findall(text)\n",
        "\n",
        "def create_tfidf_model(df: pd.DataFrame):  \n",
        "  train_data = df['text'] \n",
        "  train_x = train_data.values\n",
        "    \n",
        "  train_x_tokens = [tfidf_tokenizer(x) for x in train_x]\n",
        "    \n",
        "  tfidf_model = nmw.TfIdf()\n",
        "  tfidf_model.train(train_x_tokens)\n",
        "  tfidf_model.save(os.environ['NLPAUG_PATH'])\n",
        "  os.environ['TFIDF_MODEL_PATH']  = os.path.join(os.environ['NLPAUG_PATH'], 'tfidfaug_w2tfidf.txt')\n",
        "  os.listdir(os.environ['NLPAUG_PATH'])\n",
        "\n",
        "class MetricsCallback(Callback):\n",
        "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.metrics = []\n",
        "\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        self.metrics.append(trainer.callback_metrics)\n",
        "\n",
        "# credits: https://github.com/galatolofederico/pytorch-balanced-batch/blob/master/sampler.py        \n",
        "class BalancedBatchSampler(torch.utils.data.sampler.Sampler):\n",
        "    def __init__(self, dataset, labels=None):\n",
        "        self.labels = labels\n",
        "        self.dataset = dict()\n",
        "        self.balanced_max = 0\n",
        "        # Save all the indices for all the classes\n",
        "        for idx in range(0, len(dataset)):\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label not in self.dataset:\n",
        "                self.dataset[label] = list()\n",
        "            self.dataset[label].append(idx)\n",
        "            self.balanced_max = len(self.dataset[label]) \\\n",
        "                if len(self.dataset[label]) > self.balanced_max else self.balanced_max\n",
        "        \n",
        "        # Oversample the classes with fewer elements than the max\n",
        "        for label in self.dataset:\n",
        "            while len(self.dataset[label]) < self.balanced_max:\n",
        "                self.dataset[label].append(random.choice(self.dataset[label]))\n",
        "        self.keys = list(self.dataset.keys())\n",
        "        self.currentkey = 0\n",
        "        self.indices = [-1]*len(self.keys)\n",
        "\n",
        "    def __iter__(self):\n",
        "        while self.indices[self.currentkey] < self.balanced_max - 1:\n",
        "            self.indices[self.currentkey] += 1\n",
        "            yield self.dataset[self.keys[self.currentkey]][self.indices[self.currentkey]]\n",
        "            self.currentkey = (self.currentkey + 1) % len(self.keys)\n",
        "        self.indices = [-1]*len(self.keys)\n",
        "    \n",
        "    def _get_label(self, dataset, idx, labels = None):\n",
        "        if self.labels is not None:\n",
        "            return self.labels[idx].item()\n",
        "        else:\n",
        "            raise Exception(\"You should pass the tensor of labels to the constructor as second argument\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.balanced_max*len(self.keys)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjha3vit9omj",
        "colab_type": "text"
      },
      "source": [
        "### Tests "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DWq_Q17ENpc",
        "colab_type": "text"
      },
      "source": [
        "#### Check if lf.cv.split splits in stratified fashion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e-bcGspDUnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ds = lf.CsvDataset(os.path.join(os.environ['DATASETS_PATH'], 'NLU-Data-Home-Domain-preprocessed-without-ner.csv'), header=True)\n",
        "# ds = ds.filter(lambda x: x['text'] is not None)\n",
        "\n",
        "# train, test = lf.cross_validation.split_dataset_random(ds, int(len(ds) * 0.8), seed=42)\n",
        "# df_train = pd.DataFrame(train)\n",
        "# df_test = pd.DataFrame(test)\n",
        "# # df_train['intent'].value_counts().plot(kind=\"bar\", figsize= (21,20))\n",
        "# df_test['intent'].value_counts().plot(kind=\"bar\", figsize= (21,20))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vhpT-QpSOrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pd.set_option('display.max_rows', None)\n",
        "# df_train.info()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfAShMKL9oH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
        "# uter = '[greetings : Hello] there what is [swear_word: fuck] up ?'\n",
        "# # uter =  '[greetings : Hello] there what is fuck up ?'\n",
        "# # uter = \"hello adfafdasfsaf sfsafdsafsa\"\n",
        "# comment = nlp(uter)\n",
        "# processed_comment = prepare_NLUHD(comment, nlp=nlp,ner_abstract_tag=False)\n",
        "# processed_comment"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isgALgNl86Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train = lfds.MsrParaphrase(\"train\")\n",
        "# print(len(train))\n",
        "# test = lfds.MsrParaphrase(\"test\")\n",
        "# train.first()\n",
        "# def nonefilter(dataset):\n",
        "#   filtered = []\n",
        "#   for x in dataset:\n",
        "#       if x[\"string1\"] is None:\n",
        "#           continue\n",
        "#       if x[\"string2\"] is None:\n",
        "#           continue\n",
        "#       filtered.append(x)\n",
        "#   return lf.Dataset(filtered)\n",
        "# # train = nonefilter(train)\n",
        "# train = train.filter(lambda x: x[\"string1\"] is not None and x[\"string2\"] is not None)\n",
        "# print(len(train))\n",
        "# train.take(3)\n",
        "# unique = list(['ale', 'beka'])\n",
        "# le = preprocessing.LabelEncoder().fit(unique)\n",
        "# torch.tensor(int(le.transform(['ale'])))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9cu0GG3qjHz",
        "colab_type": "text"
      },
      "source": [
        "#### Check IMDB preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOH9NEJHhTyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hparams = {'max_sentence_len': 200,\n",
        "#            'embed_dim': 300,\n",
        "#            'seed': 42,\n",
        "#            'train_test_split': 0.8}\n",
        "\n",
        "# train = lfds.Imdb('train')\n",
        "# test = lfds.Imdb('test')\n",
        "# ds = train + test\n",
        "# ds = ds.map(lambda x: {'text':x[0] , 'label': x[1]})\n",
        "# embeder = nlp \n",
        "# tokenizer_fun = lambda x: x#gensim_tokenizer\n",
        "# unique_labels = list(pd.DataFrame(ds).label.unique())\n",
        "# le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "\n",
        "# preprocessor = partial(\n",
        "#                       preprocess_IMDB,\n",
        "#                       hparams,\n",
        "#                       tokenizer_fun, \n",
        "#                       embeder,\n",
        "#                       le,\n",
        "#                       )\n",
        "# s = ds.first()\n",
        "# print(s)\n",
        "# preprocessor(s)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YDThFsqiBYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# train = lfds.Imdb('train')\n",
        "# test = lfds.Imdb('test')\n",
        "# ds = train + test\n",
        "# ds = ds.map(lambda x: {'text':x[0] , 'label': x[1]})\n",
        "# df = pd.DataFrame(ds)\n",
        "# pattern1 = re.compile(r'<.*?>')\n",
        "# # pattern2 = re.compile('[\\W_]+')\n",
        "# # text = pattern1.sub('', sample['text'])\n",
        "# # print('text after p1: ', text)\n",
        "# # text = text.replace('_', '').lower()\n",
        "# func = partial(pattern1.sub,\n",
        "#                '')\n",
        "# df['text'] = df['text'].apply(func)\n",
        "# # df\n",
        "# texts = list(df['text'])\n",
        "# texts"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ac1FViOh833",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ft.build_vocab(texts, update=True)\n",
        "# ft.train(new_sentences, total_examples=len(texts), epochs=10)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHn7X8Oc2IJN",
        "colab_type": "text"
      },
      "source": [
        "#### Test FixMatchTransform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIOUS2Fr2NLF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "64f26a83-2f91-492c-e136-98bd123a6103"
      },
      "source": [
        "tf = TransformFix(1,show=True)\n",
        "s = tf(\"what will be the weather like tomorrow, please tell me\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Augmentation function:  delete_char_randomly\n",
            "Original: \n",
            "what will be the weather like tomorrow, please tell me\n",
            "Augmention result: \n",
            "what will be the weather like tomorrow , please ell me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0Yw_j3YaDTg",
        "colab_type": "text"
      },
      "source": [
        "#### Test balanced sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-29zVc12aDe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not_none = lambda x: x[\"text\"] is not None \n",
        "# # ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "# ds = lfds.Imdb('train') + lfds.Imdb('test')\n",
        "# ds = ds.map(lambda x: {'text': x[0], 'label': x[1]})\n",
        "# df = pd.DataFrame(ds)\n",
        "\n",
        "# create_tfidf_model(df)\n",
        "\n",
        "# print(df.info(memory_usage=True))\n",
        "# unique_labels = list(df.label.unique())\n",
        "# print(f'unique_labels: {unique_labels}')\n",
        "# print(f'number_of_categories : {len(unique_labels)}')\n",
        "# le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "# train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "#                                                         int(len(ds) * 0.9),\n",
        "#                                                         seed=42)\n",
        "# dataset_preprocessor = partial(\n",
        "#                                 preprocess_IMDB,\n",
        "#                                 le,\n",
        "#                               )\n",
        "\n",
        "# tokenizer_dict = {\n",
        "#         \"bert-base-uncased\":\n",
        "#           BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
        "#                                         do_lower_case=True),\n",
        "#         \"roberta-base\":\n",
        "#           RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "#         }\n",
        "\n",
        "# model_arch_preprocessor = partial(\n",
        "#                                   transformer_preprocessing,\n",
        "#                                   'bert-base-uncased',\n",
        "#                                   156,\n",
        "#                                   tokenizer_dict['bert-base-uncased'],\n",
        "#                                   )\n",
        "\n",
        "\n",
        "# preprocessor = toolz.compose(\n",
        "#                             model_arch_preprocessor,\n",
        "#                             dataset_preprocessor,\n",
        "#                             )\n",
        "\n",
        "# train_df, test_df = pd.DataFrame(train), pd.DataFrame(test)\n",
        "# x_train, y_train = train_df['text'].values, train_df['label'].values\n",
        "# x_test, y_test = test_df['text'].values, test_df['label'].values\n",
        "\n",
        "# # split's training parameters  \n",
        "# num_classes = len(unique_labels)\n",
        "# label_per_class = 1000 // num_classes\n",
        "# valid_size = 1000 \n",
        "\n",
        "# labeled_idx = []\n",
        "# unlabeled_idx = []\n",
        "# val_idx = []\n",
        "\n",
        "# for label in unique_labels:\n",
        "#   idx = np.where(y_train == label)[0]\n",
        "#   np.random.shuffle(idx)\n",
        "#   labeled_idx.extend(idx[:label_per_class])\n",
        "#   val_idx.extend(idx[label_per_class: label_per_class + valid_size])\n",
        "#   unlabeled_idx.extend(idx[label_per_class + valid_size:])\n",
        "\n",
        "# x_labeled, y_labeled  = x_train[labeled_idx], y_train[labeled_idx]\n",
        "# x_unlabeled, y_unlabeled = x_train[unlabeled_idx], y_train[unlabeled_idx]\n",
        "# x_val, y_val = x_train[val_idx], y_train[val_idx]\n",
        "\n",
        "\n",
        "# train_labeled_dataset = SimpleTextDataset(x_labeled,\n",
        "#                                               y_labeled,\n",
        "#                                               transform=preprocessor)\n",
        "# train_unlabeled_dataset = SimpleTextDataset(x_unlabeled,\n",
        "#                                                   y_unlabeled,\n",
        "#                                                   transform=preprocessor)\n",
        "\n",
        "# val_dataset = SimpleTextDataset(x_val,\n",
        "#                                     y_val,\n",
        "#                                     transform=preprocessor)\n",
        "\n",
        "# test_dataset = SimpleTextDataset(x_test,\n",
        "#                                       y_test,\n",
        "#                                       transform=preprocessor)\n",
        "\n",
        "\n",
        "\n",
        "# train_labeled_dataloader = torch.utils.data.DataLoader(\n",
        "#                       train_labeled_dataset,\n",
        "#                       batch_size=64,\n",
        "#                       # shuffle=True,\n",
        "#                       num_workers=0,\n",
        "#                       sampler=BalancedBatchSampler(train_labeled_dataset, y_labeled),\n",
        "#                       )\n",
        "\n",
        "# train_labeled_dataloader_iterator = iter(train_labeled_dataloader)\n",
        "# train_unlabeled_dataloader = DataLoader(\n",
        "#                 train_unlabeled_dataset,\n",
        "#                 batch_size=64,\n",
        "#                 num_workers=8,\n",
        "#                 shuffle=True # without shuffle it want work cause it need to create map index before __get_item__ function\n",
        "#                 )\n",
        "\n",
        "# train_labeled_dataloader_iterator = iter(train_labeled_dataloader)\n",
        "# b = next(train_labeled_dataloader_iterator)\n",
        "# torch.sum(b['label'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMARxMrnTMzq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e12c4814-72e4-4f4f-e686-86e23168f2dc"
      },
      "source": [
        "epochs = 3\n",
        "size = 20\n",
        "features = 5\n",
        "classes_prob = torch.tensor([0.1, 0.4, 0.5])\n",
        "\n",
        "dataset_X = torch.randn(size, features)\n",
        "dataset_Y = torch.distributions.categorical.Categorical(classes_prob.repeat(size, 1)).sample()\n",
        "print(dataset_Y)\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(dataset_X, dataset_Y)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, sampler=BalancedBatchSampler(dataset, dataset_Y), batch_size=6)\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        print(\"epoch: %d labels: %s\\ninputs: %s\\n\" % (epoch, batch_y, batch_x))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 0, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 0, 2, 1, 1])\n",
            "epoch: 0 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[-2.5872e-01, -6.3209e-01, -1.9557e+00, -7.6241e-01,  1.0710e-01],\n",
            "        [ 4.0128e-02, -6.2834e-01,  1.3109e+00, -1.5027e+00, -4.8437e-01],\n",
            "        [-2.6874e-04, -1.2500e+00,  3.3375e-01, -3.6754e-01,  1.0761e+00],\n",
            "        [ 8.6034e-02, -3.6277e-01,  9.8601e-01, -1.5842e-01,  3.4082e-01],\n",
            "        [ 7.7236e-02, -1.0646e-01, -2.9264e-01,  1.5166e-01,  1.0298e+00],\n",
            "        [ 9.0350e-02,  7.4634e-01, -9.7614e-01, -1.7413e+00, -1.3752e+00]])\n",
            "\n",
            "epoch: 0 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[ 0.1856, -1.4871, -0.2116, -0.7871,  0.4984],\n",
            "        [ 0.0772, -0.1065, -0.2926,  0.1517,  1.0298],\n",
            "        [ 2.1551,  0.0606,  0.8192, -0.1208, -0.6993],\n",
            "        [ 0.2566,  0.0615, -1.8726, -1.4071, -1.7191],\n",
            "        [ 0.0401, -0.6283,  1.3109, -1.5027, -0.4844],\n",
            "        [ 0.9686, -2.4608, -1.9809, -0.5395, -0.9889]])\n",
            "\n",
            "epoch: 0 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[ 0.2320, -0.4875,  0.4838, -1.3808,  0.5196],\n",
            "        [ 0.0401, -0.6283,  1.3109, -1.5027, -0.4844],\n",
            "        [ 1.2289,  1.0100,  0.2830, -0.5100, -0.2068],\n",
            "        [-0.7283,  0.3840,  0.0847, -0.1402, -0.5769],\n",
            "        [ 0.0772, -0.1065, -0.2926,  0.1517,  1.0298],\n",
            "        [ 1.3374, -1.3955,  0.5698, -1.4430, -1.2282]])\n",
            "\n",
            "epoch: 0 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[-3.1691e-02,  2.3805e-01, -7.3191e-01, -5.0672e-01, -6.8842e-01],\n",
            "        [ 4.0128e-02, -6.2834e-01,  1.3109e+00, -1.5027e+00, -4.8437e-01],\n",
            "        [ 2.9629e-01, -7.9887e-01,  8.8771e-01, -9.1522e-01,  3.3370e-01],\n",
            "        [ 9.6667e-01,  1.2182e+00,  4.4425e-04,  6.1105e-02, -6.3881e-01],\n",
            "        [ 4.0128e-02, -6.2834e-01,  1.3109e+00, -1.5027e+00, -4.8437e-01],\n",
            "        [-2.3675e-01,  7.8952e-01,  2.5441e-01, -2.6438e-01, -5.3156e-01]])\n",
            "\n",
            "epoch: 0 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[-0.7283,  0.3840,  0.0847, -0.1402, -0.5769],\n",
            "        [ 0.0401, -0.6283,  1.3109, -1.5027, -0.4844],\n",
            "        [ 0.1605,  0.6133,  1.1698, -0.4725, -0.0591],\n",
            "        [-0.0317,  0.2381, -0.7319, -0.5067, -0.6884],\n",
            "        [ 0.0401, -0.6283,  1.3109, -1.5027, -0.4844],\n",
            "        [ 0.7153,  0.8499, -1.8948, -2.3312,  2.1806]])\n",
            "\n",
            "epoch: 1 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[-2.5872e-01, -6.3209e-01, -1.9557e+00, -7.6241e-01,  1.0710e-01],\n",
            "        [ 4.0128e-02, -6.2834e-01,  1.3109e+00, -1.5027e+00, -4.8437e-01],\n",
            "        [-2.6874e-04, -1.2500e+00,  3.3375e-01, -3.6754e-01,  1.0761e+00],\n",
            "        [ 8.6034e-02, -3.6277e-01,  9.8601e-01, -1.5842e-01,  3.4082e-01],\n",
            "        [ 7.7236e-02, -1.0646e-01, -2.9264e-01,  1.5166e-01,  1.0298e+00],\n",
            "        [ 9.0350e-02,  7.4634e-01, -9.7614e-01, -1.7413e+00, -1.3752e+00]])\n",
            "\n",
            "epoch: 1 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[ 0.1856, -1.4871, -0.2116, -0.7871,  0.4984],\n",
            "        [ 0.0772, -0.1065, -0.2926,  0.1517,  1.0298],\n",
            "        [ 2.1551,  0.0606,  0.8192, -0.1208, -0.6993],\n",
            "        [ 0.2566,  0.0615, -1.8726, -1.4071, -1.7191],\n",
            "        [ 0.0401, -0.6283,  1.3109, -1.5027, -0.4844],\n",
            "        [ 0.9686, -2.4608, -1.9809, -0.5395, -0.9889]])\n",
            "\n",
            "epoch: 1 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[ 0.2320, -0.4875,  0.4838, -1.3808,  0.5196],\n",
            "        [ 0.0401, -0.6283,  1.3109, -1.5027, -0.4844],\n",
            "        [ 1.2289,  1.0100,  0.2830, -0.5100, -0.2068],\n",
            "        [-0.7283,  0.3840,  0.0847, -0.1402, -0.5769],\n",
            "        [ 0.0772, -0.1065, -0.2926,  0.1517,  1.0298],\n",
            "        [ 1.3374, -1.3955,  0.5698, -1.4430, -1.2282]])\n",
            "\n",
            "epoch: 1 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[-3.1691e-02,  2.3805e-01, -7.3191e-01, -5.0672e-01, -6.8842e-01],\n",
            "        [ 4.0128e-02, -6.2834e-01,  1.3109e+00, -1.5027e+00, -4.8437e-01],\n",
            "        [ 2.9629e-01, -7.9887e-01,  8.8771e-01, -9.1522e-01,  3.3370e-01],\n",
            "        [ 9.6667e-01,  1.2182e+00,  4.4425e-04,  6.1105e-02, -6.3881e-01],\n",
            "        [ 4.0128e-02, -6.2834e-01,  1.3109e+00, -1.5027e+00, -4.8437e-01],\n",
            "        [-2.3675e-01,  7.8952e-01,  2.5441e-01, -2.6438e-01, -5.3156e-01]])\n",
            "\n",
            "epoch: 1 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[-0.7283,  0.3840,  0.0847, -0.1402, -0.5769],\n",
            "        [ 0.0401, -0.6283,  1.3109, -1.5027, -0.4844],\n",
            "        [ 0.1605,  0.6133,  1.1698, -0.4725, -0.0591],\n",
            "        [-0.0317,  0.2381, -0.7319, -0.5067, -0.6884],\n",
            "        [ 0.0401, -0.6283,  1.3109, -1.5027, -0.4844],\n",
            "        [ 0.7153,  0.8499, -1.8948, -2.3312,  2.1806]])\n",
            "\n",
            "epoch: 2 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[-2.5872e-01, -6.3209e-01, -1.9557e+00, -7.6241e-01,  1.0710e-01],\n",
            "        [ 4.0128e-02, -6.2834e-01,  1.3109e+00, -1.5027e+00, -4.8437e-01],\n",
            "        [-2.6874e-04, -1.2500e+00,  3.3375e-01, -3.6754e-01,  1.0761e+00],\n",
            "        [ 8.6034e-02, -3.6277e-01,  9.8601e-01, -1.5842e-01,  3.4082e-01],\n",
            "        [ 7.7236e-02, -1.0646e-01, -2.9264e-01,  1.5166e-01,  1.0298e+00],\n",
            "        [ 9.0350e-02,  7.4634e-01, -9.7614e-01, -1.7413e+00, -1.3752e+00]])\n",
            "\n",
            "epoch: 2 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[ 0.1856, -1.4871, -0.2116, -0.7871,  0.4984],\n",
            "        [ 0.0772, -0.1065, -0.2926,  0.1517,  1.0298],\n",
            "        [ 2.1551,  0.0606,  0.8192, -0.1208, -0.6993],\n",
            "        [ 0.2566,  0.0615, -1.8726, -1.4071, -1.7191],\n",
            "        [ 0.0401, -0.6283,  1.3109, -1.5027, -0.4844],\n",
            "        [ 0.9686, -2.4608, -1.9809, -0.5395, -0.9889]])\n",
            "\n",
            "epoch: 2 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[ 0.2320, -0.4875,  0.4838, -1.3808,  0.5196],\n",
            "        [ 0.0401, -0.6283,  1.3109, -1.5027, -0.4844],\n",
            "        [ 1.2289,  1.0100,  0.2830, -0.5100, -0.2068],\n",
            "        [-0.7283,  0.3840,  0.0847, -0.1402, -0.5769],\n",
            "        [ 0.0772, -0.1065, -0.2926,  0.1517,  1.0298],\n",
            "        [ 1.3374, -1.3955,  0.5698, -1.4430, -1.2282]])\n",
            "\n",
            "epoch: 2 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[-3.1691e-02,  2.3805e-01, -7.3191e-01, -5.0672e-01, -6.8842e-01],\n",
            "        [ 4.0128e-02, -6.2834e-01,  1.3109e+00, -1.5027e+00, -4.8437e-01],\n",
            "        [ 2.9629e-01, -7.9887e-01,  8.8771e-01, -9.1522e-01,  3.3370e-01],\n",
            "        [ 9.6667e-01,  1.2182e+00,  4.4425e-04,  6.1105e-02, -6.3881e-01],\n",
            "        [ 4.0128e-02, -6.2834e-01,  1.3109e+00, -1.5027e+00, -4.8437e-01],\n",
            "        [-2.3675e-01,  7.8952e-01,  2.5441e-01, -2.6438e-01, -5.3156e-01]])\n",
            "\n",
            "epoch: 2 labels: tensor([1, 0, 2, 1, 0, 2])\n",
            "inputs: tensor([[-0.7283,  0.3840,  0.0847, -0.1402, -0.5769],\n",
            "        [ 0.0401, -0.6283,  1.3109, -1.5027, -0.4844],\n",
            "        [ 0.1605,  0.6133,  1.1698, -0.4725, -0.0591],\n",
            "        [-0.0317,  0.2381, -0.7319, -0.5067, -0.6884],\n",
            "        [ 0.0401, -0.6283,  1.3109, -1.5027, -0.4844],\n",
            "        [ 0.7153,  0.8499, -1.8948, -2.3312,  2.1806]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVxlMTAX9hUH",
        "colab_type": "text"
      },
      "source": [
        "#### Test embeder if exist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfDzoDan9gkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokens = nlp('213213dsf ma kota')\n",
        "# tokenlist = [token.vector for token in tokens]\n",
        "# t = torch.Tensor(tokenlist)\n",
        "# t"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShfV1EwDrX97",
        "colab_type": "text"
      },
      "source": [
        "### Composable Framework "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9CPFT7ormCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LitComposableFramework(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, hparams):\n",
        "\n",
        "    super().__init__()\n",
        "    self.hparams = hparams\n",
        "    self.num_classes = hparams['num_classes']\n",
        "    self.total_iterations = 0 \n",
        "    self.loss_fct = getattr(nn, hparams['loss_function'])()\n",
        "\n",
        "\n",
        "    if self.hparams['model_arch'] == \"Convolution\":\n",
        "\n",
        "      self.embeder_dict = {\n",
        "                          'fastText': (create_ft_embeder, gensim_tokenizer),\n",
        "                          'spaCy':(create_spacy_nlp_embeder, lambda x: x)\n",
        "                          }\n",
        "      embeder, self.tokenizer_fun = self.embeder_dict[hparams['embeder_type']]\n",
        "      self.embeder = nlp #embeder#nlp # hardcoded\n",
        "      self.D = hparams['embed_dim']\n",
        "      self.Ci = hparams['Ci'] \n",
        "      self.Co = hparams['kernel_num']\n",
        "      self.Ks = list(map(int, hparams['kernel_sizes'].split(','))) # (3,4,5)\n",
        "      self.convs1 = nn.ModuleList([nn.Conv2d(self.Ci, self.Co, (K, self.D)) for K in self.Ks])\n",
        "      self.dropout = nn.Dropout(hparams['dropout'])\n",
        "      self.fc1 = nn.Linear(len(self.Ks) * self.Co, self.num_classes) \n",
        "\n",
        "\n",
        "    elif self.hparams['model_arch'] == \"Transformer\":\n",
        "      self.model_class_dict = {\n",
        "            \"bert-base-uncased\": BertModel,\n",
        "            \"roberta-base\": RobertaModel\n",
        "            }\n",
        "              \n",
        "      self.tokenizer_dict = {\n",
        "              \"bert-base-uncased\":\n",
        "                BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
        "                                              do_lower_case=True),\n",
        "              \"roberta-base\":\n",
        "                RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "              }\n",
        "  \n",
        "      self.model = self.model_class_dict[self.hparams['model_type']].from_pretrained(self.hparams['model_type'],\n",
        "                                                                output_attentions=True)\n",
        "      self.encoder_features =  self.model.config.hidden_size \n",
        "      print(self.encoder_features)\n",
        "      self.num_classes = self.hparams['num_classes']\n",
        "      self.classification_head = nn.Sequential(\n",
        "            nn.Linear(self.encoder_features, self.encoder_features * 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.encoder_features * 2, self.encoder_features),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.encoder_features, self.num_classes),\n",
        "        )\n",
        "      \n",
        "    else:\n",
        "      raise ValueError('Wrong model architecture type: {} \\n Possible datasets: Transformer, Convolution'.format(self.hparams['model_arch']))\n",
        "\n",
        "  def forward(self, x, embeddings_only=False):\n",
        "\n",
        "      if self.hparams['model_arch'] == \"Convolution\":\n",
        "\n",
        "        if embeddings_only == True:\n",
        "          logits = x\n",
        "\n",
        "        else:\n",
        "          x = x.unsqueeze(self.Ci)  # (N, Ci, W, D)\n",
        "          x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
        "          x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "          x = torch.cat(x, 1)\n",
        "          x = self.dropout(x)  # (N, len(Ks)*Co)\n",
        "          logits = self.fc1(x)  # (N, C)\n",
        "\n",
        "      elif self.hparams['model_arch'] == \"Transformer\":\n",
        "        h, _, _ = self.model(x['input_ids'],\n",
        "                                attention_mask=x['attention_mask'],\n",
        "                                token_type_ids=x['token_type_ids'] if self.hparams['model_type'] != \"roberta-base\" else None)\n",
        "        h_cls = h[:, 0]\n",
        "\n",
        "        if embeddings_only == True:\n",
        "          return h_cls\n",
        "\n",
        "        logits = self.classification_head(h_cls)\n",
        "\n",
        "      else:\n",
        "        raise ValueError('Wrong model architecture type: {} \\n Possible datasets: Transformer, Convolution'.format(self.hparams['model_arch']))\n",
        "\n",
        "      return logits\n",
        "\n",
        "\n",
        "  def prepare_data(self):\n",
        "\n",
        "    if self.hparams['dataset'] == 'NLUHD':\n",
        "\n",
        "      not_none = lambda x: x[\"text\"] is not None \n",
        "      ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "      unique_labels = list(pd.DataFrame(ds).intent.unique())\n",
        "      self.le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      print(f\"Unique labels: {unique_labels}\")\n",
        "      print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                            int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                            seed=self.hparams['seed'])\n",
        "\n",
        "      nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "      dataset_preprocessor = partial(preprocess_NLUHD,\n",
        "                                     lowercase=True,\n",
        "                                     remove_stopwords=True,\n",
        "                                     with_ner_tags=False,\n",
        "                                     nlp=nlp,\n",
        "                                     label_encoder=le,\n",
        "      )\n",
        "\n",
        "    elif self.hparams['dataset'] == 'MR':\n",
        "\n",
        "      not_none = lambda x: x[\"text\"] is not None \n",
        "      ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "      df = pd.DataFrame(ds)\n",
        "\n",
        "      if self.hparams['training_method'] == \"FixMatch\":\n",
        "        create_tfidf_model(df)\n",
        "\n",
        "      unique_labels = list(df.intent.unique())\n",
        "      self.le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      print(f\"Unique labels: {unique_labels}\")\n",
        "      print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                            int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                            seed=self.hparams['seed'])\n",
        "\n",
        "      dataset_preprocessor = partial(\n",
        "                                     preprocess_MR,\n",
        "                                     le,\n",
        "                                    )\n",
        "      \n",
        "    elif self.hparams['dataset'] == 'IMDB':\n",
        "      not_none = lambda x: x[\"text\"] is not None \n",
        "      ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "      # ds = lfds.Imdb('train') + lfds.Imdb('test')\n",
        "      # ds = ds.map(lambda x: {'text': x[0], 'label': x[1]})\n",
        "      df = pd.DataFrame(ds)\n",
        "\n",
        "      if self.hparams['training_method'] == \"FixMatch\":\n",
        "        create_tfidf_model(df)\n",
        "\n",
        "      print(df.info(memory_usage=True))\n",
        "      unique_labels = list(df.label.unique())\n",
        "      print(f'unique_labels: {unique_labels}')\n",
        "      print(f'number_of_categories : {len(unique_labels)}')\n",
        "      self.le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                             int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                             seed=self.hparams['seed'])\n",
        "      dataset_preprocessor = partial(\n",
        "                                     preprocess_IMDB,\n",
        "                                     self.le,\n",
        "                                    )\n",
        "                            \n",
        "    else:\n",
        "      raise ValueError('Wrong dataset name : {} \\n Possible datasets: IMDB, NLUHD'.format(self.hparams['dataset']))\n",
        "\n",
        "    if self.hparams['model_arch'] == 'Transformer':\n",
        "\n",
        "      model_arch_preprocessor = partial(\n",
        "                                        transformer_preprocessing,\n",
        "                                        self.hparams['model_type'],\n",
        "                                        self.hparams['max_sentence_len'],\n",
        "                                        self.tokenizer_dict[self.hparams['model_type']],\n",
        "                                       )\n",
        "      \n",
        "    elif self.hparams['model_arch'] == 'Convolution':\n",
        "\n",
        "      model_arch_preprocessor = partial(\n",
        "                                        generate_embeddings,\n",
        "                                        self.hparams,\n",
        "                                        self.tokenizer_fun,\n",
        "                                        self.embeder,\n",
        "                                       )\n",
        "    \n",
        "    else:\n",
        "      raise ValueError('Wrong model architecture type: {} \\n Possible architectures: Convolution, Transformer'.format(self.hparams['model_arch']))\n",
        "      \n",
        "    preprocessor = toolz.compose(\n",
        "                                 model_arch_preprocessor,\n",
        "                                 dataset_preprocessor,\n",
        "                                 )\n",
        "\n",
        "    train_df, test_df = pd.DataFrame(train), pd.DataFrame(test)\n",
        "    x_train, y_train = train_df['text'].values, train_df['label'].values\n",
        "    self.x_test, self.y_test = test_df['text'].values, test_df['label'].values\n",
        "\n",
        "    # split's training parameters  \n",
        "    num_classes = len(unique_labels)\n",
        "    label_per_class = self.hparams['n_labeled'] // num_classes\n",
        "    valid_size = self.hparams['valid_size_per_class']\n",
        "\n",
        "    labeled_idx = []\n",
        "    unlabeled_idx = []\n",
        "    val_idx = []\n",
        "    \n",
        "    for label in unique_labels:\n",
        "        idx = np.where(y_train == label)[0]\n",
        "        np.random.shuffle(idx)\n",
        "        labeled_idx.extend(idx[:label_per_class])\n",
        "        val_idx.extend(idx[label_per_class: label_per_class + valid_size])\n",
        "        unlabeled_idx.extend(idx[label_per_class + valid_size:])\n",
        "\n",
        "    self.x_labeled, self.y_labeled  = x_train[labeled_idx], y_train[labeled_idx]\n",
        "    self.x_unlabeled, self.y_unlabeled = x_train[unlabeled_idx], y_train[unlabeled_idx]\n",
        "    self.x_val, self.y_val = x_train[val_idx], y_train[val_idx]\n",
        "\n",
        "    \n",
        "    self._train_labeled_dataset = SimpleTextDataset(self.x_labeled,\n",
        "                                                    self.y_labeled,\n",
        "                                                    transform=preprocessor)\n",
        "    if self.hparams['training_method'] == 'FixMatch':\n",
        "      x_unlabeled_paraphrases = train_df['paraphrases'].values\n",
        "      self._train_unlabeled_dataset = \\\n",
        "         FixMatchAugmentedTextDataset(self.x_unlabeled,\n",
        "                                      x_unlabeled_paraphrases[unlabeled_idx],\n",
        "                                      self.y_unlabeled,\n",
        "                                      model_preprocessing=preprocessor,\n",
        "                                      show=self.hparams['show_augmentation'],\n",
        "                                      fix_match_augmentation=TransformFix(\n",
        "                                        n_weak=self.hparams['n_weak'],\n",
        "                                        # n_strong=self.hparams['n_strong'],\n",
        "                                        show=self.hparams['show_augmentation']),\n",
        "                                     )\n",
        "    else:\n",
        "      self._train_unlabeled_dataset = SimpleTextDataset(self.x_unlabeled,\n",
        "                                                        self.y_unlabeled,\n",
        "                                                        transform=preprocessor)\n",
        "    \n",
        "    self._val_dataset = SimpleTextDataset(self.x_val,\n",
        "                                          self.y_val,\n",
        "                                          transform=preprocessor)\n",
        "    \n",
        "    self._test_dataset = SimpleTextDataset(self.x_test,\n",
        "                                           self.y_test,\n",
        "                                           transform=preprocessor)\n",
        "    \n",
        "    self.total_iterations = len(self._train_unlabeled_dataset) // self.hparams['unl_batch_size'] \n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "\n",
        "    encoded_label = torch.tensor(self.le.transform(self.y_labeled))\n",
        "    self._train_labeled_dataloader = torch.utils.data.DataLoader(\n",
        "                            self._train_labeled_dataset,\n",
        "                            batch_size=self.hparams['l_batch_size'],\n",
        "                            # shuffle=True,\n",
        "                            num_workers=0,\n",
        "                            sampler=BalancedBatchSampler(self._train_labeled_dataset,\n",
        "                                                         encoded_label),\n",
        "                            )\n",
        "      \n",
        "    self.train_labeled_dataloader_iterator = iter(self._train_labeled_dataloader)\n",
        "    return DataLoader(\n",
        "                      self._train_unlabeled_dataset,\n",
        "                      batch_size=self.hparams['unl_batch_size'],\n",
        "                      num_workers=8,\n",
        "                      shuffle=True # without shuffle it want work cause it need to create map index before __get_item__ function\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "                      self._val_dataset,\n",
        "                      batch_size=self.hparams['val_batch_size'],\n",
        "                      num_workers=8\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(\n",
        "                      self._test_dataset,\n",
        "                      batch_size=self.hparams['test_batch_size'],\n",
        "                      num_workers=8\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "\n",
        "    if self.hparams['model_arch'] == 'Transformer':\n",
        "      param_optimizer = list(self.model.named_parameters())\n",
        "      no_decay = [\"bias\", 'LayerNorm.weight']\n",
        "      optimizer_grouped_parameters = [\n",
        "              {\n",
        "                  \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                  \"weight_decay_rate\": 0.01\n",
        "                  },\n",
        "              {\n",
        "                  \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                  \"weight_decay_rate\": 0.0\n",
        "                  },\n",
        "              ]\n",
        "      print('total_iterations: ', self.total_iterations)\n",
        "      optimizer = AdamW(\n",
        "                        optimizer_grouped_parameters,\n",
        "                        lr=self.hparams['lr'],\n",
        "                      )\n",
        "      scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                  self.hparams['warmup_steps'],\n",
        "                                                  self.total_iterations,\n",
        "                                                  -1)\n",
        "\n",
        "    elif self.hparams['model_arch'] == 'Convolution':\n",
        "      optimizer_dict = {'Adam': torch.optim.Adam(self.parameters(),\n",
        "                                                lr=self.hparams['lr']),\n",
        "                      }\n",
        "  \n",
        "      optimizer = optimizer_dict[self.hparams['optimizer_type']]\n",
        "                    \n",
        "      scheduler_dict = {'ExponentialLR': torch.optim.lr_scheduler.ExponentialLR(\n",
        "                                                          optimizer=optimizer,\n",
        "                                                          gamma=0.9),\n",
        "                        'StepLR': torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                              self.hparams['decay_step_size'],\n",
        "                                              self.hparams['decay_gamma']),\n",
        "                        'None': None,\n",
        "                        \n",
        "                      }\n",
        "\n",
        "      scheduler = scheduler_dict[self.hparams['sheduler_type']]\n",
        "      \n",
        "      if scheduler == None:\n",
        "        return [optimizer]\n",
        "\n",
        "    return [optimizer], [scheduler]\n",
        "\n",
        "  def supervised(self, texts, labels, logs):\n",
        "\n",
        "    logits = self.forward(texts)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "    return logits, loss, logs\n",
        "\n",
        "  def vat(self, l_texts, labels, unl_texts, logs):\n",
        "\n",
        "    vat_loss = VATLoss(xi=self.hparams['xi'],\n",
        "                        eps=self.hparams['eps'],\n",
        "                        ip=self.hparams['ip'])\n",
        "    \n",
        "    logits = self.forward(l_texts)\n",
        "    supervised_loss = self.loss_fct(logits, labels)\n",
        "    logs.update({'supervised_loss': supervised_loss})\n",
        "\n",
        "    embeddings = self.forward(unl_texts, embeddings_only=True)\n",
        "    lds = vat_loss(self, embeddings)\n",
        "    logs.update({'lds_loss': lds})\n",
        "\n",
        "    loss = supervised_loss + self.hparams['alpha'] * lds  \n",
        "\n",
        "    return logits, loss, logs\n",
        "\n",
        "  def fixmatch(self, l_embeddings, labels, unl_embeddings, logs):\n",
        "\n",
        "    unl_w_dict, unl_s_dict = unl_embeddings\n",
        "    unl_w, unl_s = unl_w_dict['embedding'], unl_s_dict['embedding']\n",
        "\n",
        "    if self.hparams['model_arch'] == 'transformer':\n",
        "\n",
        "      stacked_inputs_ids = torch.cat((l_embeddings[\"input_ids\"],\n",
        "                                      unl_w['input_ids'],\n",
        "                                      unl_s['input_ids']))\n",
        "      \n",
        "      stacked_attention_mask = torch.cat((l_embeddings[\"attention_mask\"],\n",
        "                                          unl_w['attention_mask'],\n",
        "                                          unl_s['attention_mask']))\n",
        "      \n",
        "      stacked_token_type_ids = torch.cat((l_embeddings[\"token_type_ids\"],\n",
        "                                          unl_w['token_type_ids'],\n",
        "                                          unl_s['token_type_ids']))\n",
        "    \n",
        "      x = {\n",
        "           \"inputs_ids\": stacked_inputs_ids,\n",
        "           \"attention_mask\": stacked_attention_mask,\n",
        "           \"token_type_ids\": stacked_token_type_ids\n",
        "          }\n",
        "    else:\n",
        "      x = torch.cat((l_embeddings, unl_w, unl_s))\n",
        "\n",
        "    logits = self.forward(x)\n",
        "    batch_size = self.hparams['l_batch_size'] \n",
        "    logits_x = logits[:batch_size]\n",
        "    logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
        "\n",
        "    del logits\n",
        "     \n",
        "    Lx = F.cross_entropy(logits_x, labels, reduction='mean')\n",
        "\n",
        "    pseudo_label = torch.softmax(logits_u_w.detach_(), dim=-1)\n",
        "    max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
        "\n",
        "    confident_keys = ['unl_max_confident', 'unl_min_confident',\n",
        "                      'unl_mean_confident', 'unl_std_confident']\n",
        "    confident_values = juxt(toch.max, torch.min, torch.mean, torch.std) \\\n",
        "                           (max_probs)\n",
        "    confident_dict = dict(zip(confident_keys, confident_values))\n",
        "    logs.update(confident_dict)\n",
        "\n",
        "    mask = max_probs.ge(self.hparams['threshold']).float()\n",
        "    Lu = (F.cross_entropy(logits_u_s, targets_u,\n",
        "                          reduction='none') * mask).mean()\n",
        "\n",
        "    loss = Lx + self.hparams['lambda_u'] * Lu\n",
        "    logs.update({\"Lu\": Lu, 'Lx': Lx})\n",
        "\n",
        "    return logits_x, loss, logs\n",
        "\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "\n",
        "    try:\n",
        "      labeled = next(self.train_labeled_dataloader_iterator)\n",
        "\n",
        "    except StopIteration:\n",
        "      self.train_labeled_dataloader_iterator = iter(self._train_labeled_dataloader)\n",
        "      labeled = next(self.train_labeled_dataloader_iterator)\n",
        "\n",
        "    l_texts, labels = labeled['embedding'], labeled['label']\n",
        "\n",
        "    if self.hparams['model_arch'] == 'transformer':\n",
        "      gpu_l_texts = toolz.dicttoolz.valmap(torch.Tensor.cuda, l_texts)\n",
        "    else:\n",
        "      gpu_l_texts = l_texts.cuda() \n",
        "\n",
        "    gpu_labels = labels.cuda()  \n",
        "    logs = dict()\n",
        "    \n",
        "    if self.hparams['training_method'] == 'VAT':\n",
        "      unl_embeddings = batch['embedding'] # dont need label\n",
        "      logits, loss, logs = self.vat(gpu_l_texts, gpu_labels, unl_embeddings, logs)\n",
        "    elif self.hparams['training_method'] == \"FixMatch\":\n",
        "      logits, loss, logs = self.fixmatch(gpu_l_texts, gpu_labels, batch, logs)\n",
        "    elif self.hparams['training_method'] == \"Supervised\":\n",
        "      logits, loss, logs = self.supervised(gpu_l_texts, gpu_labels, logs)\n",
        "    else:\n",
        "      raise ValueError('Wrong training method type: {} \\n Possible methods : VAT, FixMatch, Supervised'.format(self.hparams['model_arch']))\n",
        "\n",
        "    probabilities = torch.softmax(logits.detach_(), dim=-1)\n",
        "    gpu_max_probs, gpu_labels_hat = torch.max(probabilities, dim=-1)\n",
        "\n",
        "    # labels = gpu_labels.cpu()\n",
        "    # labels_hat = gpu_labels_hat.cpu()\n",
        "    # max_probs = gpu_max_probs.cpu()\n",
        "\n",
        "    cpu_labels = gpu_labels.detach().cpu().numpy()\n",
        "    cpu_labels_hat = gpu_labels_hat.detach().cpu().numpy()\n",
        "    cpu_max_probs = gpu_max_probs.detach().cpu().numpy()\n",
        "    cpu_loss = loss.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "    metrics_dict = create_metrics_dict('train',\n",
        "                                       cpu_loss,\n",
        "                                       cpu_labels,\n",
        "                                       cpu_labels_hat,\n",
        "                                       cpu_max_probs,\n",
        "                                      )\n",
        "\n",
        "    logs.update(metrics_dict)    \n",
        "\n",
        "    return {'loss': loss,\n",
        "            'log': logs}\n",
        "\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "\n",
        "    embeddings = batch['embedding']\n",
        "    labels = batch['label']\n",
        "  \n",
        "    logits = self.forward(embeddings)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "    max_probs, labels_hat = torch.max(probabilities, dim=-1)\n",
        "\n",
        "    cpu_labels = labels.detach().cpu().numpy()\n",
        "    cpu_labels_hat = labels_hat.detach().cpu().numpy()\n",
        "    cpu_max_probs = max_probs.detach().cpu().numpy()\n",
        "    cpu_loss = loss.detach().cpu().numpy()\n",
        "\n",
        "    output = create_metrics_dict('val',\n",
        "                                 cpu_loss,\n",
        "                                 cpu_labels,\n",
        "                                 cpu_labels_hat,\n",
        "                                 cpu_max_probs)\n",
        "    return output\n",
        "\n",
        "  def validation_epoch_end(self, outputs):\n",
        "    print(outputs)\n",
        "    tqdm_dict = toolz.merge_with(np.mean, outputs)\n",
        "    return {\n",
        "            \"progress_bar\": tqdm_dict,\n",
        "            \"log\": tqdm_dict,\n",
        "           }\n",
        "\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "\n",
        "    embeddings = batch[\"embedding\"]\n",
        "    labels = batch[\"label\"]\n",
        "  \n",
        "    logits = self.forward(embeddings)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "    max_probs, labels_hat = torch.max(probabilities, dim=-1)\n",
        "\n",
        "    # labels = labels.cpu()\n",
        "    # labels_hat = labels_hat.cpu()\n",
        "    cpu_labels = labels.detach().cpu().numpy()\n",
        "    cpu_labels_hat = labels_hat.detach().cpu().numpy()\n",
        "    cpu_max_probs = max_probs.detach().cpu().numpy()\n",
        "    cpu_loss = loss.detach().cpu().numpy()\n",
        "    \n",
        "    output = create_metrics_dict('test',\n",
        "                                 cpu_loss,\n",
        "                                 cpu_labels,\n",
        "                                 cpulabels_hat,\n",
        "                                 cpu_max_probs)\n",
        "    return output \n",
        "\n",
        "\n",
        "  def test_epoch_end(self, outputs):\n",
        "    tqdm_dict = toolz.merge_with(np.mean, outputs)\n",
        "\n",
        "    return {\n",
        "            \"progress_bar\": tqdm_dict,\n",
        "            \"log\": tqdm_dict,\n",
        "            # \"test_loss\": test_loss,\n",
        "            # 'test_acc_error': test_acc,\n",
        "            # 'test_f1_error': test_f1\n",
        "           }"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aYX9ZJVIoEs",
        "colab_type": "text"
      },
      "source": [
        "### Configure experiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC9zE6cKkbPi",
        "colab_type": "text"
      },
      "source": [
        "##### Choose dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEwfu7pNkDJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = 'IMDB'\n",
        "\n",
        "if dataset == 'IMDB':\n",
        "  dataset_path = str(Path()/\n",
        "                        'gdrive'/\n",
        "                        'My Drive'/\n",
        "                        'praca_magisterska'/\n",
        "                        'pytorch_lightning'/\n",
        "                        'datasets'/\n",
        "                        'imdb_with_bt.csv')\n",
        "  \n",
        "  hparams = {\"dataset\": 'IMDB',\n",
        "            \"num_classes\": 2,\n",
        "            \"dataset_path\": dataset_path,\n",
        "            }\n",
        "\n",
        "\n",
        "elif dataset == 'MR':\n",
        "  dataset_path = str(Path()/\n",
        "                        'gdrive'/\n",
        "                        'My Drive'/\n",
        "                        'praca_magisterska'/\n",
        "                        'pytorch_lightning'/\n",
        "                        'datasets'/\n",
        "                        'mr_with_bt.csv')\n",
        "  \n",
        "  hparams = {\"dataset\": 'MR',\n",
        "            \"num_classes\": 2,\n",
        "            \"dataset_path\": dataset_path,\n",
        "            }\n",
        "\n",
        "elif dataset == 'NLUHD':\n",
        "  dataset_path = str(Path()/\n",
        "                    'gdrive'/\n",
        "                    'My Drive'/\n",
        "                    'praca_magisterska'/\n",
        "                    'pytorch_lightning'/\n",
        "                    'datasets'/\n",
        "                    'NLU-Data-Home-Domain-preprocessed-without-ner.csv')\n",
        "  \n",
        "  hparams = {\"dataset\": 'NLUHD',\n",
        "            \"num_classes\": 68, # ???\n",
        "            \"dataset_path\": dataset_path}\n",
        "            \n",
        "else:\n",
        "  raise ValueError('Wrong dataset name : {} \\\n",
        "   \\n Possible datasets: IMDB, MR, NLUHD'.format(hparams['dataset']))"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uf3R7m6kd6z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##### Choose traning method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxhbdqoNkeGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_method = 'Supervised' \n",
        "\n",
        "if training_method == 'VAT':\n",
        "  hparams.update({\"training_method\": 'VAT',\n",
        "                  'xi':1e-05,\n",
        "                  'eps':4.5, \n",
        "                  'ip':1, \n",
        "                  'alpha':1} \n",
        "                )\n",
        "elif training_method == 'FixMatch':\n",
        "  hparams.update({\"training_method\": 'FixMatch',\n",
        "                  'mu': 4, \n",
        "                  'threshold': 0.67,\n",
        "                  'lambda_u': 1,\n",
        "                  'n_weak': 0,\n",
        "                  'show_augmentation': False,\n",
        "                })\n",
        "\n",
        "elif training_method == 'Supervised':\n",
        "  hparams.update({\"training_method\": 'Supervised'})\n",
        "\n",
        "else:\n",
        "  raise ValueError('Wrong training method type: {} \\n\" \\\n",
        "       Possible methods : VAT, FixMatch, Supervised'.format(self.hparams['model_arch']))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FownSMlUmIzo",
        "colab_type": "text"
      },
      "source": [
        "#### Choose model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH4lZGqlmJAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_arch = 'Convolution'\n",
        "\n",
        "if model_arch == \"Transformer\":\n",
        "  hparams.update({\n",
        "      'model_arch': \"Transformer\",\n",
        "      'model_type': 'bert-base-uncased',\n",
        "      'max_sentence_len': 156,\n",
        "\n",
        "      'xi': 0.00001,\n",
        "      'lr': 5e-05,\n",
        "      'weight_decay': 0.01,\n",
        "      'adam_eps': 1e-06,\n",
        "      'warmup_steps': 150,\n",
        "      })\n",
        "\n",
        "elif model_arch == 'Convolution':\n",
        "  hparams.update({\n",
        "           'model_arch': \"Convolution\",\n",
        "           'max_sentence_len': 400,\n",
        "           'embeder_type': \"fastText\",\n",
        "           'embed_dim': 300,\n",
        "           'Ci': 1,\n",
        "           'kernel_num': 100,\n",
        "           'kernel_sizes': '3,4,5',\n",
        "           'dropout':0.5, \n",
        "\n",
        "           'optimizer_type': 'Adam',\n",
        "           'sheduler_type': 'None',\n",
        "           'lr': 1e-05,\n",
        "           'decay_step_size': 10000,\n",
        "           'decay_gamma':0.5,\n",
        "\n",
        "  })\n",
        "else:\n",
        "  raise ValueError('Wrong model architecture type: {} \\\n",
        "   \\n Possible datasets: Transformer, Convolution'.format(self.hparams['model_arch']))\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st6sL-cOwym2",
        "colab_type": "text"
      },
      "source": [
        "#### Choose training params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8YWsCR-jGM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " hparams.update({\n",
        "           'train_test_split': 0.9,\n",
        "           'seed': 42,\n",
        "           'l_batch_size': 16,\n",
        "           'unl_batch_size': 32,\n",
        "           'val_batch_size': 16,\n",
        "           'test_batch_size': 16,\n",
        "           'n_labeled': 160, # number of labeled samples  # must be a multiplication of l_batch_size\n",
        "           'valid_size_per_class': 1000, # 68 class => n_val_samples = 68 * 10 \n",
        "\n",
        "           'loss_function':'CrossEntropyLoss',\n",
        "\n",
        "           'test_run': False,\n",
        "           'max_epochs': 5,\n",
        "           'min_epochs': 1,\n",
        "           'val_check_interval': 0.2, \n",
        "           'patience': 15, # early stopping callback parameter\n",
        "     \n",
        " }) \n",
        "\n",
        " \n",
        "tags = []\n",
        "tags.append(hparams['dataset'])\n",
        "tags.append(hparams['model_arch'])\n",
        "tags.append(hparams['training_method'])\n",
        "tags.append(hparams['n_labeled'])\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4n0cvZNw5Ec",
        "colab_type": "text"
      },
      "source": [
        "### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "velwnCrgNCeb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "170da35b4ba0440b85fb8ec2e8e349e6",
            "433ad3af0ab941ee8ee02368de5a7552",
            "f8e91bd0bc424aa398045c0d520acf2d",
            "fe8b40a7e9634535ac12608b2b996343",
            "ccfefc0b38a244f4bc3e576c26ed8bca",
            "4714c7a6d319430fb4745b0c5a41499b",
            "7cefa9d3958e4d528b99ec9e1dc699f2",
            "fbeb2198b30c40f6a5aa9d6b6022b7f7",
            "d8352f8eb7414187a7d8db2a71d518a3",
            "2bc61761751b45c1a6a1bc89d742d2a6",
            "2f161cd484b94a28942ebc744d26080c",
            "7dfac5c9374c4d79ae82c47142799a34",
            "d8b94fa865ca4b6db07b10061756e20a",
            "644c2909cc1b42528d226db1e3f37e50",
            "e1cd1c693b8e43adace46c11b38e6a11",
            "e038e0ab872b47c4b8664d640a6a7aaa"
          ]
        },
        "outputId": "6febc4af-752f-40b4-d602-f2c80736a90e"
      },
      "source": [
        "set_seed(hparams['seed'])\n",
        "experiment_name = training_method + \"-\" + model_arch + \"-\" + dataset \n",
        "neptune_logger = NeptuneLogger(\n",
        "                               project_name=\"m1f1/lightning-exps-text\",\n",
        "                               close_after_fit=False,\n",
        "                               experiment_name=experiment_name,  # Optional,\n",
        "                               params=hparams, # Optional,\n",
        "                               tags=tags # Optional,\n",
        "                              )\n",
        "# callbacks\n",
        "early_stop_callback = EarlyStopping(\n",
        "                        monitor=\"val_loss\",\n",
        "                        min_delta=0.0,\n",
        "                        patience=hparams['patience'],\n",
        "                        verbose=True,\n",
        "                        mode='min'\n",
        "                      )\n",
        "# Path(\"./checkpoints\").mkdir(parents=True, exist_ok=True)\n",
        "# model_checkpoint = pl.callbacks.ModelCheckpoint(filepath='./checkpoints') # check if it overwrite last checkpoint\n",
        "\n",
        "# training and evaluating model\n",
        "trainer = pl.Trainer(\n",
        "                gpus=1,\n",
        "                logger=neptune_logger,\n",
        "                # checkpoint_callback=model_checkpoint,\n",
        "                # early_stop_callback=early_stop_callback,\n",
        "                val_check_interval=hparams['val_check_interval'],\n",
        "                # distributed_backend=hparams['distributed_backend'],\n",
        "                # default_root_dir=\"./test_run_logs\",\n",
        "                fast_dev_run=hparams['test_run'],\n",
        "              #  train_percent_check=0.001,\n",
        "              #  val_percent_check=0.001,\n",
        "                min_epochs=hparams['min_epochs'],\n",
        "                max_epochs=hparams['max_epochs'],\n",
        "          )\n",
        "\n",
        "model = LitComposableFramework(hparams)\n",
        "[print(f'{k}: {v}') for k, v in hparams.items()]\n",
        "    \n",
        "trainer.fit(model)\n",
        "trainer.test(model)\n",
        "\n",
        "# list_of_files = glob.glob('./checkpoints/*') # * means all if need specific format then *.csv\n",
        "# latest_file = max(list_of_files, key=os.path.getctime)\n",
        "# print(latest_file)\n",
        "# model = LitBERT.load_from_checkpoint(latest_file))\n",
        "\n",
        "\n",
        "# neptune_logger.experiment.log_artifact('./checkpoints')\n",
        "neptune_logger.experiment.log_artifact(os.environ['REQUIREMENTS_PATH'])\n",
        "neptune_logger.experiment.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://ui.neptune.ai/m1f1/lightning-exps-text/e/LIG-435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "NeptuneLogger will work in online mode\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset: IMDB\n",
            "num_classes: 2\n",
            "dataset_path: gdrive/My Drive/praca_magisterska/pytorch_lightning/datasets/imdb_with_bt.csv\n",
            "training_method: Supervised\n",
            "model_arch: Convolution\n",
            "max_sentence_len: 400\n",
            "embeder_type: fastText\n",
            "embed_dim: 300\n",
            "Ci: 1\n",
            "kernel_num: 100\n",
            "kernel_sizes: 3,4,5\n",
            "dropout: 0.5\n",
            "optimizer_type: Adam\n",
            "sheduler_type: None\n",
            "lr: 1e-05\n",
            "decay_step_size: 10000\n",
            "decay_gamma: 0.5\n",
            "train_test_split: 0.9\n",
            "seed: 42\n",
            "l_batch_size: 16\n",
            "unl_batch_size: 32\n",
            "val_batch_size: 16\n",
            "test_batch_size: 16\n",
            "n_labeled: 160\n",
            "valid_size_per_class: 1000\n",
            "loss_function: CrossEntropyLoss\n",
            "test_run: False\n",
            "max_epochs: 5\n",
            "min_epochs: 1\n",
            "val_check_interval: 0.2\n",
            "patience: 15\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 7 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0                50000 non-null  object\n",
            " 1   Unnamed: 0   50000 non-null  object\n",
            " 2   text         50000 non-null  object\n",
            " 3   label        50000 non-null  object\n",
            " 4   paraphrases  50000 non-null  object\n",
            " 5   len          50000 non-null  object\n",
            " 6   diff         50000 non-null  object\n",
            "dtypes: object(7)\n",
            "memory usage: 2.7+ MB\n",
            "None\n",
            "unique_labels: ['1', '0']\n",
            "number_of_categories : 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name     | Type             | Params\n",
            "----------------------------------------------\n",
            "0 | loss_fct | CrossEntropyLoss | 0     \n",
            "1 | convs1   | ModuleList       | 360 K \n",
            "2 | dropout  | Dropout          | 0     \n",
            "3 | fc1      | Linear           | 602   \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "170da35b4ba0440b85fb8ec2e8e349e6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0.51681966 0.52837086 0.521098   0.5054208  0.5136667  0.5129293\n",
            " 0.5307895  0.5159343  0.5160339  0.5195004  0.52951926 0.51148915\n",
            " 0.5088242  0.5164045  0.52276564 0.52515346]\n",
            "[0.5065034  0.5101935  0.5086547  0.52314097 0.5288122  0.51813877\n",
            " 0.50882    0.5218796  0.51915866 0.5084242  0.52092344 0.5037257\n",
            " 0.50361073 0.53259784 0.5047859  0.5014244 ]\n",
            "[{'val_loss': array(0.7307947, dtype=float32), 'val_accuracy_error': 1.0, 'val_f1_error': 1.0, 'val_recall_error': 1.0, 'val_precision_error': 1.0, 'val_max_confident': 0.5054208, 'val_mean_confident': 0.51842, 'val_std_confident': 0.007194235}, {'val_loss': array(0.7192166, dtype=float32), 'val_accuracy_error': 0.9375, 'val_f1_error': 0.9375, 'val_recall_error': 0.9375, 'val_precision_error': 0.9375, 'val_max_confident': 0.5014244, 'val_mean_confident': 0.51379967, 'val_std_confident': 0.009406402}]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8352f8eb7414187a7d8db2a71d518a3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0.57986426 0.5593345  0.5869603  0.59654903 0.5115523  0.50765955\n",
            " 0.5645896  0.53449345 0.5434154  0.5755346  0.52624506 0.5642527\n",
            " 0.59095436 0.5718846  0.5116768  0.50444865]\n",
            "[0.5063235  0.55315346 0.554887   0.52162373 0.6025103  0.5758537\n",
            " 0.5339208  0.5604749  0.52631736 0.52780575 0.6040487  0.5239374\n",
            " 0.53904015 0.54052806 0.5221222  0.54657334]\n",
            "[0.5651682  0.6096974  0.5617973  0.50220066 0.5357763  0.56713444\n",
            " 0.51178324 0.554804   0.55727494 0.5575144  0.5021167  0.5612232\n",
            " 0.5193276  0.5638247  0.64754444 0.5037537 ]\n",
            "[0.5207696  0.5205628  0.5360607  0.5705912  0.6647919  0.5379899\n",
            " 0.6053092  0.57782376 0.52906036 0.5294417  0.5949738  0.59360915\n",
            " 0.5425824  0.5991967  0.5072616  0.50997317]\n",
            "[0.54241186 0.51929927 0.56088096 0.58454037 0.5185317  0.50128794\n",
            " 0.5225171  0.50537145 0.6000496  0.5008094  0.60182524 0.5022888\n",
            " 0.5010738  0.5545968  0.5806575  0.51294386]\n",
            "[0.5675295  0.5523187  0.54552823 0.6083802  0.53799176 0.55499613\n",
            " 0.5132429  0.5560201  0.51807314 0.50568646 0.57189816 0.5089488\n",
            " 0.5607564  0.53270864 0.53756785 0.57808924]\n",
            "[0.54429775 0.52650595 0.5794734  0.5259819  0.52833086 0.5190589\n",
            " 0.5814442  0.51548165 0.67413425 0.548833   0.5510946  0.50103277\n",
            " 0.592301   0.50058955 0.6604382  0.51895905]\n",
            "[0.5312691  0.59128404 0.520612   0.5523561  0.59083384 0.5873625\n",
            " 0.5661858  0.5125904  0.5408194  0.5023576  0.55353636 0.5535437\n",
            " 0.5207641  0.51997167 0.5357942  0.54775566]\n",
            "[0.5129374  0.56274515 0.52780133 0.646615   0.55777603 0.5812479\n",
            " 0.52014726 0.59730476 0.5259464  0.5113954  0.5010783  0.538922\n",
            " 0.5424099  0.51652634 0.50569737 0.58334976]\n",
            "[0.5051533  0.5800822  0.51217145 0.5808831  0.53332615 0.6178906\n",
            " 0.50635004 0.5560243  0.5286113  0.5114107  0.61179733 0.51850176\n",
            " 0.5291667  0.5961645  0.54530984 0.5043429 ]\n",
            "[0.5252051  0.5912585  0.5084029  0.5152565  0.5236915  0.5532588\n",
            " 0.5155471  0.5814284  0.52947426 0.653647   0.5134116  0.5096223\n",
            " 0.5232904  0.55138874 0.5536266  0.55130017]\n",
            "[0.53485143 0.64376813 0.57582545 0.59110993 0.50022405 0.51717246\n",
            " 0.5301824  0.5464128  0.5287668  0.5160255  0.5275191  0.56151986\n",
            " 0.515344   0.5730726  0.55752385 0.5204712 ]\n",
            "[0.5325867  0.539938   0.5110049  0.56341916 0.5474651  0.5677784\n",
            " 0.54683656 0.54245406 0.5211064  0.5375002  0.5307418  0.50649226\n",
            " 0.6329202  0.5976379  0.5037368  0.50659597]\n",
            "[0.5710388  0.52989906 0.5889896  0.552168   0.55850327 0.593014\n",
            " 0.52274024 0.5364202  0.535399   0.5470724  0.61742437 0.60448706\n",
            " 0.58070713 0.5441177  0.56859356 0.5113733 ]\n",
            "[0.58025205 0.5849713  0.53412986 0.50168794 0.58984697 0.55081856\n",
            " 0.5118355  0.5411436  0.5407323  0.5598534  0.5147801  0.62870675\n",
            " 0.5654364  0.5320048  0.5605091  0.5022119 ]\n",
            "[0.5475889  0.5220555  0.54612607 0.5301717  0.52245456 0.6181597\n",
            " 0.6189801  0.5690926  0.61075824 0.5594905  0.57998604 0.5467952\n",
            " 0.5153916  0.5661312  0.52393967 0.5814781 ]\n",
            "[0.56811094 0.53798807 0.55545104 0.5428008  0.5461448  0.57954115\n",
            " 0.5356453  0.5804962  0.55182797 0.6400085  0.5096805  0.54799074\n",
            " 0.5363536  0.50006086 0.6007319  0.54543877]\n",
            "[0.5954813  0.5479044  0.55423707 0.5372195  0.5434174  0.5399724\n",
            " 0.58344    0.52026    0.50656813 0.5483328  0.50314313 0.53565466\n",
            " 0.5014506  0.52646846 0.5804104  0.5895647 ]\n",
            "[0.57547855 0.5441091  0.5407956  0.57517344 0.5194435  0.55794835\n",
            " 0.54060656 0.5382399  0.58218044 0.5268537  0.5055432  0.5288801\n",
            " 0.5155803  0.5119355  0.5448402  0.60544187]\n",
            "[0.5660777  0.5424047  0.54434466 0.5583011  0.6024736  0.52051526\n",
            " 0.51955193 0.5419893  0.605439   0.50587755 0.6489533  0.52899367\n",
            " 0.51976174 0.5588559  0.5662796  0.5693401 ]\n",
            "[0.5639736  0.5860916  0.5396661  0.52633715 0.50059915 0.51931953\n",
            " 0.53901416 0.565353   0.5268284  0.6229767  0.525421   0.5954532\n",
            " 0.55598897 0.55601245 0.5572262  0.5890955 ]\n",
            "[0.5217635  0.59509546 0.5109908  0.55306983 0.5104228  0.51910377\n",
            " 0.53792787 0.5098234  0.5269972  0.56756765 0.5030421  0.55606115\n",
            " 0.51459694 0.5603196  0.5014563  0.5165151 ]\n",
            "[0.50458753 0.5535434  0.537839   0.5340383  0.5216217  0.57599205\n",
            " 0.55311465 0.55379677 0.53602386 0.63125056 0.5345923  0.5930185\n",
            " 0.5085122  0.5185525  0.50086474 0.51605815]\n",
            "[0.5870688  0.56844234 0.52500474 0.512854   0.50160503 0.52576923\n",
            " 0.5276465  0.5093456  0.53294253 0.5041083  0.5200378  0.53523093\n",
            " 0.52459896 0.5079294  0.50442046 0.5551749 ]\n",
            "[0.52216285 0.5177543  0.5752058  0.57412624 0.5256569  0.50908947\n",
            " 0.56138295 0.5098216  0.52452743 0.54742736 0.5248671  0.50518805\n",
            " 0.51173997 0.50089437 0.51228005 0.59399813]\n",
            "[0.6148128  0.5509078  0.5126045  0.50302684 0.5231142  0.5249362\n",
            " 0.60212827 0.5667918  0.5731688  0.5630687  0.6253215  0.5893422\n",
            " 0.62782156 0.50543875 0.52002305 0.66672575]\n",
            "[0.551976   0.5712329  0.5324562  0.6283195  0.5157685  0.5466009\n",
            " 0.5046978  0.50325984 0.5646414  0.535776   0.53960115 0.54317605\n",
            " 0.5015971  0.5472667  0.5133834  0.58559763]\n",
            "[0.53339386 0.618326   0.53774506 0.5426329  0.527939   0.5518252\n",
            " 0.52113664 0.54935265 0.53805804 0.51329225 0.6006788  0.5106654\n",
            " 0.57360923 0.51461846 0.5489142  0.5892028 ]\n",
            "[0.5161698  0.5065674  0.62167674 0.52025276 0.53576463 0.5246712\n",
            " 0.5515107  0.58442396 0.5016842  0.64979523 0.51555794 0.50663173\n",
            " 0.5230503  0.58888143 0.5075177  0.5558204 ]\n",
            "[0.5121709  0.5148123  0.5637488  0.5585237  0.6384582  0.6026388\n",
            " 0.5048447  0.5072063  0.5032152  0.6394768  0.52720326 0.5455354\n",
            " 0.5582052  0.5452215  0.52082723 0.5253265 ]\n",
            "[0.5010985  0.5264238  0.5437824  0.5312137  0.5012267  0.502789\n",
            " 0.5564702  0.5721039  0.5365119  0.5585571  0.56933284 0.5312128\n",
            " 0.59208137 0.5403226  0.5504134  0.5385258 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8Loooc9iAcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = pd.read_csv(str(Path(os.environ['DATASETS_PATH'])/'imdb_with_bt.csv'))\n",
        "# df\n",
        "# df = df.rename(columns={'intent': 'label', 'bt': 'paraphrases'})\n",
        "# df.to_csv(str(Path(os.environ['DATASETS_PATH'])/'imdb_with_bt.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfaD-N-RFmpH",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhTVU9r32eyI",
        "colab_type": "text"
      },
      "source": [
        "#### System spec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucbrH_0LsQ8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LitYKConv_HPO(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, hparams, trial):\n",
        "\n",
        "    super().__init__()\n",
        "    self.hparams = hparams\n",
        "\n",
        "    if self.hparams['with_VAT']:\n",
        "      xi_interval = list(map(float, hparams['xi'].split(',')))\n",
        "      eps_interval = list(map(float, hparams['eps'].split(',')))\n",
        "      ip_interval = list(map(int, hparams['ip'].split(',')))\n",
        "      alpha_interval = list(map(float, hparams['alpha'].split(',')))\n",
        "\n",
        "      self.xi = trial.suggest_uniform('xi', *xi_interval)\n",
        "      print('xi: ', self.xi)\n",
        "      self.eps = trial.suggest_uniform('eps', *eps_interval)\n",
        "      print('eps: ', self.eps)\n",
        "      self.ip = trial.suggest_int('ip', *ip_interval)\n",
        "      print('ip: ', self.ip)\n",
        "      self.alpha = trial.suggest_uniform('alpha', *alpha_interval)\n",
        "      print('alpha: ', self.alpha)\n",
        "\n",
        "    lr_interval = list(map(float, hparams['lr'].split(',')))\n",
        "    print(lr_interval)\n",
        "    kernel_num_interval = list(map(int, hparams['kernel_num'].split(','))) \n",
        "    print(kernel_num_interval)\n",
        "    dropout_interval = list(map(float, hparams['dropout'].split(','))) \n",
        "    print(dropout_interval)\n",
        "\n",
        "    self.lr = trial.suggest_loguniform('learning_rate', *lr_interval)\n",
        "    print('lr: ', self.lr)\n",
        "    self.Co = trial.suggest_int('kernel_num', *kernel_num_interval) #hparams['kernel_num']\n",
        "    print('kernel_num: ', self.Co)\n",
        "    dropout = trial.suggest_uniform('dropout', *dropout_interval)\n",
        "    print('dropout: ', dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    self.embeder_dict = {\n",
        "                         'fastText': (create_ft_embeder, gensim_tokenizer),\n",
        "                         'spaCy':(create_spacy_nlp_embeder, lambda x: x)\n",
        "                        }\n",
        "    embeder, self.tokenizer_fun = self.embeder_dict[hparams['embeder_type']]\n",
        "    self.embeder = nlp #embeder()\n",
        "    self.D = hparams['embed_dim']\n",
        "    self.Ci = hparams['Ci'] \n",
        "    \n",
        "    self.loss_fct = getattr(nn, hparams['loss_function'])()\n",
        "    self.num_classes = hparams['num_classes']\n",
        "\n",
        "    self.Ks = list(map(int, hparams['kernel_sizes'].split(','))) # (3,4,5)\n",
        "    self.convs1 = nn.ModuleList([nn.Conv2d(self.Ci, self.Co, (K, self.D)) for K in self.Ks])\n",
        "    self.fc1 = nn.Linear(len(self.Ks) * self.Co, self.num_classes) \n",
        "\n",
        "    self.total_iterations = 0 \n",
        "\n",
        "  def forward(self, x):\n",
        "      # print('org: ', x.size())\n",
        "      x = x.unsqueeze(self.Ci)  # (N, Ci, W, D)\n",
        "      # from pdb import set_trace as st\n",
        "      # st() \n",
        "      # print(f'unsqueeze {self.Ci}: {x.size()}')\n",
        "      x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
        "      # print(f'conv, relu, squeeze : {x.size()}')\n",
        "      x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "      # print(f'max_pool1d, squeeze : {x.size()}')\n",
        "      x = torch.cat(x, 1)\n",
        "      # print(f' cat: {x.size()}')\n",
        "      x = self.dropout(x)  # (N, len(Ks)*Co)\n",
        "      logit = self.fc1(x)  # (N, C)\n",
        "      # print(f' logit: {logit.size()}')\n",
        "      return logit\n",
        "\n",
        "\n",
        "  def prepare_data(self):\n",
        "\n",
        "    if self.hparams['dataset'] == 'NLUHD':\n",
        "\n",
        "      not_none = lambda x: x[\"text\"] is not None \n",
        "      ds = lf.CsvDataset(self.hparams['dataset_path'], header=True).filter(not_none)\n",
        "      unique_labels = list(pd.DataFrame(ds).intent.unique())\n",
        "      le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      print(f\"Unique labels: {unique_labels}\")\n",
        "      print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                            int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                            seed=self.hparams['seed'])\n",
        "      preprocessor = partial(\n",
        "                            preprocess_NLUHD,\n",
        "                            self.hparams['model_type'],\n",
        "                            self.hparams['max_sentence_len'],\n",
        "                            self.tokenizer_dict[self.hparams['model_type']],\n",
        "                            le,\n",
        "                            )\n",
        "      \n",
        "    elif self.hparams['dataset'] == 'MR':\n",
        "\n",
        "      preprocessor = partial(\n",
        "                            preprocess_MR,\n",
        "                            self.hparams,\n",
        "                            self.tokenizer_fun, \n",
        "                            self.embeder,\n",
        "                            le,\n",
        "                            )\n",
        "\n",
        "    elif self.hparams['dataset'] == 'IMDB':\n",
        "\n",
        "      ds = lfds.Imdb('train') + lfds.Imdb('test')\n",
        "      ds = ds.map(lambda x: {'text': x[0], 'label': x[1]})\n",
        "      df = pd.DataFrame(ds)\n",
        "      # self.embeder.build_vocab(new_sentences, update=True)\n",
        "      # self.embeder.train(new_sentences, total_examples=len(new_sentences), epochs=)\n",
        "      print(df.info(memory_usage=True))\n",
        "      unique_labels = list(df.label.unique())\n",
        "      print(f'unique_labels: {unique_labels}')\n",
        "      print(f'number_of_categories : {len(unique_labels)}')\n",
        "      le = preprocessing.LabelEncoder().fit(unique_labels)\n",
        "      train, test = lf.cross_validation.split_dataset_random(ds,\n",
        "                                                             int(len(ds) * self.hparams['train_test_split']),\n",
        "                                                             seed=self.hparams['seed'])\n",
        "      preprocessor = partial(\n",
        "                             preprocess_IMDB,\n",
        "                             self.hparams,\n",
        "                             self.tokenizer_fun, \n",
        "                             self.embeder,\n",
        "                             le,\n",
        "                            )\n",
        "    else:\n",
        "      raise ValueError('Wrong dataset name : {}'.format(self.hparams['dataset']))\n",
        "\n",
        "    \n",
        "    train_df, test_df = pd.DataFrame(train), pd.DataFrame(test)\n",
        "    x_train, y_train = train_df['text'].values, train_df['label'].values\n",
        "    x_test, y_test = test_df['text'].values, test_df['label'].values\n",
        "\n",
        "    # split's parameters  \n",
        "    num_classes = len(unique_labels)\n",
        "    label_per_class = self.hparams['n_labeled'] // num_classes\n",
        "    valid_size = self.hparams['valid_size_per_class']\n",
        "\n",
        "    labeled_idx = []\n",
        "    unlabeled_idx = []\n",
        "    val_idx = []\n",
        "    \n",
        "    for label in unique_labels:\n",
        "        idx = np.where(y_train == label)[0]\n",
        "        np.random.shuffle(idx)\n",
        "        labeled_idx.extend(idx[:label_per_class])\n",
        "        val_idx.extend(idx[label_per_class: label_per_class + valid_size])\n",
        "        unlabeled_idx.extend(idx[label_per_class + valid_size:])\n",
        "\n",
        "    x_labeled, y_labeled  = x_train[labeled_idx], y_train[labeled_idx]\n",
        "    x_unlabeled, y_unlabeled = x_train[unlabeled_idx], y_train[unlabeled_idx]\n",
        "    x_val, y_val = x_train[val_idx], y_train[val_idx]\n",
        "    \n",
        "    train_labeled_dataset = SimpleTextDataset(x_labeled,\n",
        "                                              y_labeled,\n",
        "                                              transform=preprocessor)\n",
        "    \n",
        "    train_unlabeled_dataset = SimpleTextDataset(x_unlabeled,\n",
        "                                                y_unlabeled,\n",
        "                                                transform=preprocessor)\n",
        "    \n",
        "    self._train_dataset = TwoInOneDataset([train_labeled_dataset,\n",
        "                                            train_unlabeled_dataset])\n",
        "    \n",
        "    self._val_dataset = SimpleTextDataset(x_val,\n",
        "                                          y_val,\n",
        "                                          transform=preprocessor)\n",
        "    \n",
        "    self._test_dataset = SimpleTextDataset(x_test,\n",
        "                                           y_test,\n",
        "                                           transform=preprocessor)\n",
        "    \n",
        "    self.total_iterations = len(train_unlabeled_dataset) // self.hparams['batch_size'] \n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(\n",
        "                      self._train_dataset,\n",
        "                      batch_size=self.hparams['batch_size'],\n",
        "                      num_workers=8,\n",
        "                      shuffle=True # without shuffle it want work cause it need to create map index before __get_item__ function\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "                      self._val_dataset,\n",
        "                      batch_size=self.hparams['batch_size'],\n",
        "                      num_workers=8\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(\n",
        "                      self._test_dataset,\n",
        "                      batch_size=self.hparams['batch_size'],\n",
        "                      num_workers=8\n",
        "                     )\n",
        "    \n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "\n",
        "    optimizers = [\n",
        "                  torch.optim.Adam(self.parameters(), lr=self.lr),\n",
        "                 ]\n",
        "    schedulers = [\n",
        "                  {\n",
        "                    'scheduler': ReduceLROnPlateau(optimizers[0],'min', verbose=True), \n",
        "                    'monitor': 'val_loss', # Default: val_loss\n",
        "                    'interval': 'epoch',\n",
        "                    'frequency': 1\n",
        "                  },\n",
        "                 ]\n",
        "\n",
        "    return optimizers, schedulers\n",
        "\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "\n",
        "    l_batch = batch[0]\n",
        "    l_texts = l_batch['embedding']\n",
        "    labels = l_batch['label']\n",
        "    unl_texts = batch[1]['embedding']\n",
        "\n",
        "    if self.hparams['with_VAT']:\n",
        "      vat_loss = VATLoss(xi=self.xi,\n",
        "                         eps=self.eps,\n",
        "                         ip=self.ip)\n",
        "      lds = vat_loss(self, unl_texts)\n",
        "\n",
        "    logits = self.forward(l_texts)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "\n",
        "    if self.hparams['with_VAT']:\n",
        "      loss += self.alpha * lds \n",
        "\n",
        "    labels_hat = logits.max(dim=1)[1]\n",
        "\n",
        "    labels = labels.detach().cpu()\n",
        "    labels_hat = labels_hat.detach().cpu()\n",
        "    \n",
        "    accuracy_error = torch.tensor(1 - accuracy_score(labels, labels_hat))\n",
        "    f1_error = torch.tensor(1 - f1_score(labels, labels_hat, average='micro'))\n",
        "    recall_error = torch.tensor(1 - recall_score(labels, labels_hat, average='micro'))\n",
        "    precision_error = torch.tensor(1 - precision_score(labels, labels_hat, average='micro'))\n",
        "\n",
        "    logs = {'train_loss': loss,\n",
        "            'train_accuracy_error': accuracy_error,\n",
        "            'train_f1_error': f1_error,\n",
        "            'train_recall_error': recall_error,\n",
        "            'train_precision_error': precision_error,\n",
        "           }  \n",
        "\n",
        "    if self.hparams['with_VAT']:\n",
        "      logs.update({'lds_loss': lds.item()})\n",
        "\n",
        "\n",
        "    return {'loss': loss,\n",
        "            'log': logs}\n",
        "\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    texts = batch['embedding']\n",
        "    labels = batch['label']\n",
        "  \n",
        "    logits = self.forward(texts)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "    labels_hat = torch.argmax(logits, dim=1)\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    labels_hat = labels_hat.cpu()\n",
        "\n",
        "    accuracy_error = torch.tensor(1 - accuracy_score(labels, labels_hat))\n",
        "    f1_error = torch.tensor(1 - f1_score(labels, labels_hat, average='micro'))\n",
        "    recall_error = torch.tensor(1 - recall_score(labels, labels_hat, average='micro'))\n",
        "    precision_error = torch.tensor(1 - precision_score(labels, labels_hat, average='micro'))\n",
        "  \n",
        "    output = {\n",
        "            \"val_loss\": loss,\n",
        "            'accuracy_error': accuracy_error,\n",
        "            'f1_error': f1_error,\n",
        "            'recall_error': recall_error,\n",
        "            'precision_error': precision_error,\n",
        "            }\n",
        "  \n",
        "    return output\n",
        "\n",
        "\n",
        "  def validation_epoch_end(self, outputs):\n",
        "    # CHANGE FOR TENSORS!!!!\n",
        "    val_acc = compute_global_metric(outputs, 'accuracy_error')\n",
        "    val_f1 = compute_global_metric(outputs, 'f1_error')\n",
        "    val_recall = compute_global_metric(outputs, 'recall_error')\n",
        "    val_precision = compute_global_metric(outputs, 'precision_error')\n",
        "    val_loss = compute_global_metric(outputs, \"val_loss\")\n",
        "\n",
        "    tqdm_dict = {\n",
        "                 \"val_loss\": val_loss,\n",
        "                 \"val_acc\": val_acc,\n",
        "                 \"val_f1\": val_f1,\n",
        "                 \"val_recall\": val_recall,\n",
        "                 \"val_precision\": val_precision,\n",
        "                }\n",
        "    return {\n",
        "            \"progress_bar\": tqdm_dict,\n",
        "            \"log\": tqdm_dict,\n",
        "            \"val_loss\": val_loss,\n",
        "            'val_acc': val_acc,\n",
        "            'val_f1': val_f1\n",
        "           }\n",
        "\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "\n",
        "    texts = batch[\"embedding\"]\n",
        "    labels = batch[\"label\"]\n",
        "  \n",
        "    logits = self.forward(texts)\n",
        "    loss = self.loss_fct(logits, labels)\n",
        "    labels_hat = torch.argmax(logits, dim=1)\n",
        "\n",
        "    labels = labels.cpu()\n",
        "    labels_hat = labels_hat.cpu()\n",
        "\n",
        "\n",
        "    accuracy_error = torch.tensor(1 - accuracy_score(labels, labels_hat))\n",
        "    f1_error = torch.tensor(1 - f1_score(labels, labels_hat, average='micro'))\n",
        "    recall_error = torch.tensor(1 - recall_score(labels, labels_hat, average='micro'))\n",
        "    precision_error = torch.tensor(1 - precision_score(labels, labels_hat, average='micro'))\n",
        "  \n",
        "    return {\n",
        "            \"test_loss\": loss,\n",
        "            'accuracy_error': accuracy_error,\n",
        "            'f1_error': f1_error,\n",
        "            'recall_error': recall_error,\n",
        "            'precision_error': precision_error,\n",
        "           }\n",
        "\n",
        "\n",
        "  def test_epoch_end(self, outputs):\n",
        "\n",
        "    test_acc = compute_global_metric(outputs, 'accuracy_error') \n",
        "    test_f1 = compute_global_metric(outputs, 'f1_error')\n",
        "    test_recall = compute_global_metric(outputs, 'recall_error')\n",
        "    test_precision = compute_global_metric(outputs, 'precision_error')\n",
        "    test_loss = compute_global_metric(outputs, \"test_loss\")\n",
        "\n",
        "    tqdm_dict = {\n",
        "                 \"test_loss\": test_loss,\n",
        "                 \"test_acc\": test_acc,\n",
        "                 \"test_f1\": test_f1,\n",
        "                 \"test_recall\": test_recall,\n",
        "                 \"test_precision\": test_precision,\n",
        "                }\n",
        "    return {\n",
        "            \"progress_bar\": tqdm_dict,\n",
        "            \"log\": tqdm_dict,\n",
        "            \"test_loss\": test_loss,\n",
        "            'test_acc': test_acc,\n",
        "            'test_f1': test_f1\n",
        "           }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8blDj0ji58J4",
        "colab_type": "text"
      },
      "source": [
        "#### Define objective func\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPAMdmP6F-DM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def objective(trial):\n",
        "  # nluhd_dataset_path = str(Path()/\n",
        "  #                         'gdrive'/\n",
        "  #                         'My Drive'/\n",
        "  #                         'praca_magisterska'/\n",
        "  #                         'pytorch_lightning'/\n",
        "  #                         'datasets'/\n",
        "  #                         'NLU-Data-Home-Domain-preprocessed-without-ner.csv')\n",
        "  hparams = {\n",
        "            # model architecture\n",
        "            'model_type': 'YoonKimConvNN',\n",
        "            'dropout':'0.2,0.7', \n",
        "            'kernel_sizes': '3,4,5', # (3,4,5)\n",
        "            'kernel_num': '60,120', # interval\n",
        "            'Ci': 1,\n",
        "            'loss_function':'CrossEntropyLoss',\n",
        "            # pl trainer params\n",
        "            'seed': 42,\n",
        "            'monitor_value': 'val_acc',\n",
        "            'percent_valid_examples': 0.5, \n",
        "            'test_run': False,\n",
        "            'with_VAT': True,\n",
        "            'max_epochs': 2,\n",
        "            'min_epochs': 1,\n",
        "            'val_check_interval': 0.5, \n",
        "            'patience': 3, # early stopping callback parameter\n",
        "            'distributed_backend': 'dp',\n",
        "            # embeddings params\n",
        "            'embeder_type': \"fastText\",\n",
        "            'embed_dim': 300,\n",
        "            'max_sentence_len': 400,\n",
        "            # dataset params\n",
        "            'train_test_split': 0.8,\n",
        "            'batch_size': 32,\n",
        "            'n_labeled': 1000, # number of labeled samples \n",
        "            'valid_size_per_class': 1000, # 68 class => n_val_samples = 68 * 10 \n",
        "            # optimizer params\n",
        "            'lr': '0.00001, 10',\n",
        "            # VAT params\n",
        "            'xi':'6,12',\n",
        "            'eps':'1,3',\n",
        "            'ip':'1,3',\n",
        "            'alpha':'1,3',\n",
        "            }\n",
        "  \n",
        "  set_seed(hparams['seed'])\n",
        "  \n",
        "  hparams.update({'dataset':'IMDB',\n",
        "                  'num_classes': 2})\n",
        "  \n",
        "  \n",
        "  # training and evaluating model\n",
        "  \n",
        "          \n",
        "  metrics_callback = MetricsCallback()\n",
        "  \n",
        "  checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "          os.path.join(os.environ['RESULT_PATH'],\n",
        "                       \"trial_{}\".format(trial.number),\n",
        "                       \"{epoch}\"), monitor=hparams['monitor_value']\n",
        "      )\n",
        "  \n",
        "  trainer = pl.Trainer(\n",
        "                  gpus=1 if torch.cuda.is_available() else None,\n",
        "                  logger=False,\n",
        "                  # val_percent_check=hparams['percent_valid_examples'],\n",
        "                  checkpoint_callback=checkpoint_callback,\n",
        "                  max_epochs=hparams['max_epochs'],\n",
        "                  fast_dev_run=hparams['test_run'],\n",
        "                  callbacks=[metrics_callback],\n",
        "                  early_stop_callback=PyTorchLightningPruningCallback(trial, monitor=hparams['monitor_value'])\n",
        "            )\n",
        "  \n",
        "  model = LitYKConv_HPO(hparams, trial=trial)\n",
        "  \n",
        "  trainer.fit(model)\n",
        "  return metrics_callback.metrics[-1][\"val_acc\"]\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjAj0j4L5vTG",
        "colab_type": "text"
      },
      "source": [
        "#### Run trails"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuNsLhst6BPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pruning = True\n",
        "\n",
        "pruner = optuna.pruners.MedianPruner() if pruning else optuna.pruners.NopPruner()\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
        "study.optimize(objective, n_trials=20, timeout=None)\n",
        "\n",
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "# shutil.rmtree(os.environ['RESULT_PATH'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8LrwpD19Y3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optuna.visualization.plot_intermediate_values(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y5eSN9y9Xqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optuna.visualization.plot_param_importances(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRChtuTLMfxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}